{"title":"Cilium实现原理","uid":"65501d6a3eead0987c9f5d07b63371fd","slug":"Cilium实现原理","date":"2022-04-12T14:09:25.000Z","updated":"2022-04-13T14:08:01.025Z","comments":true,"path":"api/articles/Cilium实现原理.json","keywords":null,"cover":[],"content":"<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Cilium是eBPF网络分支的明星项目，Cilium的发布也意味着eBPF开始向K8S领域进军。</p></blockquote>\n<div class=\"toc\">\n\n<!-- toc -->\n\n<ul>\n<li><a href=\"#cilium-ebpf-liu-cheng\">Cilium eBPF流程</a><ul>\n<li><a href=\"#liu-cheng-tu\">流程图</a></li>\n<li><a href=\"#agent-qi-dong-yuan-li\">Agent启动原理</a><ul>\n<li><a href=\"#ru-kou-han-shu\">入口函数</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#shu-ju-lu-jing\">数据路径</a><ul>\n<li><a href=\"#kube-proxy-bao-zhuan-fa-lu-jing\">kube-proxy包转发路径</a></li>\n<li><a href=\"#cilium-ebpf-bao-zhuan-fa-lu-jing\">Cilium eBPF包转发路径</a></li>\n<li><a href=\"#yan-shi\">演示</a></li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><a href=\"#cha-kan-main-xiang-qing\">查看main详情</a></li>\n<li><a href=\"#ling-yi-ge-session-zhua-bao\">另一个session抓包</a></li>\n<li><a href=\"#cha-kan-su-zhu-ji-dui-ying-wang-qia-xin-xi\">查看宿主机对应网卡信息</a></li>\n<li><a href=\"#cha-kan-tc-gua-zai\">查看tc挂载</a></li>\n</ul>\n<!-- tocstop -->\n\n</div>\n\n<h2><span id=\"cilium-ebpf-liu-cheng\">Cilium eBPF流程</span><a href=\"#cilium-ebpf-liu-cheng\" class=\"header-anchor\">#</a></h2><p>了解Cilium实现原理之前，首先我们先来看看Cilium使用eBPF实现容器网络的流程。  </p>\n<h3><span id=\"liu-cheng-tu\">流程图</span><a href=\"#liu-cheng-tu\" class=\"header-anchor\">#</a></h3><p><img src=\"../images/Cilium%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/%E6%B5%81%E7%A8%8B%E5%9B%BE.png\" alt=\"流程图\"></p>\n<p>如上图所示，有以下几个步骤：  </p>\n<ol>\n<li>Cilium agent 生成 eBPF 程序；  </li>\n<li>用 LLVM 编译 eBPF 程序，生成 eBPF 对象文件（object file，*.o）；  </li>\n<li>用 eBPF loader 将对象文件加载到 Linux 内核；</li>\n<li>校验器（verifier）对 eBPF 指令会进行合法性验证，以确保程序是安全的，例如 ，无非法内存访问、不会 crash 内核、不会有无限循环等；  </li>\n<li>对象文件被即时编译（JIT）为能直接在底层平台（例如 x86）运行的 native code；  </li>\n<li>如果要在内核和用户态之间共享状态，BPF 程序可以使用 BPF map，这种一种共享存储 ，BPF 侧和用户侧都可以访问；  </li>\n<li>BPF 程序就绪，等待事件触发其执行。对于这个例子，就是有数据包到达网络设备时，触发 BPF 程序的执行。  </li>\n<li>BPF 程序对收到的包进行处理，例如 mangle。最后返回一个裁决（verdict）结果；  </li>\n<li>根据裁决结果，如果是 DROP，这个包将被丢弃；如果是 PASS，包会被送到更网络栈的 更上层继续处理；如果是重定向，就发送给其他设备。</li>\n</ol>\n<h3><span id=\"agent-qi-dong-yuan-li\">Agent启动原理</span><a href=\"#agent-qi-dong-yuan-li\" class=\"header-anchor\">#</a></h3><p>了解了Cilium eBPF流程后我们接下来看看Agent的源码。<br>Cilium Agent的启动配置是存在ConfigMap中的，通过–config-dir参数指定配置文件目录。  </p>\n<h4><span id=\"ru-kou-han-shu\">入口函数</span><a href=\"#ru-kou-han-shu\" class=\"header-anchor\">#</a></h4><details class=\"custom-details\">\n<summary>daemon/cmd/daemon_main.go</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">func runDaemon() &#123;\n\tdatapathConfig :&#x3D; linuxdatapath.DatapathConfiguration&#123;\n\t\tHostDevice: defaults.HostDevice,\n\t&#125;\n\n\tlog.Info(&quot;Initializing daemon&quot;)\n\n\toption.Config.RunMonitorAgent &#x3D; true\n\n\tif err :&#x3D; enableIPForwarding(); err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Fatal(&quot;Error when enabling sysctl parameters&quot;)\n\t&#125;\n\n\tiptablesManager :&#x3D; &amp;iptables.IptablesManager&#123;&#125;\n\tiptablesManager.Init()\n\n\tvar wgAgent *wireguard.Agent\n\tif option.Config.EnableWireguard &#123;\n\t\tswitch &#123;\n\t\tcase option.Config.EnableIPSec:\n\t\t\tlog.Fatalf(&quot;Wireguard (--%s) cannot be used with IPSec (--%s)&quot;,\n\t\t\t\toption.EnableWireguard, option.EnableIPSecName)\n\t\tcase option.Config.EnableL7Proxy:\n\t\t\tlog.Fatalf(&quot;Wireguard (--%s) is not compatible with L7 proxy (--%s)&quot;,\n\t\t\t\toption.EnableWireguard, option.EnableL7Proxy)\n\t\t&#125;\n\n\t\tvar err error\n\t\tprivateKeyPath :&#x3D; filepath.Join(option.Config.StateDir, wireguardTypes.PrivKeyFilename)\n\t\twgAgent, err &#x3D; wireguard.NewAgent(privateKeyPath)\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Failed to initialize wireguard&quot;)\n\t\t&#125;\n\n\t\tcleaner.cleanupFuncs.Add(func() &#123;\n\t\t\t_ &#x3D; wgAgent.Close()\n\t\t&#125;)\n\t&#125; else &#123;\n\t\t&#x2F;&#x2F; Delete wireguard device from previous run (if such exists)\n\t\tlink.DeleteByName(wireguardTypes.IfaceName)\n\t&#125;\n\n\tif k8s.IsEnabled() &#123;\n\t\tbootstrapStats.k8sInit.Start()\n\t\tif err :&#x3D; k8s.Init(option.Config); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Unable to initialize Kubernetes subsystem&quot;)\n\t\t&#125;\n\t\tbootstrapStats.k8sInit.End(true)\n\t&#125;\n\n\tctx, cancel :&#x3D; context.WithCancel(server.ServerCtx)\n\td, restoredEndpoints, err :&#x3D; NewDaemon(ctx, cancel,\n\t\tWithDefaultEndpointManager(ctx, endpoint.CheckHealth),\n\t\tlinuxdatapath.NewDatapath(datapathConfig, iptablesManager, wgAgent))\n\tif err !&#x3D; nil &#123;\n\t\tselect &#123;\n\t\tcase &lt;-server.ServerCtx.Done():\n\t\t\tlog.WithError(err).Debug(&quot;Error while creating daemon&quot;)\n\t\tdefault:\n\t\t\tlog.WithError(err).Fatal(&quot;Error while creating daemon&quot;)\n\t\t&#125;\n\t\treturn\n\t&#125;\n\n\t&#x2F;&#x2F; This validation needs to be done outside of the agent until\n\t&#x2F;&#x2F; datapath.NodeAddressing is used consistently across the code base.\n\tlog.Info(&quot;Validating configured node address ranges&quot;)\n\tif err :&#x3D; node.ValidatePostInit(); err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Fatal(&quot;postinit failed&quot;)\n\t&#125;\n\n\tbootstrapStats.enableConntrack.Start()\n\tlog.Info(&quot;Starting connection tracking garbage collector&quot;)\n\tgc.Enable(option.Config.EnableIPv4, option.Config.EnableIPv6,\n\t\trestoredEndpoints.restored, d.endpointManager)\n\tbootstrapStats.enableConntrack.End(true)\n\n\tbootstrapStats.k8sInit.Start()\n\tif k8s.IsEnabled() &#123;\n\t\t&#x2F;&#x2F; Wait only for certain caches, but not all!\n\t\t&#x2F;&#x2F; (Check Daemon.InitK8sSubsystem() for more info)\n\t\t&lt;-d.k8sCachesSynced\n\t&#125;\n\tbootstrapStats.k8sInit.End(true)\n\trestoreComplete :&#x3D; d.initRestore(restoredEndpoints)\n\tif wgAgent !&#x3D; nil &#123;\n\t\tif err :&#x3D; wgAgent.RestoreFinished(); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Error(&quot;Failed to set up wireguard peers&quot;)\n\t\t&#125;\n\t&#125;\n\n\tif d.endpointManager.HostEndpointExists() &#123;\n\t\td.endpointManager.InitHostEndpointLabels(d.ctx)\n\t&#125; else &#123;\n\t\tlog.Info(&quot;Creating host endpoint&quot;)\n\t\tif err :&#x3D; d.endpointManager.AddHostEndpoint(\n\t\t\td.ctx, d, d, d.ipcache, d.l7Proxy, d.identityAllocator,\n\t\t\t&quot;Create host endpoint&quot;, nodeTypes.GetName(),\n\t\t); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Unable to create host endpoint&quot;)\n\t\t&#125;\n\t&#125;\n\n\tif option.Config.EnableIPMasqAgent &#123;\n\t\tipmasqAgent, err :&#x3D; ipmasq.NewIPMasqAgent(option.Config.IPMasqAgentConfigPath)\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Failed to create ip-masq-agent&quot;)\n\t\t&#125;\n\t\tipmasqAgent.Start()\n\t&#125;\n\n\tif !option.Config.DryMode &#123;\n\t\tgo func() &#123;\n\t\t\tif restoreComplete !&#x3D; nil &#123;\n\t\t\t\t&lt;-restoreComplete\n\t\t\t&#125;\n\t\t\td.dnsNameManager.CompleteBootstrap()\n\n\t\t\tms :&#x3D; maps.NewMapSweeper(&amp;EndpointMapManager&#123;\n\t\t\t\tEndpointManager: d.endpointManager,\n\t\t\t&#125;)\n\t\t\tms.CollectStaleMapGarbage()\n\t\t\tms.RemoveDisabledMaps()\n\n\t\t\tif len(d.restoredCIDRs) &gt; 0 &#123;\n\t\t\t\t&#x2F;&#x2F; Release restored CIDR identities after a grace period (default 10\n\t\t\t\t&#x2F;&#x2F; minutes).  Any identities actually in use will still exist after\n\t\t\t\t&#x2F;&#x2F; this.\n\t\t\t\t&#x2F;&#x2F;\n\t\t\t\t&#x2F;&#x2F; This grace period is needed when running on an external workload\n\t\t\t\t&#x2F;&#x2F; where policy synchronization is not done via k8s. Also in k8s\n\t\t\t\t&#x2F;&#x2F; case it is prudent to allow concurrent endpoint regenerations to\n\t\t\t\t&#x2F;&#x2F; (re-)allocate the restored identities before we release them.\n\t\t\t\ttime.Sleep(option.Config.IdentityRestoreGracePeriod)\n\t\t\t\tlog.Debugf(&quot;Releasing reference counts for %d restored CIDR identities&quot;, len(d.restoredCIDRs))\n\n\t\t\t\td.ipcache.ReleaseCIDRIdentitiesByCIDR(d.restoredCIDRs)\n\t\t\t\t&#x2F;&#x2F; release the memory held by restored CIDRs\n\t\t\t\td.restoredCIDRs &#x3D; nil\n\t\t\t&#125;\n\t\t&#125;()\n\t\td.endpointManager.Subscribe(d)\n\t\tdefer d.endpointManager.Unsubscribe(d)\n\t&#125;\n\n\t&#x2F;&#x2F; Migrating the ENI datapath must happen before the API is served to\n\t&#x2F;&#x2F; prevent endpoints from being created. It also must be before the health\n\t&#x2F;&#x2F; initialization logic which creates the health endpoint, for the same\n\t&#x2F;&#x2F; reasons as the API being served. We want to ensure that this migration\n\t&#x2F;&#x2F; logic runs before any endpoint creates.\n\tif option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMENI &#123;\n\t\tmigrated, failed :&#x3D; linuxrouting.NewMigrator(\n\t\t\t&amp;eni.InterfaceDB&#123;&#125;,\n\t\t).MigrateENIDatapath(option.Config.EgressMultiHomeIPRuleCompat)\n\t\tswitch &#123;\n\t\tcase failed &#x3D;&#x3D; -1:\n\t\t\t&#x2F;&#x2F; No need to handle this case specifically because it is handled\n\t\t\t&#x2F;&#x2F; in the call already.\n\t\tcase migrated &gt;&#x3D; 0 &amp;&amp; failed &gt; 0:\n\t\t\tlog.Errorf(&quot;Failed to migrate ENI datapath. &quot;+\n\t\t\t\t&quot;%d endpoints were successfully migrated and %d failed to migrate completely. &quot;+\n\t\t\t\t&quot;The original datapath is still in-place, however it is recommended to retry the migration.&quot;,\n\t\t\t\tmigrated, failed)\n\n\t\tcase migrated &gt;&#x3D; 0 &amp;&amp; failed &#x3D;&#x3D; 0:\n\t\t\tlog.Infof(&quot;Migration of ENI datapath successful, %d endpoints were migrated and none failed.&quot;,\n\t\t\t\tmigrated)\n\t\t&#125;\n\t&#125;\n\n\tbootstrapStats.healthCheck.Start()\n\tif option.Config.EnableHealthChecking &#123;\n\t\td.initHealth()\n\t&#125;\n\tbootstrapStats.healthCheck.End(true)\n\n\td.startStatusCollector()\n\n\tmetricsErrs :&#x3D; initMetrics()\n\n\td.startAgentHealthHTTPService()\n\tif option.Config.KubeProxyReplacementHealthzBindAddr !&#x3D; &quot;&quot; &#123;\n\t\tif option.Config.KubeProxyReplacement !&#x3D; option.KubeProxyReplacementDisabled &#123;\n\t\t\td.startKubeProxyHealthzHTTPService(fmt.Sprintf(&quot;%s&quot;, option.Config.KubeProxyReplacementHealthzBindAddr))\n\t\t&#125;\n\t&#125;\n\n\tbootstrapStats.initAPI.Start()\n\tsrv :&#x3D; server.NewServer(d.instantiateAPI())\n\tsrv.EnabledListeners &#x3D; []string&#123;&quot;unix&quot;&#125;\n\tsrv.SocketPath &#x3D; option.Config.SocketPath\n\tsrv.ReadTimeout &#x3D; apiTimeout\n\tsrv.WriteTimeout &#x3D; apiTimeout\n\tdefer srv.Shutdown()\n\n\tsrv.ConfigureAPI()\n\tbootstrapStats.initAPI.End(true)\n\n\terr &#x3D; d.SendNotification(monitorAPI.StartMessage(time.Now()))\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Warn(&quot;Failed to send agent start monitor message&quot;)\n\t&#125;\n\n\tif !d.datapath.Node().NodeNeighDiscoveryEnabled() &#123;\n\t\t&#x2F;&#x2F; Remove all non-GC&#39;ed neighbor entries that might have previously set\n\t\t&#x2F;&#x2F; by a Cilium instance.\n\t\td.datapath.Node().NodeCleanNeighbors(false)\n\t&#125; else &#123;\n\t\t&#x2F;&#x2F; If we came from an agent upgrade, migrate entries.\n\t\td.datapath.Node().NodeCleanNeighbors(true)\n\t\t&#x2F;&#x2F; Start periodical refresh of the neighbor table from the agent if needed.\n\t\tif option.Config.ARPPingRefreshPeriod !&#x3D; 0 &amp;&amp; !option.Config.ARPPingKernelManaged &#123;\n\t\t\td.nodeDiscovery.Manager.StartNeighborRefresh(d.datapath.Node())\n\t\t&#125;\n\t&#125;\n\n\tif option.Config.BGPControlPlaneEnabled() &#123;\n\t\tlog.Info(&quot;Initializing BGP Control Plane&quot;)\n\t\tif err :&#x3D; d.instantiateBGPControlPlane(d.ctx); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Error returned when instantiating BGP control plane&quot;)\n\t\t&#125;\n\t&#125;\n\n\tlog.WithField(&quot;bootstrapTime&quot;, time.Since(bootstrapTimestamp)).\n\t\tInfo(&quot;Daemon initialization completed&quot;)\n\n\tif option.Config.WriteCNIConfigurationWhenReady !&#x3D; &quot;&quot; &#123;\n\t\tinput, err :&#x3D; os.ReadFile(option.Config.ReadCNIConfiguration)\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Unable to read CNI configuration file&quot;)\n\t\t&#125;\n\n\t\tif err &#x3D; os.WriteFile(option.Config.WriteCNIConfigurationWhenReady, input, 0644); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatalf(&quot;Unable to write CNI configuration file to %s&quot;, option.Config.WriteCNIConfigurationWhenReady)\n\t\t&#125; else &#123;\n\t\t\tlog.Infof(&quot;Wrote CNI configuration file to %s&quot;, option.Config.WriteCNIConfigurationWhenReady)\n\t\t&#125;\n\t&#125;\n\n\terrs :&#x3D; make(chan error, 1)\n\n\tgo func() &#123;\n\t\terrs &lt;- srv.Serve()\n\t&#125;()\n\n\tif k8s.IsEnabled() &#123;\n\t\tbootstrapStats.k8sInit.Start()\n\t\tk8s.Client().MarkNodeReady(d.k8sWatcher, nodeTypes.GetName())\n\t\tbootstrapStats.k8sInit.End(true)\n\t&#125;\n\n\tbootstrapStats.overall.End(true)\n\tbootstrapStats.updateMetrics()\n\tgo d.launchHubble()\n\n\terr &#x3D; option.Config.StoreInFile(option.Config.StateDir)\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Error(&quot;Unable to store Cilium&#39;s configuration&quot;)\n\t&#125;\n\n\terr &#x3D; option.StoreViperInFile(option.Config.StateDir)\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Error(&quot;Unable to store Viper&#39;s configuration&quot;)\n\t&#125;\n\n\tselect &#123;\n\tcase err :&#x3D; &lt;-metricsErrs:\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Cannot start metrics server&quot;)\n\t\t&#125;\n\tcase err :&#x3D; &lt;-errs:\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Error returned from non-returning Serve() call&quot;)\n\t\t&#125;\n\t&#125;\n&#125;</code></pre>\n</p>\n</details>\n<p>Cilium Agent实现主要分为以下几步：  </p>\n<ol>\n<li><p>打开IP forwarding</p>\n</li>\n<li><p>初始化k8s</p>\n</li>\n<li><p>创建守护进程  </p>\n<details class=\"custom-details\">\n<summary>daemon/cmd/daemon.go</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">     func NewDaemon(ctx context.Context, cancel context.CancelFunc, epMgr *endpointmanager.EndpointManager, dp datapath.Datapath) (*Daemon, *endpointRestoreState, error) &#123;\n\n\t&#x2F;&#x2F; Pass the cancel to our signal handler directly so that it&#39;s canceled\n\t&#x2F;&#x2F; before we run the cleanup functions (see &#96;cleanup.go&#96; for implementation).\n\tcleaner.SetCancelFunc(cancel)\n\n\tvar (\n\t\terr           error\n\t\tnetConf       *cnitypes.NetConf\n\t\tconfiguredMTU &#x3D; option.Config.MTU\n\t)\n\n\tbootstrapStats.daemonInit.Start()\n\n\t&#x2F;&#x2F; Validate the daemon-specific global options.\n\tif err :&#x3D; option.Config.Validate(); err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;invalid daemon configuration: %s&quot;, err)\n\t&#125;\n\n\tif option.Config.ReadCNIConfiguration !&#x3D; &quot;&quot; &#123;\n\t\tnetConf, err &#x3D; cnitypes.ReadNetConf(option.Config.ReadCNIConfiguration)\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;unable to read CNI configuration: %w&quot;, err)\n\t\t&#125;\n\n\t\tif netConf.MTU !&#x3D; 0 &#123;\n\t\t\tconfiguredMTU &#x3D; netConf.MTU\n\t\t\tlog.WithField(&quot;mtu&quot;, configuredMTU).Info(&quot;Overwriting MTU based on CNI configuration&quot;)\n\t\t&#125;\n\t&#125;\n\n\tapiLimiterSet, err :&#x3D; rate.NewAPILimiterSet(option.Config.APIRateLimit, apiRateLimitDefaults, &amp;apiRateLimitingMetrics&#123;&#125;)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;unable to configure API rate limiting: %w&quot;, err)\n\t&#125;\n\n\t&#x2F;&#x2F; Check the kernel if we can make use of managed neighbor entries which\n\t&#x2F;&#x2F; simplifies and fully &#39;offloads&#39; L2 resolution handling to the kernel.\n\tprobeManagedNeighborSupport()\n\n\t&#x2F;&#x2F; Do the partial kube-proxy replacement initialization before creating BPF\n\t&#x2F;&#x2F; maps. Otherwise, some maps might not be created (e.g. session affinity).\n\t&#x2F;&#x2F; finishKubeProxyReplacementInit(), which is called later after the device\n\t&#x2F;&#x2F; detection, might disable BPF NodePort and friends. But this is fine, as\n\t&#x2F;&#x2F; the feature does not influence the decision which BPF maps should be\n\t&#x2F;&#x2F; created.\n\tisKubeProxyReplacementStrict, err :&#x3D; initKubeProxyReplacementOptions()\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;unable to initialize Kube proxy replacement options: %w&quot;, err)\n\t&#125;\n\n\tctmap.InitMapInfo(option.Config.CTMapEntriesGlobalTCP, option.Config.CTMapEntriesGlobalAny,\n\t\toption.Config.EnableIPv4, option.Config.EnableIPv6, option.Config.EnableNodePort)\n\tpolicymap.InitMapInfo(option.Config.PolicyMapEntries)\n\tlbmap.Init(lbmap.InitParams&#123;\n\t\tIPv4: option.Config.EnableIPv4,\n\t\tIPv6: option.Config.EnableIPv6,\n\n\t\tMaxSockRevNatMapEntries: option.Config.SockRevNatEntries,\n\t\tMaxEntries:              option.Config.LBMapEntries,\n\t&#125;)\n\n\tif option.Config.DryMode &#x3D;&#x3D; false &#123;\n\t\tif err :&#x3D; rlimit.RemoveMemlock(); err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;unable to set memory resource limits: %w&quot;, err)\n\t\t&#125;\n\t&#125;\n\n\tauthKeySize, encryptKeyID, err :&#x3D; setupIPSec()\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;unable to setup encryption: %s&quot;, err)\n\t&#125;\n\n\tvar mtuConfig mtu.Configuration\n\texternalIP :&#x3D; node.GetIPv4()\n\tif externalIP &#x3D;&#x3D; nil &#123;\n\t\texternalIP &#x3D; node.GetIPv6()\n\t&#125;\n\t&#x2F;&#x2F; ExternalIP could be nil but we are covering that case inside NewConfiguration\n\tmtuConfig &#x3D; mtu.NewConfiguration(\n\t\tauthKeySize,\n\t\toption.Config.EnableIPSec,\n\t\toption.Config.TunnelingEnabled(),\n\t\toption.Config.EnableWireguard,\n\t\tconfiguredMTU,\n\t\texternalIP,\n\t)\n\n\tnodeMngr, err :&#x3D; nodemanager.NewManager(&quot;all&quot;, dp.Node(), option.Config, nil, nil)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, err\n\t&#125;\n\n\tidentity.IterateReservedIdentities(func(_ identity.NumericIdentity, _ *identity.Identity) &#123;\n\t\tmetrics.Identity.Inc()\n\t&#125;)\n\tif option.Config.EnableWellKnownIdentities &#123;\n\t\t&#x2F;&#x2F; Must be done before calling policy.NewPolicyRepository() below.\n\t\tnum :&#x3D; identity.InitWellKnownIdentities(option.Config)\n\t\tmetrics.Identity.Add(float64(num))\n\t&#125;\n\n\tnd :&#x3D; nodediscovery.NewNodeDiscovery(nodeMngr, mtuConfig, netConf)\n\n\td :&#x3D; Daemon&#123;\n\t\tctx:               ctx,\n\t\tcancel:            cancel,\n\t\tprefixLengths:     createPrefixLengthCounter(),\n\t\tbuildEndpointSem:  semaphore.NewWeighted(int64(numWorkerThreads())),\n\t\tcompilationMutex:  new(lock.RWMutex),\n\t\tnetConf:           netConf,\n\t\tmtuConfig:         mtuConfig,\n\t\tdatapath:          dp,\n\t\tdeviceManager:     NewDeviceManager(),\n\t\tnodeDiscovery:     nd,\n\t\tendpointCreations: newEndpointCreationManager(),\n\t\tapiLimiterSet:     apiLimiterSet,\n\t&#125;\n\n\tif option.Config.RunMonitorAgent &#123;\n\t\td.monitorAgent &#x3D; monitoragent.NewAgent(ctx)\n\t&#125;\n\n\td.configModifyQueue &#x3D; eventqueue.NewEventQueueBuffered(&quot;config-modify-queue&quot;, ConfigModifyQueueSize)\n\td.configModifyQueue.Run()\n\n\td.rec, err &#x3D; recorder.NewRecorder(d.ctx, &amp;d)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;error while initializing BPF pcap recorder: %w&quot;, err)\n\t&#125;\n\n\t&#x2F;&#x2F; Collect old CIDR identities\n\tvar oldNIDs []identity.NumericIdentity\n\tif option.Config.RestoreState &amp;&amp; !option.Config.DryMode &amp;&amp; ipcachemap.SupportsDump() &#123;\n\t\tif err :&#x3D; ipcachemap.IPCache.DumpWithCallback(func(key bpf.MapKey, value bpf.MapValue) &#123;\n\t\t\tk :&#x3D; key.(*ipcachemap.Key)\n\t\t\tv :&#x3D; value.(*ipcachemap.RemoteEndpointInfo)\n\t\t\tnid :&#x3D; identity.NumericIdentity(v.SecurityIdentity)\n\t\t\tif nid.HasLocalScope() &#123;\n\t\t\t\td.restoredCIDRs &#x3D; append(d.restoredCIDRs, k.IPNet())\n\t\t\t\toldNIDs &#x3D; append(oldNIDs, nid)\n\t\t\t&#125;\n\t\t&#125;); err !&#x3D; nil &amp;&amp; !os.IsNotExist(err) &#123;\n\t\t\tlog.WithError(err).Warning(&quot;Error dumping ipcache&quot;)\n\t\t&#125;\n\t\tipcachemap.IPCache.Close()\n\t&#125;\n\n\t&#x2F;&#x2F; Propagate identity allocator down to packages which themselves do not\n\t&#x2F;&#x2F; have types to which we can add an allocator member.\n\t&#x2F;&#x2F;\n\t&#x2F;&#x2F; **NOTE** The identity allocator is not yet initialized here; that\n\t&#x2F;&#x2F; happens below. We&#39;ve only allocated the structure at this point.\n\t&#x2F;&#x2F;\n\t&#x2F;&#x2F; TODO: convert these package level variables to types for easier unit\n\t&#x2F;&#x2F; testing in the future.\n\td.identityAllocator &#x3D; NewCachingIdentityAllocator(&amp;d)\n\tif err :&#x3D; d.initPolicy(epMgr); err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;error while initializing policy subsystem: %w&quot;, err)\n\t&#125;\n\td.ipcache &#x3D; ipcache.NewIPCache(&amp;ipcache.Configuration&#123;\n\t\tIdentityAllocator: d.identityAllocator,\n\t\tPolicyHandler:     d.policy.GetSelectorCache(),\n\t\tDatapathHandler:   epMgr,\n\t&#125;)\n\tnodeMngr &#x3D; nodeMngr.WithIPCache(d.ipcache)\n\tnodeMngr &#x3D; nodeMngr.WithSelectorCacheUpdater(d.policy.GetSelectorCache()) &#x2F;&#x2F; must be after initPolicy\n\tnodeMngr &#x3D; nodeMngr.WithPolicyTriggerer(epMgr)                            &#x2F;&#x2F; must be after initPolicy\n\n\tproxy.Allocator &#x3D; d.identityAllocator\n\n\td.endpointManager &#x3D; epMgr\n\td.endpointManager.InitMetrics()\n\n\t&#x2F;&#x2F; Start the proxy before we start K8s watcher or restore endpoints so that we can inject\n\t&#x2F;&#x2F; the daemon&#39;s proxy into the k8s watcher and each endpoint.\n\t&#x2F;&#x2F; Note: d.endpointManager needs to be set before this\n\tbootstrapStats.proxyStart.Start()\n\t&#x2F;&#x2F; FIXME: Make the port range configurable.\n\tif option.Config.EnableL7Proxy &#123;\n\t\td.l7Proxy &#x3D; proxy.StartProxySupport(10000, 20000, option.Config.RunDir,\n\t\t\t&amp;d, option.Config.AgentLabels, d.datapath, d.endpointManager, d.ipcache)\n\t&#125; else &#123;\n\t\tlog.Info(&quot;L7 proxies are disabled&quot;)\n\t\tif option.Config.EnableEnvoyConfig &#123;\n\t\t\tlog.Warningf(&quot;%s is not functional when L7 proxies are disabled&quot;,\n\t\t\t\toption.EnableEnvoyConfig)\n\t\t&#125;\n\t&#125;\n\tbootstrapStats.proxyStart.End(true)\n\n\t&#x2F;&#x2F; Start service support after proxy support so that we can inject &#39;d.l7Proxy&#96;.\n\td.svc &#x3D; service.NewService(&amp;d, d.l7Proxy)\n\n\td.redirectPolicyManager &#x3D; redirectpolicy.NewRedirectPolicyManager(d.svc)\n\tif option.Config.BGPAnnounceLBIP || option.Config.BGPAnnouncePodCIDR &#123;\n\t\td.bgpSpeaker, err &#x3D; speaker.New(ctx, speaker.Opts&#123;\n\t\t\tLoadBalancerIP: option.Config.BGPAnnounceLBIP,\n\t\t\tPodCIDR:        option.Config.BGPAnnouncePodCIDR,\n\t\t&#125;)\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Error(&quot;Error creating new BGP speaker&quot;)\n\t\t\treturn nil, nil, err\n\t\t&#125;\n\t&#125;\n\n\tif option.Config.EnableIPv4EgressGateway &#123;\n\t\td.egressGatewayManager &#x3D; egressgateway.NewEgressGatewayManager(&amp;d)\n\t&#125;\n\n\td.k8sWatcher &#x3D; watchers.NewK8sWatcher(\n\t\td.endpointManager,\n\t\td.nodeDiscovery.Manager,\n\t\t&amp;d,\n\t\td.policy,\n\t\td.svc,\n\t\td.datapath,\n\t\td.redirectPolicyManager,\n\t\td.bgpSpeaker,\n\t\td.egressGatewayManager,\n\t\td.l7Proxy,\n\t\toption.Config,\n\t\td.ipcache,\n\t)\n\tnd.RegisterK8sNodeGetter(d.k8sWatcher)\n\td.ipcache.RegisterK8sSyncedChecker(&amp;d)\n\n\td.k8sWatcher.RegisterNodeSubscriber(d.endpointManager)\n\tif option.Config.BGPAnnounceLBIP || option.Config.BGPAnnouncePodCIDR &#123;\n\t\tswitch option.Config.IPAMMode() &#123;\n\t\tcase ipamOption.IPAMKubernetes:\n\t\t\td.k8sWatcher.RegisterNodeSubscriber(d.bgpSpeaker)\n\t\tcase ipamOption.IPAMClusterPool:\n\t\t\td.k8sWatcher.RegisterCiliumNodeSubscriber(d.bgpSpeaker)\n\t\t&#125;\n\t&#125;\n\tif option.Config.EnableServiceTopology &#123;\n\t\td.k8sWatcher.RegisterNodeSubscriber(&amp;d.k8sWatcher.K8sSvcCache)\n\t&#125;\n\n\td.k8sWatcher.NodeChain.Register(watchers.NewCiliumNodeUpdater(d.k8sWatcher))\n\n\td.redirectPolicyManager.RegisterSvcCache(&amp;d.k8sWatcher.K8sSvcCache)\n\td.redirectPolicyManager.RegisterGetStores(d.k8sWatcher)\n\tif option.Config.BGPAnnounceLBIP &#123;\n\t\td.bgpSpeaker.RegisterSvcCache(&amp;d.k8sWatcher.K8sSvcCache)\n\t&#125;\n\n\tbootstrapStats.daemonInit.End(true)\n\n\t&#x2F;&#x2F; Stop all endpoints (its goroutines) on exit.\n\tcleaner.cleanupFuncs.Add(func() &#123;\n\t\tlog.Info(&quot;Waiting for all endpoints&#39; go routines to be stopped.&quot;)\n\t\tvar wg sync.WaitGroup\n\n\t\teps :&#x3D; d.endpointManager.GetEndpoints()\n\t\twg.Add(len(eps))\n\n\t\tfor _, ep :&#x3D; range eps &#123;\n\t\t\tgo func(ep *endpoint.Endpoint) &#123;\n\t\t\t\tep.Stop()\n\t\t\t\twg.Done()\n\t\t\t&#125;(ep)\n\t\t&#125;\n\n\t\twg.Wait()\n\t\tlog.Info(&quot;All endpoints&#39; goroutines stopped.&quot;)\n\t&#125;)\n\n\t&#x2F;&#x2F; Open or create BPF maps.\n\tbootstrapStats.mapsInit.Start()\n\terr &#x3D; d.initMaps()\n\tbootstrapStats.mapsInit.EndError(err)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;error while opening&#x2F;creating BPF maps: %w&quot;, err)\n\t&#125;\n\n\t&#x2F;&#x2F; Read the service IDs of existing services from the BPF map and\n\t&#x2F;&#x2F; reserve them. This must be done *before* connecting to the\n\t&#x2F;&#x2F; Kubernetes apiserver and serving the API to ensure service IDs are\n\t&#x2F;&#x2F; not changing across restarts or that a new service could accidentally\n\t&#x2F;&#x2F; use an existing service ID.\n\t&#x2F;&#x2F; Also, create missing v2 services from the corresponding legacy ones.\n\tif option.Config.RestoreState &amp;&amp; !option.Config.DryMode &#123;\n\t\tbootstrapStats.restore.Start()\n\t\tif err :&#x3D; d.svc.RestoreServices(); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Warn(&quot;Failed to restore services from BPF maps&quot;)\n\t\t&#125;\n\t\tbootstrapStats.restore.End(true)\n\t&#125;\n\n\tdebug.RegisterStatusObject(&quot;k8s-service-cache&quot;, &amp;d.k8sWatcher.K8sSvcCache)\n\tdebug.RegisterStatusObject(&quot;ipam&quot;, d.ipam)\n\tdebug.RegisterStatusObject(&quot;ongoing-endpoint-creations&quot;, d.endpointCreations)\n\n\td.k8sWatcher.RunK8sServiceHandler()\n\n\tif option.Config.DNSPolicyUnloadOnShutdown &#123;\n\t\tlog.Debugf(&quot;Registering cleanup function to unload DNS policies due to --%s&quot;, option.DNSPolicyUnloadOnShutdown)\n\n\t\t&#x2F;&#x2F; add to pre-cleanup funcs because this needs to run on graceful shutdown, but\n\t\t&#x2F;&#x2F; before the relevant subystems are being shut down.\n\t\tcleaner.preCleanupFuncs.Add(func() &#123;\n\t\t\t&#x2F;&#x2F; Stop k8s watchers\n\t\t\tlog.Info(&quot;Stopping k8s service handler&quot;)\n\t\t\td.k8sWatcher.StopK8sServiceHandler()\n\n\t\t\t&#x2F;&#x2F; Iterate over the policy repository and remove L7 DNS part\n\t\t\tneedsPolicyRegen :&#x3D; false\n\t\t\tremoveL7DNSRules :&#x3D; func(pr policyAPI.Ports) error &#123;\n\t\t\t\tportProtocols :&#x3D; pr.GetPortProtocols()\n\t\t\t\tif len(portProtocols) &#x3D;&#x3D; 0 &#123;\n\t\t\t\t\treturn nil\n\t\t\t\t&#125;\n\t\t\t\tportRule :&#x3D; pr.GetPortRule()\n\t\t\t\tif portRule &#x3D;&#x3D; nil || portRule.Rules &#x3D;&#x3D; nil &#123;\n\t\t\t\t\treturn nil\n\t\t\t\t&#125;\n\t\t\t\tdnsRules :&#x3D; portRule.Rules.DNS\n\t\t\t\tlog.Debugf(&quot;Found egress L7 DNS rules (portProtocol %#v): %#v&quot;, portProtocols[0], dnsRules)\n\n\t\t\t\t&#x2F;&#x2F; For security reasons, the L7 DNS policy must be a\n\t\t\t\t&#x2F;&#x2F; wildcard in order to trigger this logic.\n\t\t\t\t&#x2F;&#x2F; Otherwise we could invalidate the L7 security\n\t\t\t\t&#x2F;&#x2F; rules. This means if any of the DNS L7 rules\n\t\t\t\t&#x2F;&#x2F; have a matchPattern of * then it is OK to delete\n\t\t\t\t&#x2F;&#x2F; the L7 portion of those rules.\n\t\t\t\thasWildcard :&#x3D; false\n\t\t\t\tfor _, dns :&#x3D; range dnsRules &#123;\n\t\t\t\t\tif dns.MatchPattern &#x3D;&#x3D; &quot;*&quot; &#123;\n\t\t\t\t\t\thasWildcard &#x3D; true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t&#125;\n\t\t\t\t&#125;\n\t\t\t\tif hasWildcard &#123;\n\t\t\t\t\tportRule.Rules &#x3D; nil\n\t\t\t\t\tneedsPolicyRegen &#x3D; true\n\t\t\t\t&#125;\n\t\t\t\treturn nil\n\t\t\t&#125;\n\n\t\t\tpolicyRepo :&#x3D; d.GetPolicyRepository()\n\t\t\tpolicyRepo.Iterate(func(rule *policyAPI.Rule) &#123;\n\t\t\t\tfor _, er :&#x3D; range rule.Egress &#123;\n\t\t\t\t\t_ &#x3D; er.ToPorts.Iterate(removeL7DNSRules)\n\t\t\t\t&#125;\n\t\t\t&#125;)\n\n\t\t\tif !needsPolicyRegen &#123;\n\t\t\t\tlog.Infof(&quot;No policy recalculation needed to remove DNS rules due to --%s&quot;, option.DNSPolicyUnloadOnShutdown)\n\t\t\t\treturn\n\t\t\t&#125;\n\n\t\t\t&#x2F;&#x2F; Bump revision to trigger policy recalculation\n\t\t\tlog.Infof(&quot;Triggering policy recalculation to remove DNS rules due to --%s&quot;, option.DNSPolicyUnloadOnShutdown)\n\t\t\tpolicyRepo.BumpRevision()\n\t\t\tregenerationMetadata :&#x3D; &amp;regeneration.ExternalRegenerationMetadata&#123;\n\t\t\t\tReason:            &quot;unloading DNS rules on graceful shutdown&quot;,\n\t\t\t\tRegenerationLevel: regeneration.RegenerateWithoutDatapath,\n\t\t\t&#125;\n\t\t\twg :&#x3D; d.endpointManager.RegenerateAllEndpoints(regenerationMetadata)\n\t\t\twg.Wait()\n\t\t\tlog.Info(&quot;All endpoints regenerated after unloading DNS rules on graceful shutdown&quot;)\n\t\t&#125;)\n\t&#125;\n\n\ttreatRemoteNodeAsHost :&#x3D; option.Config.AlwaysAllowLocalhost() &amp;&amp; !option.Config.EnableRemoteNodeIdentity\n\tpolicyAPI.InitEntities(option.Config.ClusterName, treatRemoteNodeAsHost)\n\n\tbootstrapStats.restore.Start()\n\t&#x2F;&#x2F; fetch old endpoints before k8s is configured.\n\trestoredEndpoints, err :&#x3D; d.fetchOldEndpoints(option.Config.StateDir)\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Error(&quot;Unable to read existing endpoints&quot;)\n\t&#125;\n\tbootstrapStats.restore.End(true)\n\n\tbootstrapStats.fqdn.Start()\n\terr &#x3D; d.bootstrapFQDN(restoredEndpoints.possible, option.Config.ToFQDNsPreCache)\n\tif err !&#x3D; nil &#123;\n\t\tbootstrapStats.fqdn.EndError(err)\n\t\treturn nil, restoredEndpoints, err\n\t&#125;\n\tbootstrapStats.fqdn.End(true)\n\n\tif k8s.IsEnabled() &#123;\n\t\tbootstrapStats.k8sInit.Start()\n\t\t&#x2F;&#x2F; Errors are handled inside WaitForCRDsToRegister. It will fatal on a\n\t\t&#x2F;&#x2F; context deadline or if the context has been cancelled, the context&#39;s\n\t\t&#x2F;&#x2F; error will be returned. Otherwise, it succeeded.\n\t\tif err :&#x3D; d.k8sWatcher.WaitForCRDsToRegister(d.ctx); err !&#x3D; nil &#123;\n\t\t\treturn nil, restoredEndpoints, err\n\t\t&#125;\n\n\t\t&#x2F;&#x2F; Launch the K8s node watcher so we can start receiving node events.\n\t\t&#x2F;&#x2F; Launching the k8s node watcher at this stage will prevent all agents\n\t\t&#x2F;&#x2F; from performing Gets directly into kube-apiserver to get the most up\n\t\t&#x2F;&#x2F; to date version of the k8s node. This allows for better scalability\n\t\t&#x2F;&#x2F; in large clusters.\n\t\td.k8sWatcher.NodesInit(k8s.Client())\n\n\t\tif option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMClusterPool || option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMClusterPoolV2 &#123;\n\t\t\t&#x2F;&#x2F; Create the CiliumNode custom resource. This call will block until\n\t\t\t&#x2F;&#x2F; the custom resource has been created\n\t\t\td.nodeDiscovery.UpdateCiliumNodeResource()\n\t\t&#125;\n\n\t\tif err :&#x3D; k8s.WaitForNodeInformation(d.ctx, d.k8sWatcher); err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;unable to connect to get node spec from apiserver: %w&quot;, err)\n\t\t&#125;\n\n\t\t&#x2F;&#x2F; Kubernetes demands that the localhost can always reach local\n\t\t&#x2F;&#x2F; pods. Therefore unless the AllowLocalhost policy is set to a\n\t\t&#x2F;&#x2F; specific mode, always allow localhost to reach local\n\t\t&#x2F;&#x2F; endpoints.\n\t\tif option.Config.AllowLocalhost &#x3D;&#x3D; option.AllowLocalhostAuto &#123;\n\t\t\toption.Config.AllowLocalhost &#x3D; option.AllowLocalhostAlways\n\t\t\tlog.Info(&quot;k8s mode: Allowing localhost to reach local endpoints&quot;)\n\t\t&#125;\n\n\t\tbootstrapStats.k8sInit.End(true)\n\t&#125;\n\n\tif wgAgent :&#x3D; dp.WireguardAgent(); option.Config.EnableWireguard &#123;\n\t\tif err :&#x3D; wgAgent.(*wg.Agent).Init(d.ipcache, mtuConfig); err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;failed to initialize wireguard agent: %w&quot;, err)\n\t\t&#125;\n\t&#125;\n\n\t&#x2F;&#x2F; Perform an early probe on the underlying kernel on whether BandwidthManager\n\t&#x2F;&#x2F; can be supported or not. This needs to be done before handleNativeDevices()\n\t&#x2F;&#x2F; as BandwidthManager needs these to be available for setup.\n\tbandwidth.ProbeBandwidthManager()\n\n\t&#x2F;&#x2F; The kube-proxy replacement and host-fw devices detection should happen after\n\t&#x2F;&#x2F; establishing a connection to kube-apiserver, but before starting a k8s watcher.\n\t&#x2F;&#x2F; This is because the device detection requires self (Cilium)Node object,\n\t&#x2F;&#x2F; and the k8s service watcher depends on option.Config.EnableNodePort flag\n\t&#x2F;&#x2F; which can be modified after the device detection.\n\tif err :&#x3D; d.deviceManager.Detect(); err !&#x3D; nil &#123;\n\t\tif areDevicesRequired() &#123;\n\t\t\t&#x2F;&#x2F; Fail hard if devices are required to function.\n\t\t\treturn nil, nil, fmt.Errorf(&quot;failed to detect devices: %w&quot;, err)\n\t\t&#125;\n\t\tlog.WithError(err).Warn(&quot;failed to detect devices, disabling BPF NodePort&quot;)\n\t\tdisableNodePort()\n\t&#125;\n\tif err :&#x3D; finishKubeProxyReplacementInit(isKubeProxyReplacementStrict); err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;failed to finalise LB initialization: %w&quot;, err)\n\t&#125;\n\n\t&#x2F;&#x2F; BPF masquerade depends on BPF NodePort and require host-reachable svc to\n\t&#x2F;&#x2F; be fully enabled in the tunneling mode, so the following checks should\n\t&#x2F;&#x2F; happen after invoking initKubeProxyReplacementOptions().\n\tif option.Config.EnableIPv4Masquerade &amp;&amp; option.Config.EnableBPFMasquerade &amp;&amp;\n\t\t(!option.Config.EnableNodePort || option.Config.EgressMasqueradeInterfaces !&#x3D; &quot;&quot; || !option.Config.EnableRemoteNodeIdentity ||\n\t\t\t(option.Config.TunnelingEnabled() &amp;&amp; !hasFullHostReachableServices())) &#123;\n\n\t\tvar msg string\n\t\tswitch &#123;\n\t\tcase !option.Config.EnableNodePort:\n\t\t\tmsg &#x3D; fmt.Sprintf(&quot;BPF masquerade requires NodePort (--%s&#x3D;\\&quot;true\\&quot;).&quot;,\n\t\t\t\toption.EnableNodePort)\n\t\tcase !option.Config.EnableRemoteNodeIdentity:\n\t\t\tmsg &#x3D; fmt.Sprintf(&quot;BPF masquerade requires remote node identities (--%s&#x3D;\\&quot;true\\&quot;).&quot;,\n\t\t\t\toption.EnableRemoteNodeIdentity)\n\t\tcase option.Config.EgressMasqueradeInterfaces !&#x3D; &quot;&quot;:\n\t\t\tmsg &#x3D; fmt.Sprintf(&quot;BPF masquerade does not allow to specify devices via --%s (use --%s instead).&quot;,\n\t\t\t\toption.EgressMasqueradeInterfaces, option.Devices)\n\t\t&#125;\n\t\t&#x2F;&#x2F; ipt.InstallRules() (called by Reinitialize()) happens later than\n\t\t&#x2F;&#x2F; this  statement, so it&#39;s OK to fallback to iptables-based MASQ.\n\t\toption.Config.EnableBPFMasquerade &#x3D; false\n\t\tlog.Warn(msg + &quot; Falling back to iptables-based masquerading.&quot;)\n\t\t&#x2F;&#x2F; Too bad, if we need to revert to iptables-based MASQ, we also cannot\n\t\t&#x2F;&#x2F; use BPF host routing since we need the upper stack.\n\t\tif !option.Config.EnableHostLegacyRouting &#123;\n\t\t\toption.Config.EnableHostLegacyRouting &#x3D; true\n\t\t\tlog.Infof(&quot;BPF masquerade could not be enabled. Falling back to legacy host routing (--%s&#x3D;\\&quot;true\\&quot;).&quot;,\n\t\t\t\toption.EnableHostLegacyRouting)\n\t\t&#125;\n\t&#125;\n\tif option.Config.EnableIPv4EgressGateway &#123;\n\t\tif !probes.NewProbeManager().GetMisc().HaveLargeInsnLimit &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;egress gateway needs kernel 5.2 or newer&quot;)\n\t\t&#125;\n\n\t\t&#x2F;&#x2F; datapath code depends on remote node identities to distinguish between cluser-local and\n\t\t&#x2F;&#x2F; cluster-egress traffic\n\t\tif !option.Config.EnableRemoteNodeIdentity &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;egress gateway requires remote node identities (--%s&#x3D;\\&quot;true\\&quot;).&quot;,\n\t\t\t\toption.EnableRemoteNodeIdentity)\n\t\t&#125;\n\t&#125;\n\tif option.Config.EnableIPv4Masquerade &amp;&amp; option.Config.EnableBPFMasquerade &#123;\n\t\t&#x2F;&#x2F; TODO(brb) nodeport + ipvlan constraints will be lifted once the SNAT BPF code has been refactored\n\t\tif option.Config.DatapathMode &#x3D;&#x3D; datapathOption.DatapathModeIpvlan &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;BPF masquerade works only in veth mode (--%s&#x3D;\\&quot;%s\\&quot;&quot;, option.DatapathMode, datapathOption.DatapathModeVeth)\n\t\t&#125;\n\t\tif err :&#x3D; node.InitBPFMasqueradeAddrs(option.Config.Devices); err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;failed to determine BPF masquerade IPv4 addrs: %w&quot;, err)\n\t\t&#125;\n\t&#125; else if option.Config.EnableIPMasqAgent &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;BPF ip-masq-agent requires --%s&#x3D;\\&quot;true\\&quot; and --%s&#x3D;\\&quot;true\\&quot;&quot;, option.EnableIPv4Masquerade, option.EnableBPFMasquerade)\n\t&#125; else if option.Config.EnableIPv4EgressGateway &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;egress gateway requires --%s&#x3D;\\&quot;true\\&quot; and --%s&#x3D;\\&quot;true\\&quot;&quot;, option.EnableIPv4Masquerade, option.EnableBPFMasquerade)\n\t&#125; else if !option.Config.EnableIPv4Masquerade &amp;&amp; option.Config.EnableBPFMasquerade &#123;\n\t\t&#x2F;&#x2F; There is not yet support for option.Config.EnableIPv6Masquerade\n\t\tlog.Infof(&quot;Auto-disabling %q feature since IPv4 masquerading was generally disabled&quot;,\n\t\t\toption.EnableBPFMasquerade)\n\t\toption.Config.EnableBPFMasquerade &#x3D; false\n\t&#125;\n\tif option.Config.EnableIPMasqAgent &#123;\n\t\tif !option.Config.EnableIPv4 &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;BPF ip-masq-agent requires IPv4 support (--%s&#x3D;\\&quot;true\\&quot;)&quot;, option.EnableIPv4Name)\n\t\t&#125;\n\t\tif !probe.HaveFullLPM() &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;BPF ip-masq-agent needs kernel 4.16 or newer&quot;)\n\t\t&#125;\n\t&#125;\n\tif option.Config.EnableHostFirewall &amp;&amp; len(option.Config.Devices) &#x3D;&#x3D; 0 &#123;\n\t\tmsg :&#x3D; &quot;host firewall&#39;s external facing device could not be determined. Use --%s to specify.&quot;\n\t\treturn nil, nil, fmt.Errorf(msg, option.Devices)\n\t&#125;\n\n\t&#x2F;&#x2F; Some of the k8s watchers rely on option flags set above (specifically\n\t&#x2F;&#x2F; EnableBPFMasquerade), so we should only start them once the flag values\n\t&#x2F;&#x2F; are set.\n\tif k8s.IsEnabled() &#123;\n\t\tbootstrapStats.k8sInit.Start()\n\t\t&#x2F;&#x2F; Launch the K8s watchers in parallel as we continue to process other\n\t\t&#x2F;&#x2F; daemon options.\n\t\td.k8sCachesSynced &#x3D; d.k8sWatcher.InitK8sSubsystem(d.ctx)\n\t\tbootstrapStats.k8sInit.End(true)\n\t&#125;\n\n\tbootstrapStats.cleanup.Start()\n\terr &#x3D; clearCiliumVeths()\n\tbootstrapStats.cleanup.EndError(err)\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Warning(&quot;Unable to clean stale endpoint interfaces&quot;)\n\t&#125;\n\n\t&#x2F;&#x2F; Must init kvstore before starting node discovery\n\tif option.Config.KVStore &#x3D;&#x3D; &quot;&quot; &#123;\n\t\tlog.Info(&quot;Skipping kvstore configuration&quot;)\n\t&#125; else &#123;\n\t\tbootstrapStats.kvstore.Start()\n\t\td.initKVStore()\n\t\tbootstrapStats.kvstore.End(true)\n\t&#125;\n\n\t&#x2F;&#x2F; Fetch the router (&#96;cilium_host&#96;) IPs in case they were set a priori from\n\t&#x2F;&#x2F; the Kubernetes or CiliumNode resource in the K8s subsystem from call\n\t&#x2F;&#x2F; k8s.WaitForNodeInformation(). These will be used later after starting\n\t&#x2F;&#x2F; IPAM initialization to finish off the &#96;cilium_host&#96; IP restoration (part\n\t&#x2F;&#x2F; 2&#x2F;2).\n\trouter4FromK8s, router6FromK8s :&#x3D; node.GetInternalIPv4Router(), node.GetIPv6Router()\n\n\t&#x2F;&#x2F; Configure IPAM without using the configuration yet.\n\td.configureIPAM()\n\n\tif option.Config.JoinCluster &#123;\n\t\tif k8s.IsEnabled() &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;cannot join a Cilium cluster (--%s) when configured as a Kubernetes node&quot;, option.JoinClusterName)\n\t\t&#125;\n\t\tif option.Config.KVStore &#x3D;&#x3D; &quot;&quot; &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;joining a Cilium cluster (--%s) requires kvstore (--%s) be set&quot;, option.JoinClusterName, option.KVStore)\n\t\t&#125;\n\t\tagentLabels :&#x3D; labels.NewLabelsFromModel(option.Config.AgentLabels).K8sStringMap()\n\t\tif option.Config.K8sNamespace !&#x3D; &quot;&quot; &#123;\n\t\t\tagentLabels[k8sConst.PodNamespaceLabel] &#x3D; option.Config.K8sNamespace\n\t\t&#125;\n\t\tagentLabels[k8sConst.PodNameLabel] &#x3D; nodeTypes.GetName()\n\t\tagentLabels[k8sConst.PolicyLabelCluster] &#x3D; option.Config.ClusterName\n\t\t&#x2F;&#x2F; Set configured agent labels to local node for node registration\n\t\tnode.SetLabels(agentLabels)\n\n\t\t&#x2F;&#x2F; This can override node addressing config, so do this before starting IPAM\n\t\tlog.WithField(logfields.NodeName, nodeTypes.GetName()).Debug(&quot;Calling JoinCluster()&quot;)\n\t\td.nodeDiscovery.JoinCluster(nodeTypes.GetName())\n\n\t\t&#x2F;&#x2F; Start services watcher\n\t\tserviceStore.JoinClusterServices(&amp;d.k8sWatcher.K8sSvcCache, option.Config)\n\t&#125;\n\n\t&#x2F;&#x2F; Start IPAM\n\td.startIPAM()\n\n\t&#x2F;&#x2F; After the IPAM is started, in particular IPAM modes (CRD, ENI, Alibaba)\n\t&#x2F;&#x2F; which use the VPC CIDR as the pod CIDR, we must attempt restoring the\n\t&#x2F;&#x2F; router IPs from the K8s resources if we weren&#39;t able to restore them\n\t&#x2F;&#x2F; from the fs. We must do this after IPAM because we must wait until the\n\t&#x2F;&#x2F; K8s resources have been synced. Part 2&#x2F;2 of restoration.\n\tif option.Config.EnableIPv4 &#123;\n\t\trestoreCiliumHostIPs(false, router4FromK8s)\n\t&#125;\n\tif option.Config.EnableIPv6 &#123;\n\t\trestoreCiliumHostIPs(true, router6FromK8s)\n\t&#125;\n\n\t&#x2F;&#x2F; restore endpoints before any IPs are allocated to avoid eventual IP\n\t&#x2F;&#x2F; conflicts later on, otherwise any IP conflict will result in the\n\t&#x2F;&#x2F; endpoint not being able to be restored.\n\terr &#x3D; d.restoreOldEndpoints(restoredEndpoints, true)\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Error(&quot;Unable to restore existing endpoints&quot;)\n\t&#125;\n\tbootstrapStats.restore.End(true)\n\n\tif err :&#x3D; d.allocateIPs(); err !&#x3D; nil &#123;\n\t\treturn nil, nil, err\n\t&#125;\n\n\t&#x2F;&#x2F; Must occur after d.allocateIPs(), see GH-14245 and its fix.\n\td.nodeDiscovery.StartDiscovery()\n\n\t&#x2F;&#x2F; Annotation of the k8s node must happen after discovery of the\n\t&#x2F;&#x2F; PodCIDR range and allocation of the health IPs.\n\tif k8s.IsEnabled() &amp;&amp; option.Config.AnnotateK8sNode &#123;\n\t\tbootstrapStats.k8sInit.Start()\n\t\tlog.WithFields(logrus.Fields&#123;\n\t\t\tlogfields.V4Prefix:       node.GetIPv4AllocRange(),\n\t\t\tlogfields.V6Prefix:       node.GetIPv6AllocRange(),\n\t\t\tlogfields.V4HealthIP:     node.GetEndpointHealthIPv4(),\n\t\t\tlogfields.V6HealthIP:     node.GetEndpointHealthIPv6(),\n\t\t\tlogfields.V4CiliumHostIP: node.GetInternalIPv4Router(),\n\t\t\tlogfields.V6CiliumHostIP: node.GetIPv6Router(),\n\t\t&#125;).Info(&quot;Annotating k8s node&quot;)\n\n\t\terr :&#x3D; k8s.Client().AnnotateNode(nodeTypes.GetName(),\n\t\t\tencryptKeyID,\n\t\t\tnode.GetIPv4AllocRange(), node.GetIPv6AllocRange(),\n\t\t\tnode.GetEndpointHealthIPv4(), node.GetEndpointHealthIPv6(),\n\t\t\tnode.GetInternalIPv4Router(), node.GetIPv6Router())\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Warning(&quot;Cannot annotate k8s node with CIDR range&quot;)\n\t\t&#125;\n\t\tbootstrapStats.k8sInit.End(true)\n\t&#125; else if !option.Config.AnnotateK8sNode &#123;\n\t\tlog.Debug(&quot;Annotate k8s node is disabled.&quot;)\n\t&#125;\n\n\t&#x2F;&#x2F; Trigger refresh and update custom resource in the apiserver with all restored endpoints.\n\t&#x2F;&#x2F; Trigger after nodeDiscovery.StartDiscovery to avoid custom resource update conflict.\n\tif option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMCRD || option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMENI || option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMAzure ||\n\t\toption.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMAlibabaCloud || option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMClusterPoolV2 &#123;\n\t\tif option.Config.EnableIPv6 &#123;\n\t\t\td.ipam.IPv6Allocator.RestoreFinished()\n\t\t&#125;\n\t\tif option.Config.EnableIPv4 &#123;\n\t\t\td.ipam.IPv4Allocator.RestoreFinished()\n\t\t&#125;\n\t&#125;\n\n\tif option.Config.DatapathMode !&#x3D; datapathOption.DatapathModeLBOnly &#123;\n\t\t&#x2F;&#x2F; This needs to be done after the node addressing has been configured\n\t\t&#x2F;&#x2F; as the node address is required as suffix.\n\t\t&#x2F;&#x2F; well known identities have already been initialized above.\n\t\t&#x2F;&#x2F; Ignore the channel returned by this function, as we want the global\n\t\t&#x2F;&#x2F; identity allocator to run asynchronously.\n\t\trealIdentityAllocator :&#x3D; d.identityAllocator\n\t\trealIdentityAllocator.InitIdentityAllocator(k8s.CiliumClient(), nil)\n\n\t\t&#x2F;&#x2F; Preallocate IDs for old CIDRs, must be called after InitIdentityAllocator\n\t\tif len(d.restoredCIDRs) &gt; 0 &#123;\n\t\t\tlog.Infof(&quot;Restoring %d old CIDR identities&quot;, len(d.restoredCIDRs))\n\t\t\t_, err &#x3D; d.ipcache.AllocateCIDRs(d.restoredCIDRs, oldNIDs, nil)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\tlog.WithError(err).Error(&quot;Error allocating old CIDR identities&quot;)\n\t\t\t&#125;\n\t\t&#125;\n\n\t\td.bootstrapClusterMesh(nodeMngr)\n\t&#125;\n\n\t&#x2F;&#x2F; Must be done at least after initializing BPF LB-related maps\n\t&#x2F;&#x2F; (lbmap.Init()).\n\tbootstrapStats.bpfBase.Start()\n\terr &#x3D; d.init()\n\tbootstrapStats.bpfBase.EndError(err)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, restoredEndpoints, fmt.Errorf(&quot;error while initializing daemon: %w&quot;, err)\n\t&#125;\n\n\t&#x2F;&#x2F; iptables rules can be updated only after d.init() intializes the iptables above.\n\terr &#x3D; d.updateDNSDatapathRules()\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, restoredEndpoints, err\n\t&#125;\n\n\t&#x2F;&#x2F; We can only attach the monitor agent once cilium_event has been set up.\n\tif option.Config.RunMonitorAgent &#123;\n\t\terr &#x3D; d.monitorAgent.AttachToEventsMap(defaults.MonitorBufferPages)\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, err\n\t\t&#125;\n\n\t\tif option.Config.EnableMonitor &#123;\n\t\t\terr &#x3D; monitoragent.ServeMonitorAPI(d.monitorAgent)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn nil, nil, err\n\t\t\t&#125;\n\t\t&#125;\n\t&#125;\n\n\tif err :&#x3D; d.syncEndpointsAndHostIPs(); err !&#x3D; nil &#123;\n\t\treturn nil, nil, err\n\t&#125;\n\n\t&#x2F;&#x2F; Start the controller for periodic sync. The purpose of the\n\t&#x2F;&#x2F; controller is to ensure that endpoints and host IPs entries are\n\t&#x2F;&#x2F; reinserted to the bpf maps if they are ever removed from them.\n\tcontroller.NewManager().UpdateController(&quot;sync-endpoints-and-host-ips&quot;,\n\t\tcontroller.ControllerParams&#123;\n\t\t\tDoFunc: func(ctx context.Context) error &#123;\n\t\t\t\treturn d.syncEndpointsAndHostIPs()\n\t\t\t&#125;,\n\t\t\tRunInterval: time.Minute,\n\t\t\tContext:     d.ctx,\n\t\t&#125;)\n\n\tif err :&#x3D; loader.RestoreTemplates(option.Config.StateDir); err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Error(&quot;Unable to restore previous BPF templates&quot;)\n\t&#125;\n\n\t&#x2F;&#x2F; Start watcher for endpoint IP --&gt; identity mappings in key-value store.\n\t&#x2F;&#x2F; this needs to be done *after* init() for the daemon in that function,\n\t&#x2F;&#x2F; we populate the IPCache with the host&#39;s IP(s).\n\td.ipcache.InitIPIdentityWatcher()\n\tidentitymanager.Subscribe(d.policy)\n\n\treturn &amp;d, restoredEndpoints, nil\n&#125;</code></pre>\n\n</p>\n</details>\n</li>\n<li><p>打开conntrack/nat GC</p>\n</li>\n<li><p>加载BPF程序  </p>\n<details class=\"custom-details\">\n<summary>daemon/cmd/datapath.go</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">     &#x2F;&#x2F; initMaps opens all BPF maps (and creates them if they do not exist).\nfunc (d *Daemon) initMaps() error &#123;\n    lxcmap.LXCMap.OpenOrCreate()\n    ipcachemap.IPCache.OpenParallel()\n    ...\n    d.svc.InitMaps(Config.EnableIPv4, createSockRevNatMaps, Config.RestoreState)\n    policymap.InitCallMap()\n\n    for ep :&#x3D; range d.endpointManager.GetEndpoints()\n        ep.InitMap()\n\n    for ep :&#x3D; range d.endpointManager.GetEndpoints()\n        for m :&#x3D; range ctmap.LocalMaps(ep, Config.EnableIPv4)\n            m.Create()\n\n    for m :&#x3D; range ctmap.GlobalMaps(Config.EnableIPv4)\n        m.Create()\n\n    ipv4Nat :&#x3D; nat.GlobalMaps(Config.EnableIPv4)\n    ipv4Nat.Create()\n    if Config.EnableNodePort\n       neighborsmap.InitMaps(Config.EnableIPv4)\n\n    &#x2F;&#x2F; Set up the list of IPCache listeners in the daemon, to be used by syncEndpointsAndHostIPs()\n    ipcache.IPIdentityCache.SetListeners()\n\n    if !Config.RestoreState\n        lxcmap.LXCMap.DeleteAll() &#x2F;&#x2F; If we are not restoring state, all endpoints can be deleted.\n\n    if Config.EnableSessionAffinity &#123;\n        lbmap.AffinityMatchMap.OpenOrCreate()\n        lbmap.Affinity4Map.OpenOrCreate()\n    &#125;\n&#125;\n\nfunc (s *Service) RestoreServices() error &#123;\n    s.restoreBackendsLocked() &#x2F;&#x2F; Restore backend IDs\n    s.restoreServicesLocked() &#x2F;&#x2F; Restore service cache from BPF maps\n\n    if option.Config.EnableSessionAffinity\n        s.deleteOrphanAffinityMatchesLocked() &#x2F;&#x2F; Remove no longer existing affinity matches\n\n    s.deleteOrphanBackends() &#x2F;&#x2F; Remove obsolete backends and release their IDs\n&#125;</code></pre>\n\n</p>\n</details>\n</li>\n<li><p>启动检查，包括健康检查，状态检查，健康指标检查</p>\n</li>\n<li><p>记录Agent启动到监控指标</p>\n</li>\n<li><p>提供Cilium HTTP API</p>\n</li>\n<li><p>标记启动状态</p>\n</li>\n<li><p>打开可视化工具Hubble</p>\n</li>\n</ol>\n<h2><span id=\"shu-ju-lu-jing\">数据路径</span><a href=\"#shu-ju-lu-jing\" class=\"header-anchor\">#</a></h2><h3><span id=\"kube-proxy-bao-zhuan-fa-lu-jing\">kube-proxy包转发路径</span><a href=\"#kube-proxy-bao-zhuan-fa-lu-jing\" class=\"header-anchor\">#</a></h3><p><img src=\"../images/Cilium%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/kubeproxy%E6%95%B0%E6%8D%AE%E8%B7%AF%E5%BE%84.png\" alt=\"kube-proxy数据路径图\"><br>数据路径：  </p>\n<ol>\n<li>网卡收到一个包（通过 DMA 放到 ring-buffer）；  </li>\n<li>包经过 XDP hook 点；  </li>\n<li>内核给包分配内存，此时才有了大家熟悉的 skb（包的内核结构体表示），然后 送到内核协议栈；  </li>\n<li>包经过 GRO 处理，对分片包进行重组；  </li>\n<li>包进入 tc（traffic control）的 ingress hook。接下来，所有橙色的框都是 Netfilter 处理点；  </li>\n<li>Netfilter：在 PREROUTING hook 点处理 raw table 里的 iptables 规则；  </li>\n<li>包经过内核的连接跟踪（conntrack）模块；  </li>\n<li>Netfilter：在 PREROUTING hook 点处理 mangle table 的 iptables 规则；  </li>\n<li>Netfilter：在 PREROUTING hook 点处理 nat table 的 iptables 规则；  </li>\n<li>进行路由判断（FIB：Forwarding Information Base，路由条目的内核表示，译者注） 。接下来又是四个 Netfilter 处理点；  </li>\n<li>Netfilter：在 FORWARD hook 点处理 mangle table 里的 iptables 规则；  </li>\n<li>Netfilter：在 FORWARD hook 点处理 filter table 里的 iptables 规则；  </li>\n<li>Netfilter：在 POSTROUTING hook 点处理 mangle table 里的 iptables 规则；  </li>\n<li>Netfilter：在 POSTROUTING hook 点处理 nat table 里的 iptables 规则；  </li>\n<li>包到达 TC egress hook 点，会进行出方向（egress）的判断，例如判断这个包是到本 地设备，还是到主机外；  </li>\n<li>对大包进行分片。根据 step 15 判断的结果，这个包接下来可能会；  </li>\n<li>发送到一个本机 veth 设备，或者一个本机 service endpoint；</li>\n<li>或者，如果目的 IP 是主机外，就通过网卡发出去。</li>\n</ol>\n<h3><span id=\"cilium-ebpf-bao-zhuan-fa-lu-jing\">Cilium eBPF包转发路径</span><a href=\"#cilium-ebpf-bao-zhuan-fa-lu-jing\" class=\"header-anchor\">#</a></h3><p><img src=\"../images/Cilium%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/cilium%E6%95%B0%E6%8D%AE%E8%B7%AF%E5%BE%84.png\" alt=\"Cilium数据路径图\"></p>\n<p>对比可以看出，Cilium eBPF datapath 做了短路处理：从 tc ingress 直接 shortcut 到 tc egress，节省了 9 个中间步骤（总共 17 个）。更重要的是：这个 datapath 绕过了 整个 Netfilter 框架（橘黄色的框们），Netfilter 在大流量情况下性能是很差的。<br>Cilium/eBPF 还能走的更远。例如，如果包的目的端是另一台主机上的 service endpoint，那你可以直接在 XDP 框中完成包的重定向（收包 1-&gt;2，在步骤 2 中对包 进行修改，再通过 2-&gt;1 发送出去），将其发送出去，如下图所示：<br><img src=\"../images/Cilium%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/service%E6%95%B0%E6%8D%AE%E8%B7%AF%E5%BE%84.png\" alt=\"service数据路径图\"></p>\n<p>可以看到，这种情况下包都没有进入内核协议栈（准确地说，都没有创建 skb）就被转 发出去了，性能可想而知。</p>\n<h3><span id=\"yan-shi\">演示</span><a href=\"#yan-shi\" class=\"header-anchor\">#</a></h3><p>下面通过minikube安装cilium来演示一下数据路径  </p>\n<ol>\n<li><p>查看路由表  </p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\"># 查看路由列表\n&#x2F; # ip rule list\n# 查看main详情\n&#x2F; # ip route show table main</code></pre>\n<p><img src=\"../images/Cilium%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/iprulelist.png\" alt=\"iprulelist\"><br>可以看出，该容器下的所有包都由eth0发往10.0.0.156这个IP，接下我们看看ARP解析，看数据包发往哪？</p>\n</li>\n<li><p>查看ARP信息</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\"># 向80端口发起请求\n&#x2F; # nc -v 1.1.1.1 80\n\n# 另一个session抓包\n   &#x2F; # tcpdump -n -vvv -i eth0</code></pre>\n<p><img src=\"../images/Cilium%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/tcpdump.png\" alt=\"tcpdump\"><br>可以看到ARP响应-&gt; Reply 10.0.0.156 is-at 32:cb:68:5c:d0:14</p>\n</li>\n<li><p>查看网卡对信息  </p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\"># 查看容器网卡信息，找到对应的网卡对\n&#x2F; # ip link\n\n# 查看宿主机对应网卡信息\n$ ip -c link | grep -A 1 35\n\n# 查看tc挂载\n$ tc filter show dev lxc2ba1e99c75fc egress\n$ tc filter show dev lxc2ba1e99c75fc ingress\n</code></pre>\n<p><img src=\"../images/Cilium%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/arp.png\" alt=\"arp\"><br>可以看到egress上没有挂载，但ingress 上则挂载了一段逻辑，section 为from-container，源码：</p>\n<details class=\"custom-details\">\n<summary>bpf/bpf_lxc.c</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\"> __section(&quot;from-container&quot;)\nint handle_xgress(struct __ctx_buff *ctx)\n&#123;\n\tswitch (proto) &#123;\n\t&#x2F;&#x2F; 判断协议为 ARP\n  case bpf_htons(ETH_P_ARP):\n\t\tunion macaddr mac &#x3D; NODE_MAC;\n\t  union macaddr smac;\n\t  __be32 sip;\n\t  __be32 tip;\n    &#x2F;&#x2F; 校验并填充 smac 为当前设备的物理地址\n\t  if (!arp_validate(ctx, &amp;mac, &amp;smac, &amp;sip, &amp;tip))\n\t\t  return CTX_ACT_OK;\n\n\t  if (tip &#x3D;&#x3D; LXC_IPV4)\n\t\t  return CTX_ACT_OK;\n\n     int ret &#x3D; arp_prepare_response(ctx, smac, sip, dmac, tip);\n     if (unlikely(ret !&#x3D; 0))\n\t\t   goto error;\n\n\t   ctx_redirect(ctx, ctx_get_ifindex(ctx), direction);\n     ret &#x3D; DROP_MISSED_TAIL_CALL;\n\t&#125;\n&#125;</code></pre>\n\n</p>\n</details>\n<p>判断出这是一个来自容器的 ARP 协议包之后，Cilium 会在代码中构建出一个 ARP 响应，将当前设备的 MAC 地址设置为结果，然后通过内核提供的 eBPF 辅助函数 bpf_redirect跳过后续的路径。</p>\n</li>\n</ol>\n","feature":true,"text":" Cilium是eBPF网络分支的明星项目，Cilium的发布也意味着eBPF开始向K8S领域进军。 Cilium eBPF流程 流程图 Agent启动原理 入口函数 数据路径 kube-proxy包转发路径 Cilium eBPF包转发路径 演示 查看main详情 另一个ses...","link":"","photos":[],"count_time":{"symbolsCount":"49k","symbolsTime":"44 mins."},"categories":[{"name":"eBPF","slug":"eBPF","count":2,"path":"api/categories/eBPF.json"}],"tags":[{"name":"eBPF","slug":"eBPF","count":2,"path":"api/tags/eBPF.json"},{"name":"CNI","slug":"CNI","count":1,"path":"api/tags/CNI.json"},{"name":"Kubernetes","slug":"Kubernetes","count":1,"path":"api/tags/Kubernetes.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\"><span class=\"toc-text\">Cilium eBPF流程</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">流程图</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">Agent启动原理</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">入口函数</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\"><span class=\"toc-text\">数据路径</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">kube-proxy包转发路径</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">Cilium eBPF包转发路径</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">演示</span></a></li></ol></li></ol>","author":{"name":"Asura","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Keep runnig!","socials":{"github":"https://github.com/G-Asura","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"让内核开发变的简单的eBPF","uid":"8b17daa7707d9d476a6f46015ac8ef41","slug":"让内核开发变的简单的eBPF","date":"2022-04-10T12:43:44.000Z","updated":"2022-04-12T18:01:27.169Z","comments":true,"path":"api/articles/让内核开发变的简单的eBPF.json","keywords":null,"cover":[],"text":" eBPF 是一项革命性的技术，起源于 Linux 内核，可以在操作系统内核中运行沙盒程序。它用于安全有效地扩展内核的功能，而无需更改内核源代码或加载内核模块。 eBPF is a revolutionary technology with origins in the Linu...","link":"","photos":[],"count_time":{"symbolsCount":"7.1k","symbolsTime":"6 mins."},"categories":[{"name":"eBPF","slug":"eBPF","count":2,"path":"api/categories/eBPF.json"}],"tags":[{"name":"Linux","slug":"Linux","count":2,"path":"api/tags/Linux.json"},{"name":"eBPF","slug":"eBPF","count":2,"path":"api/tags/eBPF.json"}],"author":{"name":"Asura","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Keep runnig!","socials":{"github":"https://github.com/G-Asura","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}