{"title":"Prometheus数据流","uid":"78cf92f2c0639bec80519c22649cfa6b","slug":"Prometheus数据流","date":"2022-04-08T08:46:07.000Z","updated":"2022-04-12T04:20:58.958Z","comments":true,"path":"api/articles/Prometheus数据流.json","keywords":null,"cover":[],"content":"<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Prometheus是一个开源的监控和报警工具。它优秀的设计理念，灵活的扩展，丰富的生态以及活跃的社区使它正在成为众多开发者喜爱的监控工具。</p></blockquote>\n<div class=\"toc\">\n\n<!-- toc -->\n\n<ul>\n<li><a href=\"#jian-jie\">简介</a></li>\n<li><a href=\"#shu-ju-la-qu\">数据拉取</a><ul>\n<li><a href=\"#metrics-shu-ju\">Metrics数据</a><ul>\n<li><a href=\"#shi-jian-xiang-liang\">时间向量</a></li>\n<li><a href=\"#sample\">Sample</a></li>\n<li><a href=\"#metric\">Metric</a></li>\n<li><a href=\"#gou-zao-metrics\">构造Metrics</a></li>\n</ul>\n</li>\n<li><a href=\"#shu-ju-zhua-qu\">数据抓取</a><ul>\n<li><a href=\"#job\">Job</a></li>\n<li><a href=\"#target\">Target</a></li>\n<li><a href=\"#cai-ji-fang-shi\">采集方式</a></li>\n<li><a href=\"#fu-wu-fa-xian\">服务发现</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#shu-ju-cun-chu\">数据存储</a><ul>\n<li><a href=\"#remotewrite\">RemoteWrite</a></li>\n<li><a href=\"#remoteread\">RemoteRead</a></li>\n<li><a href=\"#shu-ju-cun-chu-1\">数据存储</a><ul>\n<li><a href=\"#cun-chu-jie-gou\">存储结构</a></li>\n<li><a href=\"#shu-ju-fen-xi\">数据分析</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#bao-jing\">报警</a></li>\n<li><a href=\"#cha-xun\">查询</a><ul>\n<li><a href=\"#dao-pai-suo-yin\">倒排索引</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n\n</div>\n\n<h2><span id=\"jian-jie\">简介</span><a href=\"#jian-jie\" class=\"header-anchor\">#</a></h2><p>在开始之前，我们先看看官方给出的Prometheus的生态架构图：<br><img src=\"../images/Promtheus%E6%95%B0%E6%8D%AE%E6%B5%81/%E6%9E%B6%E6%9E%84%E5%9B%BE.png\" alt=\"架构图\"><br>从图上我们可以看出Prometheus数据流转有几处比较核心：  </p>\n<ol>\n<li>拉取数据(pull metrics)</li>\n<li>采集任务(Job)</li>\n<li>服务发现(ServiceDiscovery)</li>\n<li>数据存储(TSDB)</li>\n<li>报警(Alert)</li>\n<li>查询(PromQL)</li>\n</ol>\n<h2><span id=\"shu-ju-la-qu\">数据拉取</span><a href=\"#shu-ju-la-qu\" class=\"header-anchor\">#</a></h2><h3><span id=\"metrics-shu-ju\">Metrics数据</span><a href=\"#metrics-shu-ju\" class=\"header-anchor\">#</a></h3><p>在了解Prometheus数据流转之前，需要对流转的数据有充分的认识，及Metrics。下面是一条Metrics数据的数据结构，由指标名，标签集，数值和时间戳构成。</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">go_gc_duration_seconds&#123;quantile&#x3D;&quot;0&quot;&#125; 3.4524e-05@1639549978.439\n|--------指标名--------|-labelsets--| |--value--||---timestamp--|</code></pre>\n\n<h4><span id=\"shi-jian-xiang-liang\">时间向量</span><a href=\"#shi-jian-xiang-liang\" class=\"header-anchor\">#</a></h4><p>以时间方向，保存的数值，分为Instant vector（瞬时向量）和Range vector（区间向量）<br>时序数据有“垂直写，水平读”的模式，在同一个时间点上垂直写入多个指标数据，读取一段时间的某个指标的数值  </p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">^\n│   . . . . . . . . . . . . . . . . .   . .   node_cpu_seconds_total&#123;cpu&#x3D;&quot;cpu0&quot;,mode&#x3D;&quot;idle&quot;&#125;\n│     . . . . . . . . . . . . . . . . . . .   node_cpu_seconds_total&#123;cpu&#x3D;&quot;cpu0&quot;,mode&#x3D;&quot;system&quot;&#125;\n│     . . . . . . . . . .   . . . . . . . .   node_load1&#123;&#125;\n│     . . . . . . . . . . . . . . . .   . .  \nv\n  &lt;------------------ 时间 ----------------&gt;</code></pre>\n<h4><span id=\"sample\">Sample</span><a href=\"#sample\" class=\"header-anchor\">#</a></h4><p>上图中的每一个点都是一个Sample（样本数据），每个Sample包含一个时间戳和float64的数值，Sample是Prometheus中最小的数据存储单位。</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">type Sample struct &#123;\n\tValue float64 &#96;protobuf:&quot;fixed64,1,opt,name&#x3D;value,proto3&quot; json:&quot;value,omitempty&quot;&#96;\n\t&#x2F;&#x2F; timestamp is in ms format, see pkg&#x2F;timestamp&#x2F;timestamp.go for\n\t&#x2F;&#x2F; conversion from time.Time to Prometheus timestamp.\n\tTimestamp            int64    &#96;protobuf:&quot;varint,2,opt,name&#x3D;timestamp,proto3&quot; json:&quot;timestamp,omitempty&quot;&#96;\n\tXXX_NoUnkeyedLiteral struct&#123;&#125; &#96;json:&quot;-&quot;&#96;\n\tXXX_unrecognized     []byte   &#96;json:&quot;-&quot;&#96;\n\tXXX_sizecache        int32    &#96;json:&quot;-&quot;&#96;\n&#125;</code></pre>\n<h4><span id=\"metric\">Metric</span><a href=\"#metric\" class=\"header-anchor\">#</a></h4><p>一个Metric包含指标名和LabelSets，能唯一标识出监控数据的含义</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">type metricMap struct &#123;\n\tmtx       sync.RWMutex &#x2F;&#x2F; Protects metrics.\n\tmetrics   map[uint64][]metricWithLabelValues\n\tdesc      *Desc\n\tnewMetric func(labelValues ...string) Metric\n&#125;\n\ntype metricWithLabelValues struct &#123;\n\tvalues []string\n\tmetric Metric\n&#125;</code></pre>\n\n<h4><span id=\"gou-zao-metrics\">构造Metrics</span><a href=\"#gou-zao-metrics\" class=\"header-anchor\">#</a></h4><p>以Prometheus SDK client_golang为例，使用SDK构造Metrics的流程：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>声明不同类型Metric的Collector —&gt;  注册Collectors到本地Register —&gt;  暴露端口  —&gt;  数据存储到Map输出</p></blockquote>\n<p>Metric类型包括下面四种：</p>\n<ul>\n<li>Counter：计数类型指标，只增不减</li>\n</ul>\n<details class=\"custom-details\">\n<summary>Counter Struct</summary>\n<p>type counter struct {<br>    // valBits contains the bits of the represented float64 value, while<br>    // valInt stores values that are exact integers. Both have to go first<br>    // in the struct to guarantee alignment for atomic operations.<br>    // <a href=\"http://golang.org/pkg/sync/atomic/#pkg-note-BUG\">http://golang.org/pkg/sync/atomic/#pkg-note-BUG</a><br>    valBits uint64<br>    valInt  uint64</p>\n<pre><code>selfCollector\ndesc *Desc\n\nlabelPairs []*dto.LabelPair\nexemplar   atomic.Value // Containing nil or a *dto.Exemplar.\n\nnow func() time.Time // To mock out time.Now() for testing.</code></pre>\n<p>}</p>\n<p>type CounterVec struct {<br>    *MetricVec<br>}</p>\n<p>type Counter interface {<br>    Metric<br>    Collector</p>\n<pre><code>// Inc increments the counter by 1. Use Add to increment it by arbitrary\n// non-negative values.\nInc()\n// Add adds the given value to the counter. It panics if the value is &lt;\n// 0.\nAdd(float64)</code></pre>\n<p>}</p>\n</details>\n<ul>\n<li>Gauge：瞬时指标，主要反映瞬时状态</li>\n</ul>\n<details class=\"custom-details\">\n<summary>Gauge Struct</summary>\n<p>type gauge struct {<br>    // valBits contains the bits of the represented float64 value. It has<br>    // to go first in the struct to guarantee alignment for atomic<br>    // operations.  <a href=\"http://golang.org/pkg/sync/atomic/#pkg-note-BUG\">http://golang.org/pkg/sync/atomic/#pkg-note-BUG</a><br>    valBits uint64</p>\n<pre><code>selfCollector\n\ndesc       *Desc\nlabelPairs []*dto.LabelPair</code></pre>\n<p>}</p>\n<p>type GaugeVec struct {<br>    *MetricVec<br>}</p>\n<p>type Gauge interface {<br>    Metric<br>    Collector</p>\n<pre><code>// Set sets the Gauge to an arbitrary value.\nSet(float64)\n// Inc increments the Gauge by 1. Use Add to increment it by arbitrary\n// values.\nInc()\n// Dec decrements the Gauge by 1. Use Sub to decrement it by arbitrary\n// values.\nDec()\n// Add adds the given value to the Gauge. (The value can be negative,\n// resulting in a decrease of the Gauge.)\nAdd(float64)\n// Sub subtracts the given value from the Gauge. (The value can be\n// negative, resulting in an increase of the Gauge.)\nSub(float64)\n\n// SetToCurrentTime sets the Gauge to the current Unix time in seconds.\nSetToCurrentTime()</code></pre>\n<p>}</p>\n</details>\n<ul>\n<li>Histogram：直方图统计，对指标比较全面的描述， 包括sum和count及不同区间的统计，le标签标记</li>\n</ul>\n<details class=\"custom-details\">\n<summary>Histogram Struct</summary>\n<p>type histogram struct {<br>    // countAndHotIdx enables lock-free writes with use of atomic updates.<br>    // The most significant bit is the hot index [0 or 1] of the count field<br>    // below. Observe calls update the hot one. All remaining bits count the<br>    // number of Observe calls. Observe starts by incrementing this counter,<br>    // and finish by incrementing the count field in the respective<br>    // histogramCounts, as a marker for completion.<br>    //<br>    // Calls of the Write method (which are non-mutating reads from the<br>    // perspective of the histogram) swap the hot–cold under the writeMtx<br>    // lock. A cooldown is awaited (while locked) by comparing the number of<br>    // observations with the initiation count. Once they match, then the<br>    // last observation on the now cool one has completed. All cool fields must<br>    // be merged into the new hot before releasing writeMtx.<br>    //<br>    // Fields with atomic access first! See alignment constraint:<br>    // <a href=\"http://golang.org/pkg/sync/atomic/#pkg-note-BUG\">http://golang.org/pkg/sync/atomic/#pkg-note-BUG</a><br>    countAndHotIdx uint64</p>\n<pre><code>selfCollector\ndesc     *Desc\nwriteMtx sync.Mutex // Only used in the Write method.\n\n// Two counts, one is &quot;hot&quot; for lock-free observations, the other is\n// &quot;cold&quot; for writing out a dto.Metric. It has to be an array of\n// pointers to guarantee 64bit alignment of the histogramCounts, see\n// http://golang.org/pkg/sync/atomic/#pkg-note-BUG.\ncounts [2]*histogramCounts\n\nupperBounds []float64\nlabelPairs  []*dto.LabelPair\nexemplars   []atomic.Value // One more than buckets (to include +Inf), each a *dto.Exemplar.\n\nnow func() time.Time // To mock out time.Now() for testing.</code></pre>\n<p>}</p>\n<p>type HistogramOpts struct {<br>    // Namespace, Subsystem, and Name are components of the fully-qualified<br>    // name of the Histogram (created by joining these components with<br>    // “_”). Only Name is mandatory, the others merely help structuring the<br>    // name. Note that the fully-qualified name of the Histogram must be a<br>    // valid Prometheus metric name.<br>    Namespace string<br>    Subsystem string<br>    Name      string</p>\n<pre><code>// Help provides information about this Histogram.\n//\n// Metrics with the same fully-qualified name must have the same Help\n// string.\nHelp string\n\n// ConstLabels are used to attach fixed labels to this metric. Metrics\n// with the same fully-qualified name must have the same label names in\n// their ConstLabels.\n//\n// ConstLabels are only used rarely. In particular, do not use them to\n// attach the same labels to all your metrics. Those use cases are\n// better covered by target labels set by the scraping Prometheus\n// server, or by one specific metric (e.g. a build_info or a\n// machine_role metric). See also\n// https://prometheus.io/docs/instrumenting/writing_exporters/#target-labels-not-static-scraped-labels\nConstLabels Labels\n\n// Buckets defines the buckets into which observations are counted. Each\n// element in the slice is the upper inclusive bound of a bucket. The\n// values must be sorted in strictly increasing order. There is no need\n// to add a highest bucket with +Inf bound, it will be added\n// implicitly. The default value is DefBuckets.\nBuckets []float64</code></pre>\n<p>}</p>\n<p>type histogramCounts struct {<br>    // sumBits contains the bits of the float64 representing the sum of all<br>    // observations. sumBits and count have to go first in the struct to<br>    // guarantee alignment for atomic operations.<br>    // <a href=\"http://golang.org/pkg/sync/atomic/#pkg-note-BUG\">http://golang.org/pkg/sync/atomic/#pkg-note-BUG</a><br>    sumBits uint64<br>    count   uint64<br>    buckets []uint64<br>}</p>\n<p>type HistogramVec struct {<br>    *MetricVec<br>}</p>\n<p>type Histogram interface {<br>    Metric<br>    Collector</p>\n<pre><code>// Observe adds a single observation to the histogram. Observations are\n// usually positive or zero. Negative observations are accepted but\n// prevent current versions of Prometheus from properly detecting\n// counter resets in the sum of observations. See\n// https://prometheus.io/docs/practices/histograms/#count-and-sum-of-observations\n// for details.\nObserve(float64)</code></pre>\n<p>}</p>\n</details>\n<ul>\n<li>Summary：和Histogram类似，包括sum和count及不同分位值，quantile标记</li>\n</ul>\n<details class=\"custom-details\">\n<summary>Summary Struct</summary>\n<p>type summary struct {<br>    selfCollector</p>\n<pre><code>bufMtx sync.Mutex // Protects hotBuf and hotBufExpTime.\nmtx    sync.Mutex // Protects every other moving part.\n// Lock bufMtx before mtx if both are needed.\n\ndesc *Desc\n\nobjectives       map[float64]float64\nsortedObjectives []float64\n\nlabelPairs []*dto.LabelPair\n\nsum float64\ncnt uint64\n\nhotBuf, coldBuf []float64\n\nstreams                          []*quantile.Stream\nstreamDuration                   time.Duration\nheadStream                       *quantile.Stream\nheadStreamIdx                    int\nheadStreamExpTime, hotBufExpTime time.Time</code></pre>\n<p>}</p>\n<p>type SummaryVec struct {<br>    *MetricVec<br>}</p>\n<p>type SummaryOpts struct {<br>    // Namespace, Subsystem, and Name are components of the fully-qualified<br>    // name of the Summary (created by joining these components with<br>    // “_”). Only Name is mandatory, the others merely help structuring the<br>    // name. Note that the fully-qualified name of the Summary must be a<br>    // valid Prometheus metric name.<br>    Namespace string<br>    Subsystem string<br>    Name      string</p>\n<pre><code>// Help provides information about this Summary.\n//\n// Metrics with the same fully-qualified name must have the same Help\n// string.\nHelp string\n\n// ConstLabels are used to attach fixed labels to this metric. Metrics\n// with the same fully-qualified name must have the same label names in\n// their ConstLabels.\n//\n// Due to the way a Summary is represented in the Prometheus text format\n// and how it is handled by the Prometheus server internally, “quantile”\n// is an illegal label name. Construction of a Summary or SummaryVec\n// will panic if this label name is used in ConstLabels.\n//\n// ConstLabels are only used rarely. In particular, do not use them to\n// attach the same labels to all your metrics. Those use cases are\n// better covered by target labels set by the scraping Prometheus\n// server, or by one specific metric (e.g. a build_info or a\n// machine_role metric). See also\n// https://prometheus.io/docs/instrumenting/writing_exporters/#target-labels-not-static-scraped-labels\nConstLabels Labels\n\n// Objectives defines the quantile rank estimates with their respective\n// absolute error. If Objectives[q] = e, then the value reported for q\n// will be the φ-quantile value for some φ between q-e and q+e.  The\n// default value is an empty map, resulting in a summary without\n// quantiles.\nObjectives map[float64]float64\n\n// MaxAge defines the duration for which an observation stays relevant\n// for the summary. Only applies to pre-calculated quantiles, does not\n// apply to _sum and _count. Must be positive. The default value is\n// DefMaxAge.\nMaxAge time.Duration\n\n// AgeBuckets is the number of buckets used to exclude observations that\n// are older than MaxAge from the summary. A higher number has a\n// resource penalty, so only increase it if the higher resolution is\n// really required. For very high observation rates, you might want to\n// reduce the number of age buckets. With only one age bucket, you will\n// effectively see a complete reset of the summary each time MaxAge has\n// passed. The default value is DefAgeBuckets.\nAgeBuckets uint32\n\n// BufCap defines the default sample stream buffer size.  The default\n// value of DefBufCap should suffice for most uses. If there is a need\n// to increase the value, a multiple of 500 is recommended (because that\n// is the internal buffer size of the underlying package\n// &quot;github.com/bmizerany/perks/quantile&quot;).\nBufCap uint32</code></pre>\n<p>}</p>\n<p>type SummaryVec struct {<br>    *MetricVec<br>}</p>\n<p>type MetricVec struct {<br>    *metricMap</p>\n<pre><code>curry []curriedLabelValue\n\n// hashAdd and hashAddByte can be replaced for testing collision handling.\nhashAdd     func(h uint64, s string) uint64\nhashAddByte func(h uint64, b byte) uint64</code></pre>\n<p>}</p>\n<p>type metricMap struct {<br>    mtx       sync.RWMutex // Protects metrics.<br>    metrics   map[uint64][]metricWithLabelValues<br>    desc      *Desc<br>    newMetric func(labelValues …string) Metric<br>}</p>\n</details>\n<h3><span id=\"shu-ju-zhua-qu\">数据抓取</span><a href=\"#shu-ju-zhua-qu\" class=\"header-anchor\">#</a></h3><p>Prometheus抓取数据是通过配置不同的抓取任务进行的，我们先看看Prometheus抓取代码：</p>\n<details class=\"custom-details\">\n<summary>scrape.go</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">&#x2F;&#x2F; 新建抓取线程池\nfunc newScrapePool(cfg *config.ScrapeConfig, app storage.Appendable, jitterSeed uint64, logger log.Logger, reportExtraMetrics bool, httpOpts []config_util.HTTPClientOption) (*scrapePool, error) &#123;\n\ttargetScrapePools.Inc()\n\tif logger &#x3D;&#x3D; nil &#123;\n\t\tlogger &#x3D; log.NewNopLogger()\n\t&#125;\n\n    &#x2F;&#x2F; 构建抓取所用HTTP Client\n\tclient, err :&#x3D; config_util.NewClientFromConfig(cfg.HTTPClientConfig, cfg.JobName, httpOpts...)\n\tif err !&#x3D; nil &#123;\n\t\ttargetScrapePoolsFailed.Inc()\n\t\treturn nil, errors.Wrap(err, &quot;error creating HTTP client&quot;)\n\t&#125;\n\n\tbuffers :&#x3D; pool.New(1e3, 100e6, 3, func(sz int) interface&#123;&#125; &#123; return make([]byte, 0, sz) &#125;)\n\n\tctx, cancel :&#x3D; context.WithCancel(context.Background())\n    &#x2F;&#x2F; 构建一个抓取线程池\n\tsp :&#x3D; &amp;scrapePool&#123;\n\t\tcancel:        cancel,\n\t\tappendable:    app,\n\t\tconfig:        cfg,\n\t\tclient:        client,\n\t\tactiveTargets: map[uint64]*Target&#123;&#125;,\n\t\tloops:         map[uint64]loop&#123;&#125;,\n\t\tlogger:        logger,\n\t\thttpOpts:      httpOpts,\n\t&#125;\n\tsp.newLoop &#x3D; func(opts scrapeLoopOptions) loop &#123;\n\t\t&#x2F;&#x2F; Update the targets retrieval function for metadata to a new scrape cache.\n\t\tcache :&#x3D; opts.cache\n\t\tif cache &#x3D;&#x3D; nil &#123;\n\t\t\tcache &#x3D; newScrapeCache()\n\t\t&#125;\n\t\topts.target.SetMetadataStore(cache)\n\n\t\t&#x2F;&#x2F; Store the cache in the context.\n\t\tloopCtx :&#x3D; ContextWithMetricMetadataStore(ctx, cache)\n\t\tloopCtx &#x3D; ContextWithTarget(loopCtx, opts.target)\n\n        &#x2F;&#x2F; Loop执行抓取\n\t\treturn newScrapeLoop(\n\t\t\tloopCtx,\n\t\t\topts.scraper,\n\t\t\tlog.With(logger, &quot;target&quot;, opts.target),\n\t\t\tbuffers,\n\t\t\tfunc(l labels.Labels) labels.Labels &#123;\n\t\t\t\treturn mutateSampleLabels(l, opts.target, opts.honorLabels, opts.mrc)\n\t\t\t&#125;,\n\t\t\tfunc(l labels.Labels) labels.Labels &#123; return mutateReportSampleLabels(l, opts.target) &#125;,\n\t\t\tfunc(ctx context.Context) storage.Appender &#123; return app.Appender(ctx) &#125;,\n\t\t\tcache,\n\t\t\tjitterSeed,\n\t\t\topts.honorTimestamps,\n\t\t\topts.sampleLimit,\n\t\t\topts.labelLimits,\n\t\t\topts.interval,\n\t\t\topts.timeout,\n\t\t\treportExtraMetrics,\n\t\t)\n\t&#125;\n\n\treturn sp, nil\n&#125;\n\n&#x2F;&#x2F; 定时循环抓取数据\nfunc (sl *scrapeLoop) run(errc chan&lt;- error) &#123;\n\tselect &#123;\n\tcase &lt;-time.After(sl.scraper.offset(sl.interval, sl.jitterSeed)):\n\t\t&#x2F;&#x2F; Continue after a scraping offset.\n\tcase &lt;-sl.ctx.Done():\n\t\tclose(sl.stopped)\n\t\treturn\n\t&#125;\n\n\tvar last time.Time\n\n\talignedScrapeTime :&#x3D; time.Now().Round(0)\n    &#x2F;&#x2F; 根据配置抓取间隔\n\tticker :&#x3D; time.NewTicker(sl.interval)\n\tdefer ticker.Stop()\n\nmainLoop:\n\tfor &#123;\n\t\tselect &#123;\n\t\tcase &lt;-sl.parentCtx.Done():\n\t\t\tclose(sl.stopped)\n\t\t\treturn\n\t\tcase &lt;-sl.ctx.Done():\n\t\t\tbreak mainLoop\n\t\tdefault:\n\t\t&#125;\n\n\t\t&#x2F;&#x2F; Temporary workaround for a jitter in go timers that causes disk space\n\t\t&#x2F;&#x2F; increase in TSDB.\n\t\t&#x2F;&#x2F; See https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;issues&#x2F;7846\n\t\t&#x2F;&#x2F; Calling Round ensures the time used is the wall clock, as otherwise .Sub\n\t\t&#x2F;&#x2F; and .Add on time.Time behave differently (see time package docs).\n\t\tscrapeTime :&#x3D; time.Now().Round(0)\n\t\tif AlignScrapeTimestamps &amp;&amp; sl.interval &gt; 100*ScrapeTimestampTolerance &#123;\n\t\t\t&#x2F;&#x2F; For some reason, a tick might have been skipped, in which case we\n\t\t\t&#x2F;&#x2F; would call alignedScrapeTime.Add(interval) multiple times.\n\t\t\tfor scrapeTime.Sub(alignedScrapeTime) &gt;&#x3D; sl.interval &#123;\n\t\t\t\talignedScrapeTime &#x3D; alignedScrapeTime.Add(sl.interval)\n\t\t\t&#125;\n\t\t\t&#x2F;&#x2F; Align the scrape time if we are in the tolerance boundaries.\n\t\t\tif scrapeTime.Sub(alignedScrapeTime) &lt;&#x3D; ScrapeTimestampTolerance &#123;\n\t\t\t\tscrapeTime &#x3D; alignedScrapeTime\n\t\t\t&#125;\n\t\t&#125;\n\n\t\tlast &#x3D; sl.scrapeAndReport(last, scrapeTime, errc)\n\n\t\tselect &#123;\n\t\tcase &lt;-sl.parentCtx.Done():\n\t\t\tclose(sl.stopped)\n\t\t\treturn\n\t\tcase &lt;-sl.ctx.Done():\n\t\t\tbreak mainLoop\n\t\tcase &lt;-ticker.C:\n\t\t&#125;\n\t&#125;\n\n\tclose(sl.stopped)\n\n\tif !sl.disabledEndOfRunStalenessMarkers &#123;\n\t\tsl.endOfRunStaleness(last, ticker, sl.interval)\n\t&#125;\n&#125;\n\n&#x2F;&#x2F; 抓取Target数据\nfunc (s *targetScraper) scrape(ctx context.Context, w io.Writer) (string, error) &#123;\n\tif s.req &#x3D;&#x3D; nil &#123;\n\t\treq, err :&#x3D; http.NewRequest(&quot;GET&quot;, s.URL().String(), nil)\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn &quot;&quot;, err\n\t\t&#125;\n\t\treq.Header.Add(&quot;Accept&quot;, acceptHeader)\n\t\treq.Header.Add(&quot;Accept-Encoding&quot;, &quot;gzip&quot;)\n\t\treq.Header.Set(&quot;User-Agent&quot;, UserAgent)\n\t\treq.Header.Set(&quot;X-Prometheus-Scrape-Timeout-Seconds&quot;, strconv.FormatFloat(s.timeout.Seconds(), &#39;f&#39;, -1, 64))\n\n\t\ts.req &#x3D; req\n\t&#125;\n\n    &#x2F;&#x2F; 对Target发起HTTP请求\n\tresp, err :&#x3D; s.client.Do(s.req.WithContext(ctx))\n\tif err !&#x3D; nil &#123;\n\t\treturn &quot;&quot;, err\n\t&#125;\n\tdefer func() &#123;\n\t\tio.Copy(ioutil.Discard, resp.Body)\n\t\tresp.Body.Close()\n\t&#125;()\n\n\tif resp.StatusCode !&#x3D; http.StatusOK &#123;\n\t\treturn &quot;&quot;, errors.Errorf(&quot;server returned HTTP status %s&quot;, resp.Status)\n\t&#125;\n\n\tif s.bodySizeLimit &lt;&#x3D; 0 &#123;\n\t\ts.bodySizeLimit &#x3D; math.MaxInt64\n\t&#125;\n\tif resp.Header.Get(&quot;Content-Encoding&quot;) !&#x3D; &quot;gzip&quot; &#123;\n\t\tn, err :&#x3D; io.Copy(w, io.LimitReader(resp.Body, s.bodySizeLimit))\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn &quot;&quot;, err\n\t\t&#125;\n\t\tif n &gt;&#x3D; s.bodySizeLimit &#123;\n\t\t\ttargetScrapeExceededBodySizeLimit.Inc()\n\t\t\treturn &quot;&quot;, errBodySizeLimit\n\t\t&#125;\n\t\treturn resp.Header.Get(&quot;Content-Type&quot;), nil\n\t&#125;\n\n\tif s.gzipr &#x3D;&#x3D; nil &#123;\n\t\ts.buf &#x3D; bufio.NewReader(resp.Body)\n\t\ts.gzipr, err &#x3D; gzip.NewReader(s.buf)\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn &quot;&quot;, err\n\t\t&#125;\n\t&#125; else &#123;\n\t\ts.buf.Reset(resp.Body)\n\t\tif err &#x3D; s.gzipr.Reset(s.buf); err !&#x3D; nil &#123;\n\t\t\treturn &quot;&quot;, err\n\t\t&#125;\n\t&#125;\n\n\tn, err :&#x3D; io.Copy(w, io.LimitReader(s.gzipr, s.bodySizeLimit))\n\ts.gzipr.Close()\n\tif err !&#x3D; nil &#123;\n\t\treturn &quot;&quot;, err\n\t&#125;\n\tif n &gt;&#x3D; s.bodySizeLimit &#123;\n\t\ttargetScrapeExceededBodySizeLimit.Inc()\n\t\treturn &quot;&quot;, errBodySizeLimit\n\t&#125;\n\treturn resp.Header.Get(&quot;Content-Type&quot;), nil\n&#125;</code></pre>\n</p>\n</details>\n<h4><span id=\"job\">Job</span><a href=\"#job\" class=\"header-anchor\">#</a></h4><p>Prometheus抓取数据是通过定义一组Scrape采集任务来进行的，一个Scrape配置多个Job任务，Job是多个相同Target的实例的组合</p>\n<h4><span id=\"target\">Target</span><a href=\"#target\" class=\"header-anchor\">#</a></h4><p>Target是一个具体采集实例，包含多个Metric</p>\n<h4><span id=\"cai-ji-fang-shi\">采集方式</span><a href=\"#cai-ji-fang-shi\" class=\"header-anchor\">#</a></h4><p>监控数据的采集方式分为Pull和Push两种类型，官方推荐Pull类型。</p>\n<ul>\n<li>Pull：以Prometheus为代表的采集方式，采用Pull类型拉取监控数据，通过配置不同的定时任务以及需要拉取的实例列表，向指定的地址发起HTTP请求拉取监控数据并存储</li>\n<li>Push：以Telegraf和Pushgateway为代表的采集方式，Telegraf的Input插件负责采集监控指标，Output插件负责将监控数据推送到指定的TSDB，而Pushgateway，是监控程序主动推送到Pushgateway，然后由Prometheus去Pushgateway拉取监控数据</li>\n</ul>\n<h4><span id=\"fu-wu-fa-xian\">服务发现</span><a href=\"#fu-wu-fa-xian\" class=\"header-anchor\">#</a></h4><p>这是一种非常高效的采集方式，程序将自己的监控地址注册到服务注册中心，如Consul，Eureka等，然后Prometheus配置对应的采集任务，定时去拉取服务注册中心注册的监控地址，生成对应的Target进行数据来取</p>\n<h2><span id=\"shu-ju-cun-chu\">数据存储</span><a href=\"#shu-ju-cun-chu\" class=\"header-anchor\">#</a></h2><p>Prometheus支持本地存储，同时Prometheus还具备将Sample远程写入TSDB和读取远程TSDB数据的能力。  </p>\n<h3><span id=\"remotewrite\">RemoteWrite</span><a href=\"#remotewrite\" class=\"header-anchor\">#</a></h3><p>Prometheus配置RemoteWrite</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">remote_write:\n- url: https:&#x2F;&#x2F;xxxxx&#x2F;api&#x2F;monitor&#x2F;v1&#x2F;prom&#x2F;write\n  remote_timeout: 30s\n  queue_config:\n    capacity: 500\n    max_shards: 1000\n    min_shards: 1\n    max_samples_per_send: 100\n    batch_send_deadline: 5s\n    min_backoff: 30ms\n    max_backoff: 100ms</code></pre>\n<p>Prometheus远程写入以队列为单位，每个队列包含多个shard，每个shard包含多个Sample，下面是计算公式：<br><code>单次写入Sample数量=单次Shard数量*单个Shard容量Capacity</code><br>Prometheus远程写入过程：<br><code>Samples ==&gt; Protobuf序列号 ==&gt; Snappy压缩 ==&gt; TSDB</code></p>\n<details class=\"custom-details\">\n<summary>remotewrite</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">func (t *QueueManager) sendMetadataWithBackoff(ctx context.Context, metadata []prompb.MetricMetadata) error &#123;\n\t&#x2F;&#x2F; Build the WriteRequest with no samples.\n\treq, _, err :&#x3D; buildWriteRequest(nil, metadata, nil)\n\tif err !&#x3D; nil &#123;\n\t\treturn err\n\t&#125;\n\n\tmetadataCount :&#x3D; len(metadata)\n\n\tattemptStore :&#x3D; func(try int) error &#123;\n\t\tspan, ctx :&#x3D; opentracing.StartSpanFromContext(ctx, &quot;Remote Metadata Send Batch&quot;)\n\t\tdefer span.Finish()\n\n\t\tspan.SetTag(&quot;metadata&quot;, metadataCount)\n\t\tspan.SetTag(&quot;try&quot;, try)\n\t\tspan.SetTag(&quot;remote_name&quot;, t.storeClient.Name())\n\t\tspan.SetTag(&quot;remote_url&quot;, t.storeClient.Endpoint())\n\n\t\tbegin :&#x3D; time.Now()\n\t\terr :&#x3D; t.storeClient.Store(ctx, req)\n\t\tt.metrics.sentBatchDuration.Observe(time.Since(begin).Seconds())\n\n\t\tif err !&#x3D; nil &#123;\n\t\t\tspan.LogKV(&quot;error&quot;, err)\n\t\t\text.Error.Set(span, true)\n\t\t\treturn err\n\t\t&#125;\n\n\t\treturn nil\n\t&#125;\n\n\tretry :&#x3D; func() &#123;\n\t\tt.metrics.retriedMetadataTotal.Add(float64(len(metadata)))\n\t&#125;\n\terr &#x3D; sendWriteRequestWithBackoff(ctx, t.cfg, t.logger, attemptStore, retry)\n\tif err !&#x3D; nil &#123;\n\t\treturn err\n\t&#125;\n\tt.metrics.metadataTotal.Add(float64(len(metadata)))\n\tt.metrics.metadataBytesTotal.Add(float64(len(req)))\n\treturn nil\n&#125;\n\nfunc buildWriteRequest(samples []prompb.TimeSeries, metadata []prompb.MetricMetadata, buf []byte) ([]byte, int64, error) &#123;\n\tvar highest int64\n\tfor _, ts :&#x3D; range samples &#123;\n\t\t&#x2F;&#x2F; At the moment we only ever append a TimeSeries with a single sample or exemplar in it.\n\t\tif len(ts.Samples) &gt; 0 &amp;&amp; ts.Samples[0].Timestamp &gt; highest &#123;\n\t\t\thighest &#x3D; ts.Samples[0].Timestamp\n\t\t&#125;\n\t\tif len(ts.Exemplars) &gt; 0 &amp;&amp; ts.Exemplars[0].Timestamp &gt; highest &#123;\n\t\t\thighest &#x3D; ts.Exemplars[0].Timestamp\n\t\t&#125;\n\t&#125;\n\n\treq :&#x3D; &amp;prompb.WriteRequest&#123;\n\t\tTimeseries: samples,\n\t\tMetadata:   metadata,\n\t&#125;\n\n\tdata, err :&#x3D; proto.Marshal(req)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, highest, err\n\t&#125;\n\n\t&#x2F;&#x2F; snappy uses len() to see if it needs to allocate a new slice. Make the\n\t&#x2F;&#x2F; buffer as long as possible.\n\tif buf !&#x3D; nil &#123;\n\t\tbuf &#x3D; buf[0:cap(buf)]\n\t&#125;\n\tcompressed :&#x3D; snappy.Encode(buf, data)\n\treturn compressed, highest, nil\n&#125;</code></pre>\n</p>\n</details>\n<p>TSDB实现写入接口，以VictoriaMetrics为例</p>\n<details class=\"custom-details\">\n<summary>insertRows</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">func insertRows(timeseries []prompb.TimeSeries, extraLabels []prompbmarshal.Label) error &#123;\n\tctx :&#x3D; common.GetInsertCtx()\n\tdefer common.PutInsertCtx(ctx)\n\n\trowsLen :&#x3D; 0\n\tfor i :&#x3D; range timeseries &#123;\n\t\trowsLen +&#x3D; len(timeseries[i].Samples)\n\t&#125;\n\tctx.Reset(rowsLen)\n\trowsTotal :&#x3D; 0\n\thasRelabeling :&#x3D; relabel.HasRelabeling()\n\tfor i :&#x3D; range timeseries &#123;\n\t\tts :&#x3D; ×eries[i]\n\t\trowsTotal +&#x3D; len(ts.Samples)\n\t\tctx.Labels &#x3D; ctx.Labels[:0]\n\t\tsrcLabels :&#x3D; ts.Labels\n\t\tfor _, srcLabel :&#x3D; range srcLabels &#123;\n\t\t\tctx.AddLabelBytes(srcLabel.Name, srcLabel.Value)\n\t\t&#125;\n\t\tfor j :&#x3D; range extraLabels &#123;\n\t\t\tlabel :&#x3D; &amp;extraLabels[j]\n\t\t\tctx.AddLabel(label.Name, label.Value)\n\t\t&#125;\n\t\tif hasRelabeling &#123;\n\t\t\tctx.ApplyRelabeling()\n\t\t&#125;\n\t\tif len(ctx.Labels) &#x3D;&#x3D; 0 &#123;\n\t\t\t&#x2F;&#x2F; Skip metric without labels.\n\t\t\tcontinue\n\t\t&#125;\n\t\tctx.SortLabelsIfNeeded()\n\t\tvar metricNameRaw []byte\n\t\tvar err error\n\t\tsamples :&#x3D; ts.Samples\n\t\tfor i :&#x3D; range samples &#123;\n\t\t\tr :&#x3D; &amp;samples[i]\n\t\t\tmetricNameRaw, err &#x3D; ctx.WriteDataPointExt(metricNameRaw, ctx.Labels, r.Timestamp, r.Value)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn err\n\t\t\t&#125;\n\t\t&#125;\n\t&#125;\n\trowsInserted.Add(rowsTotal)\n\trowsPerInsert.Update(float64(rowsTotal))\n\treturn ctx.FlushBufs()\n&#125;\n\nfunc (ctx *InsertCtx) WriteDataPointExt(metricNameRaw []byte, labels []prompb.Label, timestamp int64, value float64) ([]byte, error) &#123;\n\tif len(metricNameRaw) &#x3D;&#x3D; 0 &#123;\n\t\tmetricNameRaw &#x3D; ctx.marshalMetricNameRaw(nil, labels)\n\t&#125;\n\terr :&#x3D; ctx.addRow(metricNameRaw, timestamp, value)\n\treturn metricNameRaw, err\n&#125;\n\nfunc (ctx *InsertCtx) addRow(metricNameRaw []byte, timestamp int64, value float64) error &#123;\n\tmrs :&#x3D; ctx.mrs\n\tif cap(mrs) &gt; len(mrs) &#123;\n\t\tmrs &#x3D; mrs[:len(mrs)+1]\n\t&#125; else &#123;\n\t\tmrs &#x3D; append(mrs, storage.MetricRow&#123;&#125;)\n\t&#125;\n\tmr :&#x3D; &amp;mrs[len(mrs)-1]\n\tctx.mrs &#x3D; mrs\n\tmr.MetricNameRaw &#x3D; metricNameRaw\n\tmr.Timestamp &#x3D; timestamp\n\tmr.Value &#x3D; value\n\tif len(ctx.metricNamesBuf) &gt; 16*1024*1024 &#123;\n\t\tif err :&#x3D; ctx.FlushBufs(); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t&#125;\n\t\n    return nil\n&#125;</code></pre>\n</p>\n</details>\n<h3><span id=\"remoteread\">RemoteRead</span><a href=\"#remoteread\" class=\"header-anchor\">#</a></h3><p>远程读取流程和写入相反</p>\n<details class=\"custom-details\">\n<summary>remoterRead</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">func (h *readHandler) remoteReadSamples(\n\tctx context.Context,\n\tw http.ResponseWriter,\n\treq *prompb.ReadRequest,\n\texternalLabels map[string]string,\n\tsortedExternalLabels []prompb.Label,\n) &#123;\n\tw.Header().Set(&quot;Content-Type&quot;, &quot;application&#x2F;x-protobuf&quot;)\n\tw.Header().Set(&quot;Content-Encoding&quot;, &quot;snappy&quot;)\n\n\tresp :&#x3D; prompb.ReadResponse&#123;\n\t\tResults: make([]*prompb.QueryResult, len(req.Queries)),\n\t&#125;\n\tfor i, query :&#x3D; range req.Queries &#123;\n\t\tif err :&#x3D; func() error &#123;\n\t\t\tfilteredMatchers, err :&#x3D; filterExtLabelsFromMatchers(query.Matchers, externalLabels)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn err\n\t\t\t&#125;\n\n\t\t\tquerier, err :&#x3D; h.queryable.Querier(ctx, query.StartTimestampMs, query.EndTimestampMs)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn err\n\t\t\t&#125;\n\t\t\tdefer func() &#123;\n\t\t\t\tif err :&#x3D; querier.Close(); err !&#x3D; nil &#123;\n\t\t\t\t\tlevel.Warn(h.logger).Log(&quot;msg&quot;, &quot;Error on querier close&quot;, &quot;err&quot;, err.Error())\n\t\t\t\t&#125;\n\t\t\t&#125;()\n\n\t\t\tvar hints *storage.SelectHints\n\t\t\tif query.Hints !&#x3D; nil &#123;\n\t\t\t\thints &#x3D; &amp;storage.SelectHints&#123;\n\t\t\t\t\tStart:    query.Hints.StartMs,\n\t\t\t\t\tEnd:      query.Hints.EndMs,\n\t\t\t\t\tStep:     query.Hints.StepMs,\n\t\t\t\t\tFunc:     query.Hints.Func,\n\t\t\t\t\tGrouping: query.Hints.Grouping,\n\t\t\t\t\tRange:    query.Hints.RangeMs,\n\t\t\t\t\tBy:       query.Hints.By,\n\t\t\t\t&#125;\n\t\t\t&#125;\n\n\t\t\tvar ws storage.Warnings\n\t\t\tresp.Results[i], ws, err &#x3D; ToQueryResult(querier.Select(false, hints, filteredMatchers...), h.remoteReadSampleLimit)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn err\n\t\t\t&#125;\n\t\t\tfor _, w :&#x3D; range ws &#123;\n\t\t\t\tlevel.Warn(h.logger).Log(&quot;msg&quot;, &quot;Warnings on remote read query&quot;, &quot;err&quot;, w.Error())\n\t\t\t&#125;\n\t\t\tfor _, ts :&#x3D; range resp.Results[i].Timeseries &#123;\n\t\t\t\tts.Labels &#x3D; MergeLabels(ts.Labels, sortedExternalLabels)\n\t\t\t&#125;\n\t\t\treturn nil\n\t\t&#125;(); err !&#x3D; nil &#123;\n\t\t\tif httpErr, ok :&#x3D; err.(HTTPError); ok &#123;\n\t\t\t\thttp.Error(w, httpErr.Error(), httpErr.Status())\n\t\t\t\treturn\n\t\t\t&#125;\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t&#125;\n\t&#125;\n\n\tif err :&#x3D; EncodeReadResponse(&amp;resp, w); err !&#x3D; nil &#123;\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t&#125;\n&#125;</code></pre>\n</p>\n</details>\n<h3><span id=\"shu-ju-cun-chu\">数据存储</span><a href=\"#shu-ju-cun-chu\" class=\"header-anchor\">#</a></h3><p>Prometheus在进行数据存储的时候并不是实时写入，而是在达到设置的最大缓存后才会落盘，使得单个时间序列的一段Chunk在存储上相邻，从而允许较快的横向读取时序数据，并对单个Chunk进行压缩，节省存储和计算成本。<br>在V2架构中，还更改为俩小时落一个Block，存储着两小时内所有Chunk和索引</p>\n<details class=\"custom-details\">\n<summary>FlushWAL</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">const (\n\t&#x2F;&#x2F; Default duration of a block in milliseconds.\n\tDefaultBlockDuration &#x3D; int64(2 * time.Hour &#x2F; time.Millisecond)\n\n\t......\n)\n\n&#x2F;&#x2F; FlushWAL creates a new block containing all data that&#39;s currently in the memory buffer&#x2F;WAL.\n&#x2F;&#x2F; Samples that are in existing blocks will not be written to the new block.\n&#x2F;&#x2F; Note that if the read only database is running concurrently with a\n&#x2F;&#x2F; writable database then writing the WAL to the database directory can race.\nfunc (db *DBReadOnly) FlushWAL(dir string) (returnErr error) &#123;\n\tblockReaders, err :&#x3D; db.Blocks()\n\tif err !&#x3D; nil &#123;\n\t\treturn errors.Wrap(err, &quot;read blocks&quot;)\n\t&#125;\n\tmaxBlockTime :&#x3D; int64(math.MinInt64)\n\tif len(blockReaders) &gt; 0 &#123;\n\t\tmaxBlockTime &#x3D; blockReaders[len(blockReaders)-1].Meta().MaxTime\n\t&#125;\n\tw, err :&#x3D; wal.Open(db.logger, filepath.Join(db.dir, &quot;wal&quot;))\n\tif err !&#x3D; nil &#123;\n\t\treturn err\n\t&#125;\n\topts :&#x3D; DefaultHeadOptions()\n\topts.ChunkDirRoot &#x3D; db.dir\n\thead, err :&#x3D; NewHead(nil, db.logger, w, opts, NewHeadStats())\n\tif err !&#x3D; nil &#123;\n\t\treturn err\n\t&#125;\n\tdefer func() &#123;\n\t\treturnErr &#x3D; tsdb_errors.NewMulti(\n\t\t\treturnErr,\n\t\t\terrors.Wrap(head.Close(), &quot;closing Head&quot;),\n\t\t).Err()\n\t&#125;()\n\t&#x2F;&#x2F; Set the min valid time for the ingested wal samples\n\t&#x2F;&#x2F; to be no lower than the maxt of the last block.\n\tif err :&#x3D; head.Init(maxBlockTime); err !&#x3D; nil &#123;\n\t\treturn errors.Wrap(err, &quot;read WAL&quot;)\n\t&#125;\n\tmint :&#x3D; head.MinTime()\n\tmaxt :&#x3D; head.MaxTime()\n\trh :&#x3D; NewRangeHead(head, mint, maxt)\n\tcompactor, err :&#x3D; NewLeveledCompactor(\n\t\tcontext.Background(),\n\t\tnil,\n\t\tdb.logger,\n\t\tExponentialBlockRanges(DefaultOptions().MinBlockDuration, 3, 5),\n\t\tchunkenc.NewPool(),\n\t\tnil,\n\t)\n\tif err !&#x3D; nil &#123;\n\t\treturn errors.Wrap(err, &quot;create leveled compactor&quot;)\n\t&#125;\n\t&#x2F;&#x2F; Add +1 millisecond to block maxt because block intervals are half-open: [b.MinTime, b.MaxTime).\n\t&#x2F;&#x2F; Because of this block intervals are always +1 than the total samples it includes.\n\t_, err &#x3D; compactor.Write(dir, rh, mint, maxt+1, nil)\n\treturn errors.Wrap(err, &quot;writing WAL&quot;)\n&#125;</code></pre>\n</p>\n</details>\n<p>Chunk写入</p>\n<details class=\"custom-details\">\n<summary>WriteChunks</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">&#x2F;&#x2F; WriteChunks writes as many chunks as possible to the current segment,\n&#x2F;&#x2F; cuts a new segment when the current segment is full and\n&#x2F;&#x2F; writes the rest of the chunks in the new segment.\nfunc (w *Writer) WriteChunks(chks ...Meta) error &#123;\n\tvar (\n\t\tbatchSize  &#x3D; int64(0)\n\t\tbatchStart &#x3D; 0\n\t\tbatches    &#x3D; make([][]Meta, 1)\n\t\tbatchID    &#x3D; 0\n\t\tfirstBatch &#x3D; true\n\t)\n\n\tfor i, chk :&#x3D; range chks &#123;\n\t\t&#x2F;&#x2F; Each chunk contains: data length + encoding + the data itself + crc32\n\t\tchkSize :&#x3D; int64(MaxChunkLengthFieldSize) &#x2F;&#x2F; The data length is a variable length field so use the maximum possible value.\n\t\tchkSize +&#x3D; ChunkEncodingSize              &#x2F;&#x2F; The chunk encoding.\n\t\tchkSize +&#x3D; int64(len(chk.Chunk.Bytes()))  &#x2F;&#x2F; The data itself.\n\t\tchkSize +&#x3D; crc32.Size                     &#x2F;&#x2F; The 4 bytes of crc32.\n\t\tbatchSize +&#x3D; chkSize\n\n\t\t&#x2F;&#x2F; Cut a new batch when it is not the first chunk(to avoid empty segments) and\n\t\t&#x2F;&#x2F; the batch is too large to fit in the current segment.\n\t\tcutNewBatch :&#x3D; (i !&#x3D; 0) &amp;&amp; (batchSize+SegmentHeaderSize &gt; w.segmentSize)\n\n\t\t&#x2F;&#x2F; When the segment already has some data than\n\t\t&#x2F;&#x2F; the first batch size calculation should account for that.\n\t\tif firstBatch &amp;&amp; w.n &gt; SegmentHeaderSize &#123;\n\t\t\tcutNewBatch &#x3D; batchSize+w.n &gt; w.segmentSize\n\t\t\tif cutNewBatch &#123;\n\t\t\t\tfirstBatch &#x3D; false\n\t\t\t&#125;\n\t\t&#125;\n\n\t\tif cutNewBatch &#123;\n\t\t\tbatchStart &#x3D; i\n\t\t\tbatches &#x3D; append(batches, []Meta&#123;&#125;)\n\t\t\tbatchID++\n\t\t\tbatchSize &#x3D; chkSize\n\t\t&#125;\n\t\tbatches[batchID] &#x3D; chks[batchStart : i+1]\n\t&#125;\n\n\t&#x2F;&#x2F; Create a new segment when one doesn&#39;t already exist.\n\tif w.n &#x3D;&#x3D; 0 &#123;\n\t\tif err :&#x3D; w.cut(); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t&#125;\n\n\tfor i, chks :&#x3D; range batches &#123;\n\t\tif err :&#x3D; w.writeChunks(chks); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t\t&#x2F;&#x2F; Cut a new segment only when there are more chunks to write.\n\t\t&#x2F;&#x2F; Avoid creating a new empty segment at the end of the write.\n\t\tif i &lt; len(batches)-1 &#123;\n\t\t\tif err :&#x3D; w.cut(); err !&#x3D; nil &#123;\n\t\t\t\treturn err\n\t\t\t&#125;\n\t\t&#125;\n\t&#125;\n\treturn nil\n&#125;\n\n&#x2F;&#x2F; writeChunks writes the chunks into the current segment irrespective\n&#x2F;&#x2F; of the configured segment size limit. A segment should have been already\n&#x2F;&#x2F; started before calling this.\nfunc (w *Writer) writeChunks(chks []Meta) error &#123;\n\tif len(chks) &#x3D;&#x3D; 0 &#123;\n\t\treturn nil\n\t&#125;\n\n\tvar seq &#x3D; uint64(w.seq()) &lt;&lt; 32\n\tfor i :&#x3D; range chks &#123;\n\t\tchk :&#x3D; &amp;chks[i]\n\n\t\t&#x2F;&#x2F; The reference is set to the segment index and the offset where\n\t\t&#x2F;&#x2F; the data starts for this chunk.\n\t\t&#x2F;&#x2F;\n\t\t&#x2F;&#x2F; The upper 4 bytes are for the segment index and\n\t\t&#x2F;&#x2F; the lower 4 bytes are for the segment offset where to start reading this chunk.\n\t\tchk.Ref &#x3D; seq | uint64(w.n)\n\n\t\tn :&#x3D; binary.PutUvarint(w.buf[:], uint64(len(chk.Chunk.Bytes())))\n\n\t\tif err :&#x3D; w.write(w.buf[:n]); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t\tw.buf[0] &#x3D; byte(chk.Chunk.Encoding())\n\t\tif err :&#x3D; w.write(w.buf[:1]); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t\tif err :&#x3D; w.write(chk.Chunk.Bytes()); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\n\t\tw.crc32.Reset()\n\t\tif err :&#x3D; chk.writeHash(w.crc32, w.buf[:]); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t\tif err :&#x3D; w.write(w.crc32.Sum(w.buf[:0])); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t&#125;\n\treturn nil\n&#125;</code></pre>\n</p>\n</details>\n<p>数据压缩</p>\n<details class=\"custom-details\">\n<summary>Compact</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">&#x2F;&#x2F; Compact creates a new block in the compactor&#39;s directory from the blocks in the\n&#x2F;&#x2F; provided directories.\nfunc (c *LeveledCompactor) Compact(dest string, dirs []string, open []*Block) (uid ulid.ULID, err error) &#123;\n\tvar (\n\t\tblocks []BlockReader\n\t\tbs     []*Block\n\t\tmetas  []*BlockMeta\n\t\tuids   []string\n\t)\n\tstart :&#x3D; time.Now()\n\n\tfor _, d :&#x3D; range dirs &#123;\n\t\tmeta, _, err :&#x3D; readMetaFile(d)\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn uid, err\n\t\t&#125;\n\n\t\tvar b *Block\n\n\t\t&#x2F;&#x2F; Use already open blocks if we can, to avoid\n\t\t&#x2F;&#x2F; having the index data in memory twice.\n\t\tfor _, o :&#x3D; range open &#123;\n\t\t\tif meta.ULID &#x3D;&#x3D; o.Meta().ULID &#123;\n\t\t\t\tb &#x3D; o\n\t\t\t\tbreak\n\t\t\t&#125;\n\t\t&#125;\n\n\t\tif b &#x3D;&#x3D; nil &#123;\n\t\t\tvar err error\n\t\t\tb, err &#x3D; OpenBlock(c.logger, d, c.chunkPool)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn uid, err\n\t\t\t&#125;\n\t\t\tdefer b.Close()\n\t\t&#125;\n\n\t\tmetas &#x3D; append(metas, meta)\n\t\tblocks &#x3D; append(blocks, b)\n\t\tbs &#x3D; append(bs, b)\n\t\tuids &#x3D; append(uids, meta.ULID.String())\n\t&#125;\n\n\tuid &#x3D; ulid.MustNew(ulid.Now(), rand.Reader)\n\n\tmeta :&#x3D; CompactBlockMetas(uid, metas...)\n\terr &#x3D; c.write(dest, meta, blocks...)\n\tif err &#x3D;&#x3D; nil &#123;\n\t\tif meta.Stats.NumSamples &#x3D;&#x3D; 0 &#123;\n\t\t\tfor _, b :&#x3D; range bs &#123;\n\t\t\t\tb.meta.Compaction.Deletable &#x3D; true\n\t\t\t\tn, err :&#x3D; writeMetaFile(c.logger, b.dir, &amp;b.meta)\n\t\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\t\tlevel.Error(c.logger).Log(\n\t\t\t\t\t\t&quot;msg&quot;, &quot;Failed to write &#39;Deletable&#39; to meta file after compaction&quot;,\n\t\t\t\t\t\t&quot;ulid&quot;, b.meta.ULID,\n\t\t\t\t\t)\n\t\t\t\t&#125;\n\t\t\t\tb.numBytesMeta &#x3D; n\n\t\t\t&#125;\n\t\t\tuid &#x3D; ulid.ULID&#123;&#125;\n\t\t\tlevel.Info(c.logger).Log(\n\t\t\t\t&quot;msg&quot;, &quot;compact blocks resulted in empty block&quot;,\n\t\t\t\t&quot;count&quot;, len(blocks),\n\t\t\t\t&quot;sources&quot;, fmt.Sprintf(&quot;%v&quot;, uids),\n\t\t\t\t&quot;duration&quot;, time.Since(start),\n\t\t\t)\n\t\t&#125; else &#123;\n\t\t\tlevel.Info(c.logger).Log(\n\t\t\t\t&quot;msg&quot;, &quot;compact blocks&quot;,\n\t\t\t\t&quot;count&quot;, len(blocks),\n\t\t\t\t&quot;mint&quot;, meta.MinTime,\n\t\t\t\t&quot;maxt&quot;, meta.MaxTime,\n\t\t\t\t&quot;ulid&quot;, meta.ULID,\n\t\t\t\t&quot;sources&quot;, fmt.Sprintf(&quot;%v&quot;, uids),\n\t\t\t\t&quot;duration&quot;, time.Since(start),\n\t\t\t)\n\t\t&#125;\n\t\treturn uid, nil\n\t&#125;\n\n\terrs :&#x3D; tsdb_errors.NewMulti(err)\n\tif err !&#x3D; context.Canceled &#123;\n\t\tfor _, b :&#x3D; range bs &#123;\n\t\t\tif err :&#x3D; b.setCompactionFailed(); err !&#x3D; nil &#123;\n\t\t\t\terrs.Add(errors.Wrapf(err, &quot;setting compaction failed for block: %s&quot;, b.Dir()))\n\t\t\t&#125;\n\t\t&#125;\n\t&#125;\n\n\treturn uid, errs.Err()\n&#125;</code></pre>\n</p>\n</details>\n<p>监控数据目录结构如下： </p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">.\n├── 01FQ6T1CN7SJK7AAXEVAD7W51J\n│   ├── chunks\n│   │   └── 000001\n│   ├── index\n│   ├── meta.json\n│   └── tombstones\n├── chunks_head\n│   ├── 002333\n│   └── 002334\n├── lock\n├── queries.active\n└── wal\n    ├── 00009654\n    ├── 00009655\n    ├── 00009656\n    ├── 00009657\n    └── checkpoint.00009653\n        └── 00000000\n\nmeta.json \n&#123;\n\t&quot;ulid&quot;: &quot;01FQ6T1CN7SJK7AAXEVAD7W51J&quot;,\n\t&quot;minTime&quot;: 1639821600349,\n\t&quot;maxTime&quot;: 1639828800000,\n\t&quot;stats&quot;: &#123;\n\t\t&quot;numSamples&quot;: 17738865,\n\t\t&quot;numSeries&quot;: 147825,\n\t\t&quot;numChunks&quot;: 147825\n\t&#125;,\n\t&quot;compaction&quot;: &#123;\n\t\t&quot;level&quot;: 1,\n\t\t&quot;sources&quot;: [\n\t\t\t&quot;01FQ6T1CN7SJK7AAXEVAD7W51J&quot;\n\t\t]\n\t&#125;,\n\t&quot;version&quot;: 1\n&#125;</code></pre>\n\n<h4><span id=\"cun-chu-jie-gou\">存储结构</span><a href=\"#cun-chu-jie-gou\" class=\"header-anchor\">#</a></h4><p><img src=\"../images/Promtheus%E6%95%B0%E6%8D%AE%E6%B5%81/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84.jpg\" alt=\"存储结构\"></p>\n<h4><span id=\"shu-ju-fen-xi\">数据分析</span><a href=\"#shu-ju-fen-xi\" class=\"header-anchor\">#</a></h4><p>我们可以使用promtool来分析监控数据，执行以下命令：<br><code>promtool tsdb analyze /data/ 01FXSQS1WSJJ8R909EPN9DZ0FE</code><br><img src=\"../images/Promtheus%E6%95%B0%E6%8D%AE%E6%B5%81/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.png\" alt=\"存储结构\"></p>\n<h2><span id=\"bao-jing\">报警</span><a href=\"#bao-jing\" class=\"header-anchor\">#</a></h2><p>Prometheus根据配置的查询规则，每间隔一定时间进行评估，由参数evaluation_interval决定，当第一次触发报警，会进入“PENDING”状态，并记录当前active的时间，下一个周期如果报警条件依然成立，这时会判断rule中报警持续时间”for“，如果active时间超过了持续时间，则alert的状态变为“FIRING”；同时调用Alertmanager接口，发送相关报警数据。<br><code>Evaluation Rule -&gt; Active -&gt; PENDING -&gt; Large than for -&gt; FIRING -&gt; Send to Alertmanager</code></p>\n<details class=\"custom-details\">\n<summary>Alert</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">&#x2F;&#x2F; 检查规则\nfunc (g *Group) Eval(ctx context.Context, ts time.Time) &#123;\n\tvar samplesTotal float64\n    &#x2F;&#x2F; 遍历规则列表\n\tfor i, rule :&#x3D; range g.rules &#123;\n\t\tselect &#123;\n\t\tcase &lt;-g.done:\n\t\t\treturn\n\t\tdefault:\n\t\t&#125;\n        &#x2F;&#x2F; 检查规则\n\t\tfunc(i int, rule Rule) &#123;\n\t\t\tctx, sp :&#x3D; otel.Tracer(&quot;&quot;).Start(ctx, &quot;rule&quot;)\n\t\t\tsp.SetAttributes(attribute.String(&quot;name&quot;, rule.Name()))\n\t\t\tdefer func(t time.Time) &#123;\n\t\t\t\tsp.End()\n\n\t\t\t\tsince :&#x3D; time.Since(t)\n\t\t\t\tg.metrics.EvalDuration.Observe(since.Seconds())\n\t\t\t\trule.SetEvaluationDuration(since)\n\t\t\t\trule.SetEvaluationTimestamp(t)\n\t\t\t&#125;(time.Now())\n\n\t\t\tg.metrics.EvalTotal.WithLabelValues(GroupKey(g.File(), g.Name())).Inc()\n\n\t\t\tvector, err :&#x3D; rule.Eval(ctx, ts, g.opts.QueryFunc, g.opts.ExternalURL, g.Limit())\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\trule.SetHealth(HealthBad)\n\t\t\t\trule.SetLastError(err)\n\t\t\t\tg.metrics.EvalFailures.WithLabelValues(GroupKey(g.File(), g.Name())).Inc()\n\n\t\t\t\t&#x2F;&#x2F; Canceled queries are intentional termination of queries. This normally\n\t\t\t\t&#x2F;&#x2F; happens on shutdown and thus we skip logging of any errors here.\n\t\t\t\tif _, ok :&#x3D; err.(promql.ErrQueryCanceled); !ok &#123;\n\t\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Evaluating rule failed&quot;, &quot;rule&quot;, rule, &quot;err&quot;, err)\n\t\t\t\t&#125;\n\t\t\t\treturn\n\t\t\t&#125;\n            &#x2F;&#x2F; 记录规则状态\n\t\t\trule.SetHealth(HealthGood)\n\t\t\trule.SetLastError(nil)\n\t\t\tsamplesTotal +&#x3D; float64(len(vector))\n\n\t\t\tif ar, ok :&#x3D; rule.(*AlertingRule); ok &#123;\n                &#x2F;&#x2F; 达成报警条件，发送报警\n\t\t\t\tar.sendAlerts(ctx, ts, g.opts.ResendDelay, g.interval, g.opts.NotifyFunc)\n\t\t\t&#125;\n\t\t\tvar (\n\t\t\t\tnumOutOfOrder &#x3D; 0\n\t\t\t\tnumDuplicates &#x3D; 0\n\t\t\t)\n\n\t\t\tapp :&#x3D; g.opts.Appendable.Appender(ctx)\n\t\t\tseriesReturned :&#x3D; make(map[string]labels.Labels, len(g.seriesInPreviousEval[i]))\n\t\t\tdefer func() &#123;\n\t\t\t\tif err :&#x3D; app.Commit(); err !&#x3D; nil &#123;\n\t\t\t\t\trule.SetHealth(HealthBad)\n\t\t\t\t\trule.SetLastError(err)\n\t\t\t\t\tg.metrics.EvalFailures.WithLabelValues(GroupKey(g.File(), g.Name())).Inc()\n\n\t\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Rule sample appending failed&quot;, &quot;err&quot;, err)\n\t\t\t\t\treturn\n\t\t\t\t&#125;\n\t\t\t\tg.seriesInPreviousEval[i] &#x3D; seriesReturned\n\t\t\t&#125;()\n\n\t\t\tfor _, s :&#x3D; range vector &#123;\n\t\t\t\tif _, err :&#x3D; app.Append(0, s.Metric, s.T, s.V); err !&#x3D; nil &#123;\n\t\t\t\t\trule.SetHealth(HealthBad)\n\t\t\t\t\trule.SetLastError(err)\n\n\t\t\t\t\tswitch errors.Cause(err) &#123;\n\t\t\t\t\tcase storage.ErrOutOfOrderSample:\n\t\t\t\t\t\tnumOutOfOrder++\n\t\t\t\t\t\tlevel.Debug(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Rule evaluation result discarded&quot;, &quot;err&quot;, err, &quot;sample&quot;, s)\n\t\t\t\t\tcase storage.ErrDuplicateSampleForTimestamp:\n\t\t\t\t\t\tnumDuplicates++\n\t\t\t\t\t\tlevel.Debug(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Rule evaluation result discarded&quot;, &quot;err&quot;, err, &quot;sample&quot;, s)\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Rule evaluation result discarded&quot;, &quot;err&quot;, err, &quot;sample&quot;, s)\n\t\t\t\t\t&#125;\n\t\t\t\t&#125; else &#123;\n\t\t\t\t\tbuf :&#x3D; [1024]byte&#123;&#125;\n\t\t\t\t\tseriesReturned[string(s.Metric.Bytes(buf[:]))] &#x3D; s.Metric\n\t\t\t\t&#125;\n\t\t\t&#125;\n\t\t\tif numOutOfOrder &gt; 0 &#123;\n\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Error on ingesting out-of-order result from rule evaluation&quot;, &quot;numDropped&quot;, numOutOfOrder)\n\t\t\t&#125;\n\t\t\tif numDuplicates &gt; 0 &#123;\n\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Error on ingesting results from rule evaluation with different value but same timestamp&quot;, &quot;numDropped&quot;, numDuplicates)\n\t\t\t&#125;\n\n\t\t\tfor metric, lset :&#x3D; range g.seriesInPreviousEval[i] &#123;\n\t\t\t\tif _, ok :&#x3D; seriesReturned[metric]; !ok &#123;\n\t\t\t\t\t&#x2F;&#x2F; Series no longer exposed, mark it stale.\n\t\t\t\t\t_, err &#x3D; app.Append(0, lset, timestamp.FromTime(ts), math.Float64frombits(value.StaleNaN))\n\t\t\t\t\tswitch errors.Cause(err) &#123;\n\t\t\t\t\tcase nil:\n\t\t\t\t\tcase storage.ErrOutOfOrderSample, storage.ErrDuplicateSampleForTimestamp:\n\t\t\t\t\t\t&#x2F;&#x2F; Do not count these in logging, as this is expected if series\n\t\t\t\t\t\t&#x2F;&#x2F; is exposed from a different rule.\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Adding stale sample failed&quot;, &quot;sample&quot;, lset.String(), &quot;err&quot;, err)\n\t\t\t\t\t&#125;\n\t\t\t\t&#125;\n\t\t\t&#125;\n\t\t&#125;(i, rule)\n\t&#125;\n\tif g.metrics !&#x3D; nil &#123;\n\t\tg.metrics.GroupSamples.WithLabelValues(GroupKey(g.File(), g.Name())).Set(samplesTotal)\n\t&#125;\n\tg.cleanupStaleSeries(ctx, ts)\n&#125;</code></pre>\n</p>\n</details>\n<h2><span id=\"cha-xun\">查询</span><a href=\"#cha-xun\" class=\"header-anchor\">#</a></h2><h3><span id=\"dao-pai-suo-yin\">倒排索引</span><a href=\"#dao-pai-suo-yin\" class=\"header-anchor\">#</a></h3><p>Prometheus采用的是倒排索引，根据Label查找对应的时间序列，从而找到对应的Chunk。<br>Prometheus索引代码：</p>\n<details class=\"custom-details\">\n<summary>Index</summary>\n<p><pre class=\"line-numbers language-none\"><code class=\"language-none\">&#x2F;&#x2F; Writer implements the IndexWriter interface for the standard\n&#x2F;&#x2F; serialization format.\ntype Writer struct &#123;\n\tctx context.Context\n\n\t&#x2F;&#x2F; For the main index file.\n\tf *FileWriter\n\n\t&#x2F;&#x2F; Temporary file for postings.\n\tfP *FileWriter\n\t&#x2F;&#x2F; Temporary file for posting offsets table.\n\tfPO   *FileWriter\n\tcntPO uint64\n\n\ttoc           TOC\n\tstage         indexWriterStage\n\tpostingsStart uint64 &#x2F;&#x2F; Due to padding, can differ from TOC entry.\n\n\t&#x2F;&#x2F; Reusable memory.\n\tbuf1 encoding.Encbuf\n\tbuf2 encoding.Encbuf\n\n\tnumSymbols  int\n\tsymbols     *Symbols\n\tsymbolFile  *fileutil.MmapFile\n\tlastSymbol  string\n\tsymbolCache map[string]symbolCacheEntry\n\n\tlabelIndexes []labelIndexHashEntry &#x2F;&#x2F; Label index offsets.\n\tlabelNames   map[string]uint64     &#x2F;&#x2F; Label names, and their usage.\n\n\t&#x2F;&#x2F; Hold last series to validate that clients insert new series in order.\n\tlastSeries labels.Labels\n\tlastRef    uint64\n\n\tcrc32 hash.Hash\n\n\tVersion int\n&#125;\n\ntype Reader struct &#123;\n\tb   ByteSlice\n\ttoc *TOC\n\n\t&#x2F;&#x2F; Close that releases the underlying resources of the byte slice.\n\tc io.Closer\n\n\t&#x2F;&#x2F; Map of LabelName to a list of some LabelValues&#39;s position in the offset table.\n\t&#x2F;&#x2F; The first and last values for each name are always present.\n\tpostings map[string][]postingOffset\n\t&#x2F;&#x2F; For the v1 format, labelname -&gt; labelvalue -&gt; offset.\n\tpostingsV1 map[string]map[string]uint64\n\n\tsymbols     *Symbols\n\tnameSymbols map[uint32]string &#x2F;&#x2F; Cache of the label name symbol lookups,\n\t&#x2F;&#x2F; as there are not many and they are half of all lookups.\n\n\tdec *Decoder\n\n\tversion int\n&#125;\n\n&#x2F;&#x2F; TOC represents index Table Of Content that states where each section of index starts.\ntype TOC struct &#123;\n\tSymbols           uint64\n\tSeries            uint64\n\tLabelIndices      uint64\n\tLabelIndicesTable uint64\n\tPostings          uint64\n\tPostingsTable     uint64\n&#125;\n\ntype postingOffset struct &#123;\n\tvalue string\n\toff   int\n&#125;\n\ntype Symbols struct &#123;\n\tbs      ByteSlice\n\tversion int\n\toff     int\n\n\toffsets []int\n\tseen    int\n&#125;</code></pre>\n</p>\n</details>\n<p>一个索引包含了以下内容：</p>\n<ul>\n<li>Symbol：数据字典，每个Label的Name和Value都对应一个ID</li>\n<li>TOC：记录了时间序列的ID和Label的Name和Value的对应关系</li>\n<li>Posting：每个label值对应series列表的偏移量<br>倒排索引的工作流程：</li>\n</ul>\n<p><img src=\"../images/Promtheus%E6%95%B0%E6%8D%AE%E6%B5%81/%E7%B4%A2%E5%BC%95.png\" alt=\"索引\"></p>\n","text":" Prometheus是一个开源的监控和报警工具。它优秀的设计理念，灵活的扩展，丰富的生态以及活跃的社区使它正在成为众多开发者喜爱的监控工具。 简介 数据拉取 Metrics数据 时间向量 Sample Metric 构造Metrics 数据抓取 Job Target 采集方式 ...","link":"","photos":[],"count_time":{"symbolsCount":"44k","symbolsTime":"40 mins."},"categories":[{"name":"Monitor","slug":"Monitor","count":1,"path":"api/categories/Monitor.json"}],"tags":[{"name":"Monitor","slug":"Monitor","count":1,"path":"api/tags/Monitor.json"},{"name":"Prometheus","slug":"Prometheus","count":1,"path":"api/tags/Prometheus.json"},{"name":"Metrics","slug":"Metrics","count":1,"path":"api/tags/Metrics.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\"><span class=\"toc-text\">简介</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\"><span class=\"toc-text\">数据拉取</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">Metrics数据</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">时间向量</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">Sample</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">Metric</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">构造Metrics</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">数据抓取</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">Job</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">Target</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">采集方式</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">服务发现</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\"><span class=\"toc-text\">数据存储</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">RemoteWrite</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">RemoteRead</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">数据存储</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">存储结构</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\">数据分析</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\"><span class=\"toc-text\">报警</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\"><span class=\"toc-text\">查询</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\"><span class=\"toc-text\">倒排索引</span></a></li></ol></li></ol>","author":{"name":"Asura","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Keep runnig!","socials":{"github":"https://github.com/G-Asura","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"让内核开发变的简单的eBPF","uid":"8b17daa7707d9d476a6f46015ac8ef41","slug":"让内核开发变的简单的eBPF","date":"2022-04-10T12:43:44.000Z","updated":"2022-04-12T18:01:27.169Z","comments":true,"path":"api/articles/让内核开发变的简单的eBPF.json","keywords":null,"cover":[],"text":" eBPF 是一项革命性的技术，起源于 Linux 内核，可以在操作系统内核中运行沙盒程序。它用于安全有效地扩展内核的功能，而无需更改内核源代码或加载内核模块。 eBPF is a revolutionary technology with origins in the Linu...","link":"","photos":[],"count_time":{"symbolsCount":"7.1k","symbolsTime":"6 mins."},"categories":[{"name":"eBPF","slug":"eBPF","count":2,"path":"api/categories/eBPF.json"}],"tags":[{"name":"eBPF","slug":"eBPF","count":2,"path":"api/tags/eBPF.json"},{"name":"Linux","slug":"Linux","count":2,"path":"api/tags/Linux.json"}],"author":{"name":"Asura","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Keep runnig!","socials":{"github":"https://github.com/G-Asura","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"容器技术实现原理","uid":"96badec9d2a8ff4ed297d972e3ac1aab","slug":"容器技术实现原理","date":"2022-04-05T14:56:56.000Z","updated":"2022-04-12T04:20:47.325Z","comments":true,"path":"api/articles/容器技术实现原理.json","keywords":null,"cover":[],"text":" 开始之前为了方便的理解容器，先举个简单的例子：假如把一套房子比作操作系统，这套房子的中控就是内核，每间卧室可以理解为是虚拟化出来的虚拟机，他们可以拥有各自独立的中控，假设这套房子中控控制着台冰箱，冰箱里有多个隔层，那么每个隔层就是一个个独立运行的容器。 Linux容器 进程 N...","link":"","photos":[],"count_time":{"symbolsCount":"4.6k","symbolsTime":"4 mins."},"categories":[{"name":"Container","slug":"Container","count":1,"path":"api/categories/Container.json"}],"tags":[{"name":"Linux","slug":"Linux","count":2,"path":"api/tags/Linux.json"},{"name":"Container","slug":"Container","count":1,"path":"api/tags/Container.json"}],"author":{"name":"Asura","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"Keep runnig!","socials":{"github":"https://github.com/G-Asura","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}