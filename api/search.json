[{"id":"0deca7558ba6f9defe92ca748e91351a","title":"使用Nocalhost在Kubernetes中调试代码","content":"\n\n\n\n\n\n\n\n\n云原生开发工程师在日常开发中会经常遇到需要操作或者连接Kubernetes集群中的资源，但是往往本地开发环境和Kubernetes集群之间网络不通，这时调试代码就会十分不方便，所以有时我们需要在Kubernetes集群中进行代码调试，下面就是基于Nocalhost的一种在Kubernetes集群中调试代码的实践方案。\n\n\n\n\n\nNocalhost\n简介\n实现原理\n\n\n实践\n开发环境\n流程\n一、创建Registry\n二、创建Kubernetes集群\n三、安装Nocalhost\n四、配置Nocalhost\n\n\n\n\n\n\n\n\n\nNocalhost#简介#官方对Nocalhost的定义如下：\nNocalhost 是一款开源的基于 IDE 的云原生应用开发工具：\n  - 直接在 Kubernetes 集群中构建、测试和调试应用程序\n  - 提供易于使用的 IDE 插件（支持 VS Code 和 JetBrains），即使在 Kubernetes 集群中进行开发和调试，Nocalhost 也能保持和本地开发一样的开发体验\n  - 使用即时文件同步进行开发： 即时将您的代码更改同步到远端容器，而无需重建镜像或重新启动容器。\n简单来讲，Nocalhost是一款基于IDE的云原生开发工具，可以使得云上开发达到和本地开发一样的效果更多具体介绍可以参考官网：Nocalhost\n实现原理#Nocalhost 由单个二进制 CLI 和 IDE 插件组成。 理想情况下，您可以直接将它与您熟悉的 IDE 一起使用。 Nocalhost 不需要服务器端组件，因为它通过 KubeConfig 直接与您的 Kubernetes 集群通信，就像 kubectl 一样。主要流程：  \n\n使用kubeconfig连接到集群\n使用workload进行调试，可以选择自定义的调试镜像运行环境，以Golang为例可以使用dlv镜像\n在项目中配置Nocalhost调试\n可以通过控制台获取程序运行状态\n\n实践#开发环境#\nIDE：VSCode\n集群：Kind\n镜像仓库：Registry\n语言：Golang\n\n流程#一、创建Registry## docker run -d --restart always --network kind --name registry -p 0.0.0.0:5000:5000 registry:2\n二、创建Kubernetes集群#\nkind create\n# kind create cluster --image kindest&#x2F;node:v1.21.12 --config&#x3D;- &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io&#x2F;v1alpha4\nnetworking:\n  apiServerPort: 6443\n  podSubnet: 172.16.0.0&#x2F;16\n  serviceSubnet: 172.19.0.0&#x2F;16\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\n  - containerPort: 8181\n    hostPort: 8181\n    protocol: TCP\nkubeadmConfigPatches:\n- |\n  kind: ClusterConfiguration\n  etcd:\n    local:\n      extraArgs:\n        listen-metrics-urls: http:&#x2F;&#x2F;0.0.0.0:2381\n  controllerManager:\n    extraArgs:\n      bind-address: 0.0.0.0\n  scheduler:\n    extraArgs:\n      bind-address: 0.0.0.0\ncontainerdConfigPatches:\n- |-\n  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;localhost:5000&quot;]\n    endpoint &#x3D; [&quot;http:&#x2F;&#x2F;registry:5000&quot;]\nEOF\n\n\n\n三、安装Nocalhost#以VSCode为例，在扩展里搜索Nocalhost进行安装，详细可以参考官网QuickStart\n四、配置Nocalhost#\n添加集群\n通过kubeconfig添加集群\n\n\n选择工作负载进入DevMode\n在Nocalhost选项卡里选择我们想要进行调试的工作负载进入DevMode，如果没有，我们可以创建一个简单的Deployment\n\ndeployment.yaml\napiVersion: apps&#x2F;v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n\n\n\n\n\n选择项目文件\n可以选择本地目录，或者远端代码库，这里有个简单的例子\n\n选择调试镜像\n选择调试镜像，可以使用官方提供的调试镜像，如果不满足需求，也可以构建自己的镜像，下面以Golang为例构建1.18版本的dlv镜像\n\nDockerfile\nFROM golang:1.18\nRUN go install github.com&#x2F;go-delve&#x2F;delve&#x2F;cmd&#x2F;dlv@v1.9.1\n\n\n\n\n\n配置Nocalhost\n如果需要对进行特殊配置，可以在文件或者浏览器里配置\n\n\nnocalhost.config\nname: nginx-deployment\nserviceType: deployment\ncontainers:\n  - name: nginx\n    dev:\n      gitUrl: git@github.com:G-Asura&#x2F;nocalhost-example.git\n      image: localhost:5000&#x2F;dlv:1.18\n      shell: &quot;&quot;\n      workDir: &quot;&quot;\n      storageClass: &quot;&quot;\n      resources:\n        limits:\n          memory: 512Mi\n          cpu: &quot;1&quot;\n        requests:\n          memory: 256Mi\n          cpu: &quot;0.5&quot;\n      persistentVolumeDirs: []\n      command:\n        run: []\n        debug:\n          - .&#x2F;debug.sh\n      debug:\n        remoteDebugPort: 8181\n        language: go\n      hotReload: true\n      sync: null\n      env: []\n      portForward: []\n\n\n\n\n开发调试\n启动了DevMode之后就可以在新窗口进行代码调试，在调试之前需要准备一个调试脚本\n\ndebug.sh\n#! &#x2F;bin&#x2F;sh\nexport GOPROXY&#x3D;https:&#x2F;&#x2F;goproxy.cn\ndlv --headless --log --listen :8181 --api-version 2 --accept-multiclient debug main.go\n\n\n\n同时需要准备IDE调试配置，下面以VSCode为例\n\ndebug.sh\n&#123;\n &quot;version&quot;: &quot;0.2.0&quot;,\n &quot;configurations&quot;: [\n   &#123;\n     &quot;type&quot;: &quot;nocalhost&quot;,\n     &quot;request&quot;: &quot;attach&quot;,\n     &quot;name&quot;: &quot;Nocalhost Debug&quot;\n   &#125;\n ]\n&#125;\n\n\n\n最后就可以在程序中打上断点进行调试了\n\n\n\n","slug":"使用Nocalhost在Kubernetes中调试代码","date":"2022-10-10T12:10:41.000Z","categories_index":"Tools","tags_index":"Kubernetes,Nocalhost,IDE","author_index":"Asura"},{"id":"65501d6a3eead0987c9f5d07b63371fd","title":"Cilium实现原理","content":"\n\n\n\n\n\n\n\n\nCilium是eBPF网络分支的明星项目，Cilium的发布也意味着eBPF开始向K8S领域进军。\n\n\n\n\n\nCilium eBPF流程\n流程图\nAgent启动原理\n入口函数\n\n\n\n\n数据路径\nkube-proxy包转发路径\nCilium eBPF包转发路径\n演示\n\n\n\n\n查看main详情\n另一个session抓包\n查看宿主机对应网卡信息\n查看tc挂载\n\n\n\n\n\nCilium eBPF流程#了解Cilium实现原理之前，首先我们先来看看Cilium使用eBPF实现容器网络的流程。  \n流程图#\n如上图所示，有以下几个步骤：  \n\nCilium agent 生成 eBPF 程序；  \n用 LLVM 编译 eBPF 程序，生成 eBPF 对象文件（object file，*.o）；  \n用 eBPF loader 将对象文件加载到 Linux 内核；\n校验器（verifier）对 eBPF 指令会进行合法性验证，以确保程序是安全的，例如 ，无非法内存访问、不会 crash 内核、不会有无限循环等；  \n对象文件被即时编译（JIT）为能直接在底层平台（例如 x86）运行的 native code；  \n如果要在内核和用户态之间共享状态，BPF 程序可以使用 BPF map，这种一种共享存储 ，BPF 侧和用户侧都可以访问；  \nBPF 程序就绪，等待事件触发其执行。对于这个例子，就是有数据包到达网络设备时，触发 BPF 程序的执行。  \nBPF 程序对收到的包进行处理，例如 mangle。最后返回一个裁决（verdict）结果；  \n根据裁决结果，如果是 DROP，这个包将被丢弃；如果是 PASS，包会被送到更网络栈的 更上层继续处理；如果是重定向，就发送给其他设备。\n\nAgent启动原理#了解了Cilium eBPF流程后我们接下来看看Agent的源码。Cilium Agent的启动配置是存在ConfigMap中的，通过–config-dir参数指定配置文件目录。  \n入口函数#\ndaemon/cmd/daemon_main.go\nfunc runDaemon() &#123;\n\tdatapathConfig :&#x3D; linuxdatapath.DatapathConfiguration&#123;\n\t\tHostDevice: defaults.HostDevice,\n\t&#125;\n\n\tlog.Info(&quot;Initializing daemon&quot;)\n\n\toption.Config.RunMonitorAgent &#x3D; true\n\n\tif err :&#x3D; enableIPForwarding(); err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Fatal(&quot;Error when enabling sysctl parameters&quot;)\n\t&#125;\n\n\tiptablesManager :&#x3D; &amp;iptables.IptablesManager&#123;&#125;\n\tiptablesManager.Init()\n\n\tvar wgAgent *wireguard.Agent\n\tif option.Config.EnableWireguard &#123;\n\t\tswitch &#123;\n\t\tcase option.Config.EnableIPSec:\n\t\t\tlog.Fatalf(&quot;Wireguard (--%s) cannot be used with IPSec (--%s)&quot;,\n\t\t\t\toption.EnableWireguard, option.EnableIPSecName)\n\t\tcase option.Config.EnableL7Proxy:\n\t\t\tlog.Fatalf(&quot;Wireguard (--%s) is not compatible with L7 proxy (--%s)&quot;,\n\t\t\t\toption.EnableWireguard, option.EnableL7Proxy)\n\t\t&#125;\n\n\t\tvar err error\n\t\tprivateKeyPath :&#x3D; filepath.Join(option.Config.StateDir, wireguardTypes.PrivKeyFilename)\n\t\twgAgent, err &#x3D; wireguard.NewAgent(privateKeyPath)\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Failed to initialize wireguard&quot;)\n\t\t&#125;\n\n\t\tcleaner.cleanupFuncs.Add(func() &#123;\n\t\t\t_ &#x3D; wgAgent.Close()\n\t\t&#125;)\n\t&#125; else &#123;\n\t\t&#x2F;&#x2F; Delete wireguard device from previous run (if such exists)\n\t\tlink.DeleteByName(wireguardTypes.IfaceName)\n\t&#125;\n\n\tif k8s.IsEnabled() &#123;\n\t\tbootstrapStats.k8sInit.Start()\n\t\tif err :&#x3D; k8s.Init(option.Config); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Unable to initialize Kubernetes subsystem&quot;)\n\t\t&#125;\n\t\tbootstrapStats.k8sInit.End(true)\n\t&#125;\n\n\tctx, cancel :&#x3D; context.WithCancel(server.ServerCtx)\n\td, restoredEndpoints, err :&#x3D; NewDaemon(ctx, cancel,\n\t\tWithDefaultEndpointManager(ctx, endpoint.CheckHealth),\n\t\tlinuxdatapath.NewDatapath(datapathConfig, iptablesManager, wgAgent))\n\tif err !&#x3D; nil &#123;\n\t\tselect &#123;\n\t\tcase &lt;-server.ServerCtx.Done():\n\t\t\tlog.WithError(err).Debug(&quot;Error while creating daemon&quot;)\n\t\tdefault:\n\t\t\tlog.WithError(err).Fatal(&quot;Error while creating daemon&quot;)\n\t\t&#125;\n\t\treturn\n\t&#125;\n\n\t&#x2F;&#x2F; This validation needs to be done outside of the agent until\n\t&#x2F;&#x2F; datapath.NodeAddressing is used consistently across the code base.\n\tlog.Info(&quot;Validating configured node address ranges&quot;)\n\tif err :&#x3D; node.ValidatePostInit(); err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Fatal(&quot;postinit failed&quot;)\n\t&#125;\n\n\tbootstrapStats.enableConntrack.Start()\n\tlog.Info(&quot;Starting connection tracking garbage collector&quot;)\n\tgc.Enable(option.Config.EnableIPv4, option.Config.EnableIPv6,\n\t\trestoredEndpoints.restored, d.endpointManager)\n\tbootstrapStats.enableConntrack.End(true)\n\n\tbootstrapStats.k8sInit.Start()\n\tif k8s.IsEnabled() &#123;\n\t\t&#x2F;&#x2F; Wait only for certain caches, but not all!\n\t\t&#x2F;&#x2F; (Check Daemon.InitK8sSubsystem() for more info)\n\t\t&lt;-d.k8sCachesSynced\n\t&#125;\n\tbootstrapStats.k8sInit.End(true)\n\trestoreComplete :&#x3D; d.initRestore(restoredEndpoints)\n\tif wgAgent !&#x3D; nil &#123;\n\t\tif err :&#x3D; wgAgent.RestoreFinished(); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Error(&quot;Failed to set up wireguard peers&quot;)\n\t\t&#125;\n\t&#125;\n\n\tif d.endpointManager.HostEndpointExists() &#123;\n\t\td.endpointManager.InitHostEndpointLabels(d.ctx)\n\t&#125; else &#123;\n\t\tlog.Info(&quot;Creating host endpoint&quot;)\n\t\tif err :&#x3D; d.endpointManager.AddHostEndpoint(\n\t\t\td.ctx, d, d, d.ipcache, d.l7Proxy, d.identityAllocator,\n\t\t\t&quot;Create host endpoint&quot;, nodeTypes.GetName(),\n\t\t); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Unable to create host endpoint&quot;)\n\t\t&#125;\n\t&#125;\n\n\tif option.Config.EnableIPMasqAgent &#123;\n\t\tipmasqAgent, err :&#x3D; ipmasq.NewIPMasqAgent(option.Config.IPMasqAgentConfigPath)\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Failed to create ip-masq-agent&quot;)\n\t\t&#125;\n\t\tipmasqAgent.Start()\n\t&#125;\n\n\tif !option.Config.DryMode &#123;\n\t\tgo func() &#123;\n\t\t\tif restoreComplete !&#x3D; nil &#123;\n\t\t\t\t&lt;-restoreComplete\n\t\t\t&#125;\n\t\t\td.dnsNameManager.CompleteBootstrap()\n\n\t\t\tms :&#x3D; maps.NewMapSweeper(&amp;EndpointMapManager&#123;\n\t\t\t\tEndpointManager: d.endpointManager,\n\t\t\t&#125;)\n\t\t\tms.CollectStaleMapGarbage()\n\t\t\tms.RemoveDisabledMaps()\n\n\t\t\tif len(d.restoredCIDRs) &gt; 0 &#123;\n\t\t\t\t&#x2F;&#x2F; Release restored CIDR identities after a grace period (default 10\n\t\t\t\t&#x2F;&#x2F; minutes).  Any identities actually in use will still exist after\n\t\t\t\t&#x2F;&#x2F; this.\n\t\t\t\t&#x2F;&#x2F;\n\t\t\t\t&#x2F;&#x2F; This grace period is needed when running on an external workload\n\t\t\t\t&#x2F;&#x2F; where policy synchronization is not done via k8s. Also in k8s\n\t\t\t\t&#x2F;&#x2F; case it is prudent to allow concurrent endpoint regenerations to\n\t\t\t\t&#x2F;&#x2F; (re-)allocate the restored identities before we release them.\n\t\t\t\ttime.Sleep(option.Config.IdentityRestoreGracePeriod)\n\t\t\t\tlog.Debugf(&quot;Releasing reference counts for %d restored CIDR identities&quot;, len(d.restoredCIDRs))\n\n\t\t\t\td.ipcache.ReleaseCIDRIdentitiesByCIDR(d.restoredCIDRs)\n\t\t\t\t&#x2F;&#x2F; release the memory held by restored CIDRs\n\t\t\t\td.restoredCIDRs &#x3D; nil\n\t\t\t&#125;\n\t\t&#125;()\n\t\td.endpointManager.Subscribe(d)\n\t\tdefer d.endpointManager.Unsubscribe(d)\n\t&#125;\n\n\t&#x2F;&#x2F; Migrating the ENI datapath must happen before the API is served to\n\t&#x2F;&#x2F; prevent endpoints from being created. It also must be before the health\n\t&#x2F;&#x2F; initialization logic which creates the health endpoint, for the same\n\t&#x2F;&#x2F; reasons as the API being served. We want to ensure that this migration\n\t&#x2F;&#x2F; logic runs before any endpoint creates.\n\tif option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMENI &#123;\n\t\tmigrated, failed :&#x3D; linuxrouting.NewMigrator(\n\t\t\t&amp;eni.InterfaceDB&#123;&#125;,\n\t\t).MigrateENIDatapath(option.Config.EgressMultiHomeIPRuleCompat)\n\t\tswitch &#123;\n\t\tcase failed &#x3D;&#x3D; -1:\n\t\t\t&#x2F;&#x2F; No need to handle this case specifically because it is handled\n\t\t\t&#x2F;&#x2F; in the call already.\n\t\tcase migrated &gt;&#x3D; 0 &amp;&amp; failed &gt; 0:\n\t\t\tlog.Errorf(&quot;Failed to migrate ENI datapath. &quot;+\n\t\t\t\t&quot;%d endpoints were successfully migrated and %d failed to migrate completely. &quot;+\n\t\t\t\t&quot;The original datapath is still in-place, however it is recommended to retry the migration.&quot;,\n\t\t\t\tmigrated, failed)\n\n\t\tcase migrated &gt;&#x3D; 0 &amp;&amp; failed &#x3D;&#x3D; 0:\n\t\t\tlog.Infof(&quot;Migration of ENI datapath successful, %d endpoints were migrated and none failed.&quot;,\n\t\t\t\tmigrated)\n\t\t&#125;\n\t&#125;\n\n\tbootstrapStats.healthCheck.Start()\n\tif option.Config.EnableHealthChecking &#123;\n\t\td.initHealth()\n\t&#125;\n\tbootstrapStats.healthCheck.End(true)\n\n\td.startStatusCollector()\n\n\tmetricsErrs :&#x3D; initMetrics()\n\n\td.startAgentHealthHTTPService()\n\tif option.Config.KubeProxyReplacementHealthzBindAddr !&#x3D; &quot;&quot; &#123;\n\t\tif option.Config.KubeProxyReplacement !&#x3D; option.KubeProxyReplacementDisabled &#123;\n\t\t\td.startKubeProxyHealthzHTTPService(fmt.Sprintf(&quot;%s&quot;, option.Config.KubeProxyReplacementHealthzBindAddr))\n\t\t&#125;\n\t&#125;\n\n\tbootstrapStats.initAPI.Start()\n\tsrv :&#x3D; server.NewServer(d.instantiateAPI())\n\tsrv.EnabledListeners &#x3D; []string&#123;&quot;unix&quot;&#125;\n\tsrv.SocketPath &#x3D; option.Config.SocketPath\n\tsrv.ReadTimeout &#x3D; apiTimeout\n\tsrv.WriteTimeout &#x3D; apiTimeout\n\tdefer srv.Shutdown()\n\n\tsrv.ConfigureAPI()\n\tbootstrapStats.initAPI.End(true)\n\n\terr &#x3D; d.SendNotification(monitorAPI.StartMessage(time.Now()))\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Warn(&quot;Failed to send agent start monitor message&quot;)\n\t&#125;\n\n\tif !d.datapath.Node().NodeNeighDiscoveryEnabled() &#123;\n\t\t&#x2F;&#x2F; Remove all non-GC&#39;ed neighbor entries that might have previously set\n\t\t&#x2F;&#x2F; by a Cilium instance.\n\t\td.datapath.Node().NodeCleanNeighbors(false)\n\t&#125; else &#123;\n\t\t&#x2F;&#x2F; If we came from an agent upgrade, migrate entries.\n\t\td.datapath.Node().NodeCleanNeighbors(true)\n\t\t&#x2F;&#x2F; Start periodical refresh of the neighbor table from the agent if needed.\n\t\tif option.Config.ARPPingRefreshPeriod !&#x3D; 0 &amp;&amp; !option.Config.ARPPingKernelManaged &#123;\n\t\t\td.nodeDiscovery.Manager.StartNeighborRefresh(d.datapath.Node())\n\t\t&#125;\n\t&#125;\n\n\tif option.Config.BGPControlPlaneEnabled() &#123;\n\t\tlog.Info(&quot;Initializing BGP Control Plane&quot;)\n\t\tif err :&#x3D; d.instantiateBGPControlPlane(d.ctx); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Error returned when instantiating BGP control plane&quot;)\n\t\t&#125;\n\t&#125;\n\n\tlog.WithField(&quot;bootstrapTime&quot;, time.Since(bootstrapTimestamp)).\n\t\tInfo(&quot;Daemon initialization completed&quot;)\n\n\tif option.Config.WriteCNIConfigurationWhenReady !&#x3D; &quot;&quot; &#123;\n\t\tinput, err :&#x3D; os.ReadFile(option.Config.ReadCNIConfiguration)\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Unable to read CNI configuration file&quot;)\n\t\t&#125;\n\n\t\tif err &#x3D; os.WriteFile(option.Config.WriteCNIConfigurationWhenReady, input, 0644); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatalf(&quot;Unable to write CNI configuration file to %s&quot;, option.Config.WriteCNIConfigurationWhenReady)\n\t\t&#125; else &#123;\n\t\t\tlog.Infof(&quot;Wrote CNI configuration file to %s&quot;, option.Config.WriteCNIConfigurationWhenReady)\n\t\t&#125;\n\t&#125;\n\n\terrs :&#x3D; make(chan error, 1)\n\n\tgo func() &#123;\n\t\terrs &lt;- srv.Serve()\n\t&#125;()\n\n\tif k8s.IsEnabled() &#123;\n\t\tbootstrapStats.k8sInit.Start()\n\t\tk8s.Client().MarkNodeReady(d.k8sWatcher, nodeTypes.GetName())\n\t\tbootstrapStats.k8sInit.End(true)\n\t&#125;\n\n\tbootstrapStats.overall.End(true)\n\tbootstrapStats.updateMetrics()\n\tgo d.launchHubble()\n\n\terr &#x3D; option.Config.StoreInFile(option.Config.StateDir)\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Error(&quot;Unable to store Cilium&#39;s configuration&quot;)\n\t&#125;\n\n\terr &#x3D; option.StoreViperInFile(option.Config.StateDir)\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Error(&quot;Unable to store Viper&#39;s configuration&quot;)\n\t&#125;\n\n\tselect &#123;\n\tcase err :&#x3D; &lt;-metricsErrs:\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Cannot start metrics server&quot;)\n\t\t&#125;\n\tcase err :&#x3D; &lt;-errs:\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Fatal(&quot;Error returned from non-returning Serve() call&quot;)\n\t\t&#125;\n\t&#125;\n&#125;\n\n\nCilium Agent实现主要分为以下几步：  \n\n打开IP forwarding\n\n初始化k8s\n\n创建守护进程  \n\ndaemon/cmd/daemon.go\n     func NewDaemon(ctx context.Context, cancel context.CancelFunc, epMgr *endpointmanager.EndpointManager, dp datapath.Datapath) (*Daemon, *endpointRestoreState, error) &#123;\n\n\t&#x2F;&#x2F; Pass the cancel to our signal handler directly so that it&#39;s canceled\n\t&#x2F;&#x2F; before we run the cleanup functions (see &#96;cleanup.go&#96; for implementation).\n\tcleaner.SetCancelFunc(cancel)\n\n\tvar (\n\t\terr           error\n\t\tnetConf       *cnitypes.NetConf\n\t\tconfiguredMTU &#x3D; option.Config.MTU\n\t)\n\n\tbootstrapStats.daemonInit.Start()\n\n\t&#x2F;&#x2F; Validate the daemon-specific global options.\n\tif err :&#x3D; option.Config.Validate(); err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;invalid daemon configuration: %s&quot;, err)\n\t&#125;\n\n\tif option.Config.ReadCNIConfiguration !&#x3D; &quot;&quot; &#123;\n\t\tnetConf, err &#x3D; cnitypes.ReadNetConf(option.Config.ReadCNIConfiguration)\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;unable to read CNI configuration: %w&quot;, err)\n\t\t&#125;\n\n\t\tif netConf.MTU !&#x3D; 0 &#123;\n\t\t\tconfiguredMTU &#x3D; netConf.MTU\n\t\t\tlog.WithField(&quot;mtu&quot;, configuredMTU).Info(&quot;Overwriting MTU based on CNI configuration&quot;)\n\t\t&#125;\n\t&#125;\n\n\tapiLimiterSet, err :&#x3D; rate.NewAPILimiterSet(option.Config.APIRateLimit, apiRateLimitDefaults, &amp;apiRateLimitingMetrics&#123;&#125;)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;unable to configure API rate limiting: %w&quot;, err)\n\t&#125;\n\n\t&#x2F;&#x2F; Check the kernel if we can make use of managed neighbor entries which\n\t&#x2F;&#x2F; simplifies and fully &#39;offloads&#39; L2 resolution handling to the kernel.\n\tprobeManagedNeighborSupport()\n\n\t&#x2F;&#x2F; Do the partial kube-proxy replacement initialization before creating BPF\n\t&#x2F;&#x2F; maps. Otherwise, some maps might not be created (e.g. session affinity).\n\t&#x2F;&#x2F; finishKubeProxyReplacementInit(), which is called later after the device\n\t&#x2F;&#x2F; detection, might disable BPF NodePort and friends. But this is fine, as\n\t&#x2F;&#x2F; the feature does not influence the decision which BPF maps should be\n\t&#x2F;&#x2F; created.\n\tisKubeProxyReplacementStrict, err :&#x3D; initKubeProxyReplacementOptions()\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;unable to initialize Kube proxy replacement options: %w&quot;, err)\n\t&#125;\n\n\tctmap.InitMapInfo(option.Config.CTMapEntriesGlobalTCP, option.Config.CTMapEntriesGlobalAny,\n\t\toption.Config.EnableIPv4, option.Config.EnableIPv6, option.Config.EnableNodePort)\n\tpolicymap.InitMapInfo(option.Config.PolicyMapEntries)\n\tlbmap.Init(lbmap.InitParams&#123;\n\t\tIPv4: option.Config.EnableIPv4,\n\t\tIPv6: option.Config.EnableIPv6,\n\n\t\tMaxSockRevNatMapEntries: option.Config.SockRevNatEntries,\n\t\tMaxEntries:              option.Config.LBMapEntries,\n\t&#125;)\n\n\tif option.Config.DryMode &#x3D;&#x3D; false &#123;\n\t\tif err :&#x3D; rlimit.RemoveMemlock(); err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;unable to set memory resource limits: %w&quot;, err)\n\t\t&#125;\n\t&#125;\n\n\tauthKeySize, encryptKeyID, err :&#x3D; setupIPSec()\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;unable to setup encryption: %s&quot;, err)\n\t&#125;\n\n\tvar mtuConfig mtu.Configuration\n\texternalIP :&#x3D; node.GetIPv4()\n\tif externalIP &#x3D;&#x3D; nil &#123;\n\t\texternalIP &#x3D; node.GetIPv6()\n\t&#125;\n\t&#x2F;&#x2F; ExternalIP could be nil but we are covering that case inside NewConfiguration\n\tmtuConfig &#x3D; mtu.NewConfiguration(\n\t\tauthKeySize,\n\t\toption.Config.EnableIPSec,\n\t\toption.Config.TunnelingEnabled(),\n\t\toption.Config.EnableWireguard,\n\t\tconfiguredMTU,\n\t\texternalIP,\n\t)\n\n\tnodeMngr, err :&#x3D; nodemanager.NewManager(&quot;all&quot;, dp.Node(), option.Config, nil, nil)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, err\n\t&#125;\n\n\tidentity.IterateReservedIdentities(func(_ identity.NumericIdentity, _ *identity.Identity) &#123;\n\t\tmetrics.Identity.Inc()\n\t&#125;)\n\tif option.Config.EnableWellKnownIdentities &#123;\n\t\t&#x2F;&#x2F; Must be done before calling policy.NewPolicyRepository() below.\n\t\tnum :&#x3D; identity.InitWellKnownIdentities(option.Config)\n\t\tmetrics.Identity.Add(float64(num))\n\t&#125;\n\n\tnd :&#x3D; nodediscovery.NewNodeDiscovery(nodeMngr, mtuConfig, netConf)\n\n\td :&#x3D; Daemon&#123;\n\t\tctx:               ctx,\n\t\tcancel:            cancel,\n\t\tprefixLengths:     createPrefixLengthCounter(),\n\t\tbuildEndpointSem:  semaphore.NewWeighted(int64(numWorkerThreads())),\n\t\tcompilationMutex:  new(lock.RWMutex),\n\t\tnetConf:           netConf,\n\t\tmtuConfig:         mtuConfig,\n\t\tdatapath:          dp,\n\t\tdeviceManager:     NewDeviceManager(),\n\t\tnodeDiscovery:     nd,\n\t\tendpointCreations: newEndpointCreationManager(),\n\t\tapiLimiterSet:     apiLimiterSet,\n\t&#125;\n\n\tif option.Config.RunMonitorAgent &#123;\n\t\td.monitorAgent &#x3D; monitoragent.NewAgent(ctx)\n\t&#125;\n\n\td.configModifyQueue &#x3D; eventqueue.NewEventQueueBuffered(&quot;config-modify-queue&quot;, ConfigModifyQueueSize)\n\td.configModifyQueue.Run()\n\n\td.rec, err &#x3D; recorder.NewRecorder(d.ctx, &amp;d)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;error while initializing BPF pcap recorder: %w&quot;, err)\n\t&#125;\n\n\t&#x2F;&#x2F; Collect old CIDR identities\n\tvar oldNIDs []identity.NumericIdentity\n\tif option.Config.RestoreState &amp;&amp; !option.Config.DryMode &amp;&amp; ipcachemap.SupportsDump() &#123;\n\t\tif err :&#x3D; ipcachemap.IPCache.DumpWithCallback(func(key bpf.MapKey, value bpf.MapValue) &#123;\n\t\t\tk :&#x3D; key.(*ipcachemap.Key)\n\t\t\tv :&#x3D; value.(*ipcachemap.RemoteEndpointInfo)\n\t\t\tnid :&#x3D; identity.NumericIdentity(v.SecurityIdentity)\n\t\t\tif nid.HasLocalScope() &#123;\n\t\t\t\td.restoredCIDRs &#x3D; append(d.restoredCIDRs, k.IPNet())\n\t\t\t\toldNIDs &#x3D; append(oldNIDs, nid)\n\t\t\t&#125;\n\t\t&#125;); err !&#x3D; nil &amp;&amp; !os.IsNotExist(err) &#123;\n\t\t\tlog.WithError(err).Warning(&quot;Error dumping ipcache&quot;)\n\t\t&#125;\n\t\tipcachemap.IPCache.Close()\n\t&#125;\n\n\t&#x2F;&#x2F; Propagate identity allocator down to packages which themselves do not\n\t&#x2F;&#x2F; have types to which we can add an allocator member.\n\t&#x2F;&#x2F;\n\t&#x2F;&#x2F; **NOTE** The identity allocator is not yet initialized here; that\n\t&#x2F;&#x2F; happens below. We&#39;ve only allocated the structure at this point.\n\t&#x2F;&#x2F;\n\t&#x2F;&#x2F; TODO: convert these package level variables to types for easier unit\n\t&#x2F;&#x2F; testing in the future.\n\td.identityAllocator &#x3D; NewCachingIdentityAllocator(&amp;d)\n\tif err :&#x3D; d.initPolicy(epMgr); err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;error while initializing policy subsystem: %w&quot;, err)\n\t&#125;\n\td.ipcache &#x3D; ipcache.NewIPCache(&amp;ipcache.Configuration&#123;\n\t\tIdentityAllocator: d.identityAllocator,\n\t\tPolicyHandler:     d.policy.GetSelectorCache(),\n\t\tDatapathHandler:   epMgr,\n\t&#125;)\n\tnodeMngr &#x3D; nodeMngr.WithIPCache(d.ipcache)\n\tnodeMngr &#x3D; nodeMngr.WithSelectorCacheUpdater(d.policy.GetSelectorCache()) &#x2F;&#x2F; must be after initPolicy\n\tnodeMngr &#x3D; nodeMngr.WithPolicyTriggerer(epMgr)                            &#x2F;&#x2F; must be after initPolicy\n\n\tproxy.Allocator &#x3D; d.identityAllocator\n\n\td.endpointManager &#x3D; epMgr\n\td.endpointManager.InitMetrics()\n\n\t&#x2F;&#x2F; Start the proxy before we start K8s watcher or restore endpoints so that we can inject\n\t&#x2F;&#x2F; the daemon&#39;s proxy into the k8s watcher and each endpoint.\n\t&#x2F;&#x2F; Note: d.endpointManager needs to be set before this\n\tbootstrapStats.proxyStart.Start()\n\t&#x2F;&#x2F; FIXME: Make the port range configurable.\n\tif option.Config.EnableL7Proxy &#123;\n\t\td.l7Proxy &#x3D; proxy.StartProxySupport(10000, 20000, option.Config.RunDir,\n\t\t\t&amp;d, option.Config.AgentLabels, d.datapath, d.endpointManager, d.ipcache)\n\t&#125; else &#123;\n\t\tlog.Info(&quot;L7 proxies are disabled&quot;)\n\t\tif option.Config.EnableEnvoyConfig &#123;\n\t\t\tlog.Warningf(&quot;%s is not functional when L7 proxies are disabled&quot;,\n\t\t\t\toption.EnableEnvoyConfig)\n\t\t&#125;\n\t&#125;\n\tbootstrapStats.proxyStart.End(true)\n\n\t&#x2F;&#x2F; Start service support after proxy support so that we can inject &#39;d.l7Proxy&#96;.\n\td.svc &#x3D; service.NewService(&amp;d, d.l7Proxy)\n\n\td.redirectPolicyManager &#x3D; redirectpolicy.NewRedirectPolicyManager(d.svc)\n\tif option.Config.BGPAnnounceLBIP || option.Config.BGPAnnouncePodCIDR &#123;\n\t\td.bgpSpeaker, err &#x3D; speaker.New(ctx, speaker.Opts&#123;\n\t\t\tLoadBalancerIP: option.Config.BGPAnnounceLBIP,\n\t\t\tPodCIDR:        option.Config.BGPAnnouncePodCIDR,\n\t\t&#125;)\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Error(&quot;Error creating new BGP speaker&quot;)\n\t\t\treturn nil, nil, err\n\t\t&#125;\n\t&#125;\n\n\tif option.Config.EnableIPv4EgressGateway &#123;\n\t\td.egressGatewayManager &#x3D; egressgateway.NewEgressGatewayManager(&amp;d)\n\t&#125;\n\n\td.k8sWatcher &#x3D; watchers.NewK8sWatcher(\n\t\td.endpointManager,\n\t\td.nodeDiscovery.Manager,\n\t\t&amp;d,\n\t\td.policy,\n\t\td.svc,\n\t\td.datapath,\n\t\td.redirectPolicyManager,\n\t\td.bgpSpeaker,\n\t\td.egressGatewayManager,\n\t\td.l7Proxy,\n\t\toption.Config,\n\t\td.ipcache,\n\t)\n\tnd.RegisterK8sNodeGetter(d.k8sWatcher)\n\td.ipcache.RegisterK8sSyncedChecker(&amp;d)\n\n\td.k8sWatcher.RegisterNodeSubscriber(d.endpointManager)\n\tif option.Config.BGPAnnounceLBIP || option.Config.BGPAnnouncePodCIDR &#123;\n\t\tswitch option.Config.IPAMMode() &#123;\n\t\tcase ipamOption.IPAMKubernetes:\n\t\t\td.k8sWatcher.RegisterNodeSubscriber(d.bgpSpeaker)\n\t\tcase ipamOption.IPAMClusterPool:\n\t\t\td.k8sWatcher.RegisterCiliumNodeSubscriber(d.bgpSpeaker)\n\t\t&#125;\n\t&#125;\n\tif option.Config.EnableServiceTopology &#123;\n\t\td.k8sWatcher.RegisterNodeSubscriber(&amp;d.k8sWatcher.K8sSvcCache)\n\t&#125;\n\n\td.k8sWatcher.NodeChain.Register(watchers.NewCiliumNodeUpdater(d.k8sWatcher))\n\n\td.redirectPolicyManager.RegisterSvcCache(&amp;d.k8sWatcher.K8sSvcCache)\n\td.redirectPolicyManager.RegisterGetStores(d.k8sWatcher)\n\tif option.Config.BGPAnnounceLBIP &#123;\n\t\td.bgpSpeaker.RegisterSvcCache(&amp;d.k8sWatcher.K8sSvcCache)\n\t&#125;\n\n\tbootstrapStats.daemonInit.End(true)\n\n\t&#x2F;&#x2F; Stop all endpoints (its goroutines) on exit.\n\tcleaner.cleanupFuncs.Add(func() &#123;\n\t\tlog.Info(&quot;Waiting for all endpoints&#39; go routines to be stopped.&quot;)\n\t\tvar wg sync.WaitGroup\n\n\t\teps :&#x3D; d.endpointManager.GetEndpoints()\n\t\twg.Add(len(eps))\n\n\t\tfor _, ep :&#x3D; range eps &#123;\n\t\t\tgo func(ep *endpoint.Endpoint) &#123;\n\t\t\t\tep.Stop()\n\t\t\t\twg.Done()\n\t\t\t&#125;(ep)\n\t\t&#125;\n\n\t\twg.Wait()\n\t\tlog.Info(&quot;All endpoints&#39; goroutines stopped.&quot;)\n\t&#125;)\n\n\t&#x2F;&#x2F; Open or create BPF maps.\n\tbootstrapStats.mapsInit.Start()\n\terr &#x3D; d.initMaps()\n\tbootstrapStats.mapsInit.EndError(err)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;error while opening&#x2F;creating BPF maps: %w&quot;, err)\n\t&#125;\n\n\t&#x2F;&#x2F; Read the service IDs of existing services from the BPF map and\n\t&#x2F;&#x2F; reserve them. This must be done *before* connecting to the\n\t&#x2F;&#x2F; Kubernetes apiserver and serving the API to ensure service IDs are\n\t&#x2F;&#x2F; not changing across restarts or that a new service could accidentally\n\t&#x2F;&#x2F; use an existing service ID.\n\t&#x2F;&#x2F; Also, create missing v2 services from the corresponding legacy ones.\n\tif option.Config.RestoreState &amp;&amp; !option.Config.DryMode &#123;\n\t\tbootstrapStats.restore.Start()\n\t\tif err :&#x3D; d.svc.RestoreServices(); err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Warn(&quot;Failed to restore services from BPF maps&quot;)\n\t\t&#125;\n\t\tbootstrapStats.restore.End(true)\n\t&#125;\n\n\tdebug.RegisterStatusObject(&quot;k8s-service-cache&quot;, &amp;d.k8sWatcher.K8sSvcCache)\n\tdebug.RegisterStatusObject(&quot;ipam&quot;, d.ipam)\n\tdebug.RegisterStatusObject(&quot;ongoing-endpoint-creations&quot;, d.endpointCreations)\n\n\td.k8sWatcher.RunK8sServiceHandler()\n\n\tif option.Config.DNSPolicyUnloadOnShutdown &#123;\n\t\tlog.Debugf(&quot;Registering cleanup function to unload DNS policies due to --%s&quot;, option.DNSPolicyUnloadOnShutdown)\n\n\t\t&#x2F;&#x2F; add to pre-cleanup funcs because this needs to run on graceful shutdown, but\n\t\t&#x2F;&#x2F; before the relevant subystems are being shut down.\n\t\tcleaner.preCleanupFuncs.Add(func() &#123;\n\t\t\t&#x2F;&#x2F; Stop k8s watchers\n\t\t\tlog.Info(&quot;Stopping k8s service handler&quot;)\n\t\t\td.k8sWatcher.StopK8sServiceHandler()\n\n\t\t\t&#x2F;&#x2F; Iterate over the policy repository and remove L7 DNS part\n\t\t\tneedsPolicyRegen :&#x3D; false\n\t\t\tremoveL7DNSRules :&#x3D; func(pr policyAPI.Ports) error &#123;\n\t\t\t\tportProtocols :&#x3D; pr.GetPortProtocols()\n\t\t\t\tif len(portProtocols) &#x3D;&#x3D; 0 &#123;\n\t\t\t\t\treturn nil\n\t\t\t\t&#125;\n\t\t\t\tportRule :&#x3D; pr.GetPortRule()\n\t\t\t\tif portRule &#x3D;&#x3D; nil || portRule.Rules &#x3D;&#x3D; nil &#123;\n\t\t\t\t\treturn nil\n\t\t\t\t&#125;\n\t\t\t\tdnsRules :&#x3D; portRule.Rules.DNS\n\t\t\t\tlog.Debugf(&quot;Found egress L7 DNS rules (portProtocol %#v): %#v&quot;, portProtocols[0], dnsRules)\n\n\t\t\t\t&#x2F;&#x2F; For security reasons, the L7 DNS policy must be a\n\t\t\t\t&#x2F;&#x2F; wildcard in order to trigger this logic.\n\t\t\t\t&#x2F;&#x2F; Otherwise we could invalidate the L7 security\n\t\t\t\t&#x2F;&#x2F; rules. This means if any of the DNS L7 rules\n\t\t\t\t&#x2F;&#x2F; have a matchPattern of * then it is OK to delete\n\t\t\t\t&#x2F;&#x2F; the L7 portion of those rules.\n\t\t\t\thasWildcard :&#x3D; false\n\t\t\t\tfor _, dns :&#x3D; range dnsRules &#123;\n\t\t\t\t\tif dns.MatchPattern &#x3D;&#x3D; &quot;*&quot; &#123;\n\t\t\t\t\t\thasWildcard &#x3D; true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t&#125;\n\t\t\t\t&#125;\n\t\t\t\tif hasWildcard &#123;\n\t\t\t\t\tportRule.Rules &#x3D; nil\n\t\t\t\t\tneedsPolicyRegen &#x3D; true\n\t\t\t\t&#125;\n\t\t\t\treturn nil\n\t\t\t&#125;\n\n\t\t\tpolicyRepo :&#x3D; d.GetPolicyRepository()\n\t\t\tpolicyRepo.Iterate(func(rule *policyAPI.Rule) &#123;\n\t\t\t\tfor _, er :&#x3D; range rule.Egress &#123;\n\t\t\t\t\t_ &#x3D; er.ToPorts.Iterate(removeL7DNSRules)\n\t\t\t\t&#125;\n\t\t\t&#125;)\n\n\t\t\tif !needsPolicyRegen &#123;\n\t\t\t\tlog.Infof(&quot;No policy recalculation needed to remove DNS rules due to --%s&quot;, option.DNSPolicyUnloadOnShutdown)\n\t\t\t\treturn\n\t\t\t&#125;\n\n\t\t\t&#x2F;&#x2F; Bump revision to trigger policy recalculation\n\t\t\tlog.Infof(&quot;Triggering policy recalculation to remove DNS rules due to --%s&quot;, option.DNSPolicyUnloadOnShutdown)\n\t\t\tpolicyRepo.BumpRevision()\n\t\t\tregenerationMetadata :&#x3D; &amp;regeneration.ExternalRegenerationMetadata&#123;\n\t\t\t\tReason:            &quot;unloading DNS rules on graceful shutdown&quot;,\n\t\t\t\tRegenerationLevel: regeneration.RegenerateWithoutDatapath,\n\t\t\t&#125;\n\t\t\twg :&#x3D; d.endpointManager.RegenerateAllEndpoints(regenerationMetadata)\n\t\t\twg.Wait()\n\t\t\tlog.Info(&quot;All endpoints regenerated after unloading DNS rules on graceful shutdown&quot;)\n\t\t&#125;)\n\t&#125;\n\n\ttreatRemoteNodeAsHost :&#x3D; option.Config.AlwaysAllowLocalhost() &amp;&amp; !option.Config.EnableRemoteNodeIdentity\n\tpolicyAPI.InitEntities(option.Config.ClusterName, treatRemoteNodeAsHost)\n\n\tbootstrapStats.restore.Start()\n\t&#x2F;&#x2F; fetch old endpoints before k8s is configured.\n\trestoredEndpoints, err :&#x3D; d.fetchOldEndpoints(option.Config.StateDir)\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Error(&quot;Unable to read existing endpoints&quot;)\n\t&#125;\n\tbootstrapStats.restore.End(true)\n\n\tbootstrapStats.fqdn.Start()\n\terr &#x3D; d.bootstrapFQDN(restoredEndpoints.possible, option.Config.ToFQDNsPreCache)\n\tif err !&#x3D; nil &#123;\n\t\tbootstrapStats.fqdn.EndError(err)\n\t\treturn nil, restoredEndpoints, err\n\t&#125;\n\tbootstrapStats.fqdn.End(true)\n\n\tif k8s.IsEnabled() &#123;\n\t\tbootstrapStats.k8sInit.Start()\n\t\t&#x2F;&#x2F; Errors are handled inside WaitForCRDsToRegister. It will fatal on a\n\t\t&#x2F;&#x2F; context deadline or if the context has been cancelled, the context&#39;s\n\t\t&#x2F;&#x2F; error will be returned. Otherwise, it succeeded.\n\t\tif err :&#x3D; d.k8sWatcher.WaitForCRDsToRegister(d.ctx); err !&#x3D; nil &#123;\n\t\t\treturn nil, restoredEndpoints, err\n\t\t&#125;\n\n\t\t&#x2F;&#x2F; Launch the K8s node watcher so we can start receiving node events.\n\t\t&#x2F;&#x2F; Launching the k8s node watcher at this stage will prevent all agents\n\t\t&#x2F;&#x2F; from performing Gets directly into kube-apiserver to get the most up\n\t\t&#x2F;&#x2F; to date version of the k8s node. This allows for better scalability\n\t\t&#x2F;&#x2F; in large clusters.\n\t\td.k8sWatcher.NodesInit(k8s.Client())\n\n\t\tif option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMClusterPool || option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMClusterPoolV2 &#123;\n\t\t\t&#x2F;&#x2F; Create the CiliumNode custom resource. This call will block until\n\t\t\t&#x2F;&#x2F; the custom resource has been created\n\t\t\td.nodeDiscovery.UpdateCiliumNodeResource()\n\t\t&#125;\n\n\t\tif err :&#x3D; k8s.WaitForNodeInformation(d.ctx, d.k8sWatcher); err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;unable to connect to get node spec from apiserver: %w&quot;, err)\n\t\t&#125;\n\n\t\t&#x2F;&#x2F; Kubernetes demands that the localhost can always reach local\n\t\t&#x2F;&#x2F; pods. Therefore unless the AllowLocalhost policy is set to a\n\t\t&#x2F;&#x2F; specific mode, always allow localhost to reach local\n\t\t&#x2F;&#x2F; endpoints.\n\t\tif option.Config.AllowLocalhost &#x3D;&#x3D; option.AllowLocalhostAuto &#123;\n\t\t\toption.Config.AllowLocalhost &#x3D; option.AllowLocalhostAlways\n\t\t\tlog.Info(&quot;k8s mode: Allowing localhost to reach local endpoints&quot;)\n\t\t&#125;\n\n\t\tbootstrapStats.k8sInit.End(true)\n\t&#125;\n\n\tif wgAgent :&#x3D; dp.WireguardAgent(); option.Config.EnableWireguard &#123;\n\t\tif err :&#x3D; wgAgent.(*wg.Agent).Init(d.ipcache, mtuConfig); err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;failed to initialize wireguard agent: %w&quot;, err)\n\t\t&#125;\n\t&#125;\n\n\t&#x2F;&#x2F; Perform an early probe on the underlying kernel on whether BandwidthManager\n\t&#x2F;&#x2F; can be supported or not. This needs to be done before handleNativeDevices()\n\t&#x2F;&#x2F; as BandwidthManager needs these to be available for setup.\n\tbandwidth.ProbeBandwidthManager()\n\n\t&#x2F;&#x2F; The kube-proxy replacement and host-fw devices detection should happen after\n\t&#x2F;&#x2F; establishing a connection to kube-apiserver, but before starting a k8s watcher.\n\t&#x2F;&#x2F; This is because the device detection requires self (Cilium)Node object,\n\t&#x2F;&#x2F; and the k8s service watcher depends on option.Config.EnableNodePort flag\n\t&#x2F;&#x2F; which can be modified after the device detection.\n\tif err :&#x3D; d.deviceManager.Detect(); err !&#x3D; nil &#123;\n\t\tif areDevicesRequired() &#123;\n\t\t\t&#x2F;&#x2F; Fail hard if devices are required to function.\n\t\t\treturn nil, nil, fmt.Errorf(&quot;failed to detect devices: %w&quot;, err)\n\t\t&#125;\n\t\tlog.WithError(err).Warn(&quot;failed to detect devices, disabling BPF NodePort&quot;)\n\t\tdisableNodePort()\n\t&#125;\n\tif err :&#x3D; finishKubeProxyReplacementInit(isKubeProxyReplacementStrict); err !&#x3D; nil &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;failed to finalise LB initialization: %w&quot;, err)\n\t&#125;\n\n\t&#x2F;&#x2F; BPF masquerade depends on BPF NodePort and require host-reachable svc to\n\t&#x2F;&#x2F; be fully enabled in the tunneling mode, so the following checks should\n\t&#x2F;&#x2F; happen after invoking initKubeProxyReplacementOptions().\n\tif option.Config.EnableIPv4Masquerade &amp;&amp; option.Config.EnableBPFMasquerade &amp;&amp;\n\t\t(!option.Config.EnableNodePort || option.Config.EgressMasqueradeInterfaces !&#x3D; &quot;&quot; || !option.Config.EnableRemoteNodeIdentity ||\n\t\t\t(option.Config.TunnelingEnabled() &amp;&amp; !hasFullHostReachableServices())) &#123;\n\n\t\tvar msg string\n\t\tswitch &#123;\n\t\tcase !option.Config.EnableNodePort:\n\t\t\tmsg &#x3D; fmt.Sprintf(&quot;BPF masquerade requires NodePort (--%s&#x3D;\\&quot;true\\&quot;).&quot;,\n\t\t\t\toption.EnableNodePort)\n\t\tcase !option.Config.EnableRemoteNodeIdentity:\n\t\t\tmsg &#x3D; fmt.Sprintf(&quot;BPF masquerade requires remote node identities (--%s&#x3D;\\&quot;true\\&quot;).&quot;,\n\t\t\t\toption.EnableRemoteNodeIdentity)\n\t\tcase option.Config.EgressMasqueradeInterfaces !&#x3D; &quot;&quot;:\n\t\t\tmsg &#x3D; fmt.Sprintf(&quot;BPF masquerade does not allow to specify devices via --%s (use --%s instead).&quot;,\n\t\t\t\toption.EgressMasqueradeInterfaces, option.Devices)\n\t\t&#125;\n\t\t&#x2F;&#x2F; ipt.InstallRules() (called by Reinitialize()) happens later than\n\t\t&#x2F;&#x2F; this  statement, so it&#39;s OK to fallback to iptables-based MASQ.\n\t\toption.Config.EnableBPFMasquerade &#x3D; false\n\t\tlog.Warn(msg + &quot; Falling back to iptables-based masquerading.&quot;)\n\t\t&#x2F;&#x2F; Too bad, if we need to revert to iptables-based MASQ, we also cannot\n\t\t&#x2F;&#x2F; use BPF host routing since we need the upper stack.\n\t\tif !option.Config.EnableHostLegacyRouting &#123;\n\t\t\toption.Config.EnableHostLegacyRouting &#x3D; true\n\t\t\tlog.Infof(&quot;BPF masquerade could not be enabled. Falling back to legacy host routing (--%s&#x3D;\\&quot;true\\&quot;).&quot;,\n\t\t\t\toption.EnableHostLegacyRouting)\n\t\t&#125;\n\t&#125;\n\tif option.Config.EnableIPv4EgressGateway &#123;\n\t\tif !probes.NewProbeManager().GetMisc().HaveLargeInsnLimit &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;egress gateway needs kernel 5.2 or newer&quot;)\n\t\t&#125;\n\n\t\t&#x2F;&#x2F; datapath code depends on remote node identities to distinguish between cluser-local and\n\t\t&#x2F;&#x2F; cluster-egress traffic\n\t\tif !option.Config.EnableRemoteNodeIdentity &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;egress gateway requires remote node identities (--%s&#x3D;\\&quot;true\\&quot;).&quot;,\n\t\t\t\toption.EnableRemoteNodeIdentity)\n\t\t&#125;\n\t&#125;\n\tif option.Config.EnableIPv4Masquerade &amp;&amp; option.Config.EnableBPFMasquerade &#123;\n\t\t&#x2F;&#x2F; TODO(brb) nodeport + ipvlan constraints will be lifted once the SNAT BPF code has been refactored\n\t\tif option.Config.DatapathMode &#x3D;&#x3D; datapathOption.DatapathModeIpvlan &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;BPF masquerade works only in veth mode (--%s&#x3D;\\&quot;%s\\&quot;&quot;, option.DatapathMode, datapathOption.DatapathModeVeth)\n\t\t&#125;\n\t\tif err :&#x3D; node.InitBPFMasqueradeAddrs(option.Config.Devices); err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;failed to determine BPF masquerade IPv4 addrs: %w&quot;, err)\n\t\t&#125;\n\t&#125; else if option.Config.EnableIPMasqAgent &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;BPF ip-masq-agent requires --%s&#x3D;\\&quot;true\\&quot; and --%s&#x3D;\\&quot;true\\&quot;&quot;, option.EnableIPv4Masquerade, option.EnableBPFMasquerade)\n\t&#125; else if option.Config.EnableIPv4EgressGateway &#123;\n\t\treturn nil, nil, fmt.Errorf(&quot;egress gateway requires --%s&#x3D;\\&quot;true\\&quot; and --%s&#x3D;\\&quot;true\\&quot;&quot;, option.EnableIPv4Masquerade, option.EnableBPFMasquerade)\n\t&#125; else if !option.Config.EnableIPv4Masquerade &amp;&amp; option.Config.EnableBPFMasquerade &#123;\n\t\t&#x2F;&#x2F; There is not yet support for option.Config.EnableIPv6Masquerade\n\t\tlog.Infof(&quot;Auto-disabling %q feature since IPv4 masquerading was generally disabled&quot;,\n\t\t\toption.EnableBPFMasquerade)\n\t\toption.Config.EnableBPFMasquerade &#x3D; false\n\t&#125;\n\tif option.Config.EnableIPMasqAgent &#123;\n\t\tif !option.Config.EnableIPv4 &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;BPF ip-masq-agent requires IPv4 support (--%s&#x3D;\\&quot;true\\&quot;)&quot;, option.EnableIPv4Name)\n\t\t&#125;\n\t\tif !probe.HaveFullLPM() &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;BPF ip-masq-agent needs kernel 4.16 or newer&quot;)\n\t\t&#125;\n\t&#125;\n\tif option.Config.EnableHostFirewall &amp;&amp; len(option.Config.Devices) &#x3D;&#x3D; 0 &#123;\n\t\tmsg :&#x3D; &quot;host firewall&#39;s external facing device could not be determined. Use --%s to specify.&quot;\n\t\treturn nil, nil, fmt.Errorf(msg, option.Devices)\n\t&#125;\n\n\t&#x2F;&#x2F; Some of the k8s watchers rely on option flags set above (specifically\n\t&#x2F;&#x2F; EnableBPFMasquerade), so we should only start them once the flag values\n\t&#x2F;&#x2F; are set.\n\tif k8s.IsEnabled() &#123;\n\t\tbootstrapStats.k8sInit.Start()\n\t\t&#x2F;&#x2F; Launch the K8s watchers in parallel as we continue to process other\n\t\t&#x2F;&#x2F; daemon options.\n\t\td.k8sCachesSynced &#x3D; d.k8sWatcher.InitK8sSubsystem(d.ctx)\n\t\tbootstrapStats.k8sInit.End(true)\n\t&#125;\n\n\tbootstrapStats.cleanup.Start()\n\terr &#x3D; clearCiliumVeths()\n\tbootstrapStats.cleanup.EndError(err)\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Warning(&quot;Unable to clean stale endpoint interfaces&quot;)\n\t&#125;\n\n\t&#x2F;&#x2F; Must init kvstore before starting node discovery\n\tif option.Config.KVStore &#x3D;&#x3D; &quot;&quot; &#123;\n\t\tlog.Info(&quot;Skipping kvstore configuration&quot;)\n\t&#125; else &#123;\n\t\tbootstrapStats.kvstore.Start()\n\t\td.initKVStore()\n\t\tbootstrapStats.kvstore.End(true)\n\t&#125;\n\n\t&#x2F;&#x2F; Fetch the router (&#96;cilium_host&#96;) IPs in case they were set a priori from\n\t&#x2F;&#x2F; the Kubernetes or CiliumNode resource in the K8s subsystem from call\n\t&#x2F;&#x2F; k8s.WaitForNodeInformation(). These will be used later after starting\n\t&#x2F;&#x2F; IPAM initialization to finish off the &#96;cilium_host&#96; IP restoration (part\n\t&#x2F;&#x2F; 2&#x2F;2).\n\trouter4FromK8s, router6FromK8s :&#x3D; node.GetInternalIPv4Router(), node.GetIPv6Router()\n\n\t&#x2F;&#x2F; Configure IPAM without using the configuration yet.\n\td.configureIPAM()\n\n\tif option.Config.JoinCluster &#123;\n\t\tif k8s.IsEnabled() &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;cannot join a Cilium cluster (--%s) when configured as a Kubernetes node&quot;, option.JoinClusterName)\n\t\t&#125;\n\t\tif option.Config.KVStore &#x3D;&#x3D; &quot;&quot; &#123;\n\t\t\treturn nil, nil, fmt.Errorf(&quot;joining a Cilium cluster (--%s) requires kvstore (--%s) be set&quot;, option.JoinClusterName, option.KVStore)\n\t\t&#125;\n\t\tagentLabels :&#x3D; labels.NewLabelsFromModel(option.Config.AgentLabels).K8sStringMap()\n\t\tif option.Config.K8sNamespace !&#x3D; &quot;&quot; &#123;\n\t\t\tagentLabels[k8sConst.PodNamespaceLabel] &#x3D; option.Config.K8sNamespace\n\t\t&#125;\n\t\tagentLabels[k8sConst.PodNameLabel] &#x3D; nodeTypes.GetName()\n\t\tagentLabels[k8sConst.PolicyLabelCluster] &#x3D; option.Config.ClusterName\n\t\t&#x2F;&#x2F; Set configured agent labels to local node for node registration\n\t\tnode.SetLabels(agentLabels)\n\n\t\t&#x2F;&#x2F; This can override node addressing config, so do this before starting IPAM\n\t\tlog.WithField(logfields.NodeName, nodeTypes.GetName()).Debug(&quot;Calling JoinCluster()&quot;)\n\t\td.nodeDiscovery.JoinCluster(nodeTypes.GetName())\n\n\t\t&#x2F;&#x2F; Start services watcher\n\t\tserviceStore.JoinClusterServices(&amp;d.k8sWatcher.K8sSvcCache, option.Config)\n\t&#125;\n\n\t&#x2F;&#x2F; Start IPAM\n\td.startIPAM()\n\n\t&#x2F;&#x2F; After the IPAM is started, in particular IPAM modes (CRD, ENI, Alibaba)\n\t&#x2F;&#x2F; which use the VPC CIDR as the pod CIDR, we must attempt restoring the\n\t&#x2F;&#x2F; router IPs from the K8s resources if we weren&#39;t able to restore them\n\t&#x2F;&#x2F; from the fs. We must do this after IPAM because we must wait until the\n\t&#x2F;&#x2F; K8s resources have been synced. Part 2&#x2F;2 of restoration.\n\tif option.Config.EnableIPv4 &#123;\n\t\trestoreCiliumHostIPs(false, router4FromK8s)\n\t&#125;\n\tif option.Config.EnableIPv6 &#123;\n\t\trestoreCiliumHostIPs(true, router6FromK8s)\n\t&#125;\n\n\t&#x2F;&#x2F; restore endpoints before any IPs are allocated to avoid eventual IP\n\t&#x2F;&#x2F; conflicts later on, otherwise any IP conflict will result in the\n\t&#x2F;&#x2F; endpoint not being able to be restored.\n\terr &#x3D; d.restoreOldEndpoints(restoredEndpoints, true)\n\tif err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Error(&quot;Unable to restore existing endpoints&quot;)\n\t&#125;\n\tbootstrapStats.restore.End(true)\n\n\tif err :&#x3D; d.allocateIPs(); err !&#x3D; nil &#123;\n\t\treturn nil, nil, err\n\t&#125;\n\n\t&#x2F;&#x2F; Must occur after d.allocateIPs(), see GH-14245 and its fix.\n\td.nodeDiscovery.StartDiscovery()\n\n\t&#x2F;&#x2F; Annotation of the k8s node must happen after discovery of the\n\t&#x2F;&#x2F; PodCIDR range and allocation of the health IPs.\n\tif k8s.IsEnabled() &amp;&amp; option.Config.AnnotateK8sNode &#123;\n\t\tbootstrapStats.k8sInit.Start()\n\t\tlog.WithFields(logrus.Fields&#123;\n\t\t\tlogfields.V4Prefix:       node.GetIPv4AllocRange(),\n\t\t\tlogfields.V6Prefix:       node.GetIPv6AllocRange(),\n\t\t\tlogfields.V4HealthIP:     node.GetEndpointHealthIPv4(),\n\t\t\tlogfields.V6HealthIP:     node.GetEndpointHealthIPv6(),\n\t\t\tlogfields.V4CiliumHostIP: node.GetInternalIPv4Router(),\n\t\t\tlogfields.V6CiliumHostIP: node.GetIPv6Router(),\n\t\t&#125;).Info(&quot;Annotating k8s node&quot;)\n\n\t\terr :&#x3D; k8s.Client().AnnotateNode(nodeTypes.GetName(),\n\t\t\tencryptKeyID,\n\t\t\tnode.GetIPv4AllocRange(), node.GetIPv6AllocRange(),\n\t\t\tnode.GetEndpointHealthIPv4(), node.GetEndpointHealthIPv6(),\n\t\t\tnode.GetInternalIPv4Router(), node.GetIPv6Router())\n\t\tif err !&#x3D; nil &#123;\n\t\t\tlog.WithError(err).Warning(&quot;Cannot annotate k8s node with CIDR range&quot;)\n\t\t&#125;\n\t\tbootstrapStats.k8sInit.End(true)\n\t&#125; else if !option.Config.AnnotateK8sNode &#123;\n\t\tlog.Debug(&quot;Annotate k8s node is disabled.&quot;)\n\t&#125;\n\n\t&#x2F;&#x2F; Trigger refresh and update custom resource in the apiserver with all restored endpoints.\n\t&#x2F;&#x2F; Trigger after nodeDiscovery.StartDiscovery to avoid custom resource update conflict.\n\tif option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMCRD || option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMENI || option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMAzure ||\n\t\toption.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMAlibabaCloud || option.Config.IPAM &#x3D;&#x3D; ipamOption.IPAMClusterPoolV2 &#123;\n\t\tif option.Config.EnableIPv6 &#123;\n\t\t\td.ipam.IPv6Allocator.RestoreFinished()\n\t\t&#125;\n\t\tif option.Config.EnableIPv4 &#123;\n\t\t\td.ipam.IPv4Allocator.RestoreFinished()\n\t\t&#125;\n\t&#125;\n\n\tif option.Config.DatapathMode !&#x3D; datapathOption.DatapathModeLBOnly &#123;\n\t\t&#x2F;&#x2F; This needs to be done after the node addressing has been configured\n\t\t&#x2F;&#x2F; as the node address is required as suffix.\n\t\t&#x2F;&#x2F; well known identities have already been initialized above.\n\t\t&#x2F;&#x2F; Ignore the channel returned by this function, as we want the global\n\t\t&#x2F;&#x2F; identity allocator to run asynchronously.\n\t\trealIdentityAllocator :&#x3D; d.identityAllocator\n\t\trealIdentityAllocator.InitIdentityAllocator(k8s.CiliumClient(), nil)\n\n\t\t&#x2F;&#x2F; Preallocate IDs for old CIDRs, must be called after InitIdentityAllocator\n\t\tif len(d.restoredCIDRs) &gt; 0 &#123;\n\t\t\tlog.Infof(&quot;Restoring %d old CIDR identities&quot;, len(d.restoredCIDRs))\n\t\t\t_, err &#x3D; d.ipcache.AllocateCIDRs(d.restoredCIDRs, oldNIDs, nil)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\tlog.WithError(err).Error(&quot;Error allocating old CIDR identities&quot;)\n\t\t\t&#125;\n\t\t&#125;\n\n\t\td.bootstrapClusterMesh(nodeMngr)\n\t&#125;\n\n\t&#x2F;&#x2F; Must be done at least after initializing BPF LB-related maps\n\t&#x2F;&#x2F; (lbmap.Init()).\n\tbootstrapStats.bpfBase.Start()\n\terr &#x3D; d.init()\n\tbootstrapStats.bpfBase.EndError(err)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, restoredEndpoints, fmt.Errorf(&quot;error while initializing daemon: %w&quot;, err)\n\t&#125;\n\n\t&#x2F;&#x2F; iptables rules can be updated only after d.init() intializes the iptables above.\n\terr &#x3D; d.updateDNSDatapathRules()\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, restoredEndpoints, err\n\t&#125;\n\n\t&#x2F;&#x2F; We can only attach the monitor agent once cilium_event has been set up.\n\tif option.Config.RunMonitorAgent &#123;\n\t\terr &#x3D; d.monitorAgent.AttachToEventsMap(defaults.MonitorBufferPages)\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn nil, nil, err\n\t\t&#125;\n\n\t\tif option.Config.EnableMonitor &#123;\n\t\t\terr &#x3D; monitoragent.ServeMonitorAPI(d.monitorAgent)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn nil, nil, err\n\t\t\t&#125;\n\t\t&#125;\n\t&#125;\n\n\tif err :&#x3D; d.syncEndpointsAndHostIPs(); err !&#x3D; nil &#123;\n\t\treturn nil, nil, err\n\t&#125;\n\n\t&#x2F;&#x2F; Start the controller for periodic sync. The purpose of the\n\t&#x2F;&#x2F; controller is to ensure that endpoints and host IPs entries are\n\t&#x2F;&#x2F; reinserted to the bpf maps if they are ever removed from them.\n\tcontroller.NewManager().UpdateController(&quot;sync-endpoints-and-host-ips&quot;,\n\t\tcontroller.ControllerParams&#123;\n\t\t\tDoFunc: func(ctx context.Context) error &#123;\n\t\t\t\treturn d.syncEndpointsAndHostIPs()\n\t\t\t&#125;,\n\t\t\tRunInterval: time.Minute,\n\t\t\tContext:     d.ctx,\n\t\t&#125;)\n\n\tif err :&#x3D; loader.RestoreTemplates(option.Config.StateDir); err !&#x3D; nil &#123;\n\t\tlog.WithError(err).Error(&quot;Unable to restore previous BPF templates&quot;)\n\t&#125;\n\n\t&#x2F;&#x2F; Start watcher for endpoint IP --&gt; identity mappings in key-value store.\n\t&#x2F;&#x2F; this needs to be done *after* init() for the daemon in that function,\n\t&#x2F;&#x2F; we populate the IPCache with the host&#39;s IP(s).\n\td.ipcache.InitIPIdentityWatcher()\n\tidentitymanager.Subscribe(d.policy)\n\n\treturn &amp;d, restoredEndpoints, nil\n&#125;\n\n\n\n\n打开conntrack/nat GC\n\n加载BPF程序  \n\ndaemon/cmd/datapath.go\n     &#x2F;&#x2F; initMaps opens all BPF maps (and creates them if they do not exist).\nfunc (d *Daemon) initMaps() error &#123;\n    lxcmap.LXCMap.OpenOrCreate()\n    ipcachemap.IPCache.OpenParallel()\n    ...\n    d.svc.InitMaps(Config.EnableIPv4, createSockRevNatMaps, Config.RestoreState)\n    policymap.InitCallMap()\n\n    for ep :&#x3D; range d.endpointManager.GetEndpoints()\n        ep.InitMap()\n\n    for ep :&#x3D; range d.endpointManager.GetEndpoints()\n        for m :&#x3D; range ctmap.LocalMaps(ep, Config.EnableIPv4)\n            m.Create()\n\n    for m :&#x3D; range ctmap.GlobalMaps(Config.EnableIPv4)\n        m.Create()\n\n    ipv4Nat :&#x3D; nat.GlobalMaps(Config.EnableIPv4)\n    ipv4Nat.Create()\n    if Config.EnableNodePort\n       neighborsmap.InitMaps(Config.EnableIPv4)\n\n    &#x2F;&#x2F; Set up the list of IPCache listeners in the daemon, to be used by syncEndpointsAndHostIPs()\n    ipcache.IPIdentityCache.SetListeners()\n\n    if !Config.RestoreState\n        lxcmap.LXCMap.DeleteAll() &#x2F;&#x2F; If we are not restoring state, all endpoints can be deleted.\n\n    if Config.EnableSessionAffinity &#123;\n        lbmap.AffinityMatchMap.OpenOrCreate()\n        lbmap.Affinity4Map.OpenOrCreate()\n    &#125;\n&#125;\n\nfunc (s *Service) RestoreServices() error &#123;\n    s.restoreBackendsLocked() &#x2F;&#x2F; Restore backend IDs\n    s.restoreServicesLocked() &#x2F;&#x2F; Restore service cache from BPF maps\n\n    if option.Config.EnableSessionAffinity\n        s.deleteOrphanAffinityMatchesLocked() &#x2F;&#x2F; Remove no longer existing affinity matches\n\n    s.deleteOrphanBackends() &#x2F;&#x2F; Remove obsolete backends and release their IDs\n&#125;\n\n\n\n\n启动检查，包括健康检查，状态检查，健康指标检查\n\n记录Agent启动到监控指标\n\n提供Cilium HTTP API\n\n标记启动状态\n\n打开可视化工具Hubble\n\n\n数据路径#kube-proxy包转发路径#数据路径：  \n\n网卡收到一个包（通过 DMA 放到 ring-buffer）；  \n包经过 XDP hook 点；  \n内核给包分配内存，此时才有了大家熟悉的 skb（包的内核结构体表示），然后 送到内核协议栈；  \n包经过 GRO 处理，对分片包进行重组；  \n包进入 tc（traffic control）的 ingress hook。接下来，所有橙色的框都是 Netfilter 处理点；  \nNetfilter：在 PREROUTING hook 点处理 raw table 里的 iptables 规则；  \n包经过内核的连接跟踪（conntrack）模块；  \nNetfilter：在 PREROUTING hook 点处理 mangle table 的 iptables 规则；  \nNetfilter：在 PREROUTING hook 点处理 nat table 的 iptables 规则；  \n进行路由判断（FIB：Forwarding Information Base，路由条目的内核表示，译者注） 。接下来又是四个 Netfilter 处理点；  \nNetfilter：在 FORWARD hook 点处理 mangle table 里的 iptables 规则；  \nNetfilter：在 FORWARD hook 点处理 filter table 里的 iptables 规则；  \nNetfilter：在 POSTROUTING hook 点处理 mangle table 里的 iptables 规则；  \nNetfilter：在 POSTROUTING hook 点处理 nat table 里的 iptables 规则；  \n包到达 TC egress hook 点，会进行出方向（egress）的判断，例如判断这个包是到本 地设备，还是到主机外；  \n对大包进行分片。根据 step 15 判断的结果，这个包接下来可能会；  \n发送到一个本机 veth 设备，或者一个本机 service endpoint；\n或者，如果目的 IP 是主机外，就通过网卡发出去。\n\nCilium eBPF包转发路径#\n对比可以看出，Cilium eBPF datapath 做了短路处理：从 tc ingress 直接 shortcut 到 tc egress，节省了 9 个中间步骤（总共 17 个）。更重要的是：这个 datapath 绕过了 整个 Netfilter 框架（橘黄色的框们），Netfilter 在大流量情况下性能是很差的。Cilium/eBPF 还能走的更远。例如，如果包的目的端是另一台主机上的 service endpoint，那你可以直接在 XDP 框中完成包的重定向（收包 1-&gt;2，在步骤 2 中对包 进行修改，再通过 2-&gt;1 发送出去），将其发送出去，如下图所示：\n可以看到，这种情况下包都没有进入内核协议栈（准确地说，都没有创建 skb）就被转 发出去了，性能可想而知。\n演示#下面通过minikube安装cilium来演示一下数据路径  \n\n查看路由表  \n# 查看路由列表\n&#x2F; # ip rule list\n# 查看main详情\n&#x2F; # ip route show table main\n可以看出，该容器下的所有包都由eth0发往10.0.0.156这个IP，接下我们看看ARP解析，看数据包发往哪？\n\n查看ARP信息\n# 向80端口发起请求\n&#x2F; # nc -v 1.1.1.1 80\n\n# 另一个session抓包\n   &#x2F; # tcpdump -n -vvv -i eth0\n可以看到ARP响应-&gt; Reply 10.0.0.156 is-at 32:cb:68:5c:d0:14\n\n查看网卡对信息  \n# 查看容器网卡信息，找到对应的网卡对\n&#x2F; # ip link\n\n# 查看宿主机对应网卡信息\n$ ip -c link | grep -A 1 35\n\n# 查看tc挂载\n$ tc filter show dev lxc2ba1e99c75fc egress\n$ tc filter show dev lxc2ba1e99c75fc ingress\n\n可以看到egress上没有挂载，但ingress 上则挂载了一段逻辑，section 为from-container，源码：\n\nbpf/bpf_lxc.c\n __section(&quot;from-container&quot;)\nint handle_xgress(struct __ctx_buff *ctx)\n&#123;\n\tswitch (proto) &#123;\n\t&#x2F;&#x2F; 判断协议为 ARP\n  case bpf_htons(ETH_P_ARP):\n\t\tunion macaddr mac &#x3D; NODE_MAC;\n\t  union macaddr smac;\n\t  __be32 sip;\n\t  __be32 tip;\n    &#x2F;&#x2F; 校验并填充 smac 为当前设备的物理地址\n\t  if (!arp_validate(ctx, &amp;mac, &amp;smac, &amp;sip, &amp;tip))\n\t\t  return CTX_ACT_OK;\n\n\t  if (tip &#x3D;&#x3D; LXC_IPV4)\n\t\t  return CTX_ACT_OK;\n\n     int ret &#x3D; arp_prepare_response(ctx, smac, sip, dmac, tip);\n     if (unlikely(ret !&#x3D; 0))\n\t\t   goto error;\n\n\t   ctx_redirect(ctx, ctx_get_ifindex(ctx), direction);\n     ret &#x3D; DROP_MISSED_TAIL_CALL;\n\t&#125;\n&#125;\n\n\n\n判断出这是一个来自容器的 ARP 协议包之后，Cilium 会在代码中构建出一个 ARP 响应，将当前设备的 MAC 地址设置为结果，然后通过内核提供的 eBPF 辅助函数 bpf_redirect跳过后续的路径。\n\n\n","slug":"Cilium实现原理","date":"2022-04-12T14:09:25.000Z","categories_index":"eBPF","tags_index":"CNI,eBPF,Kubernetes","author_index":"Asura"},{"id":"8b17daa7707d9d476a6f46015ac8ef41","title":"让内核开发变的简单的eBPF","content":"\n\n\n\n\n\n\n\n\neBPF 是一项革命性的技术，起源于 Linux 内核，可以在操作系统内核中运行沙盒程序。它用于安全有效地扩展内核的功能，而无需更改内核源代码或加载内核模块。\neBPF is a revolutionary technology with origins in the Linux kernel that can run sandboxed programs in an operating system kernel. It is used to safely and efficiently extend the capabilities of the kernel without requiring to change kernel source code or load kernel modules.\n\n\n\n\n\n\n发展历程\n由来\nBPF\neBPF\nBPF VS eBPF\n\n\n发展历程\n\n\n重要知识点\nHook Overview\nVerifier\nJIT\nMaps\n调用机制\nHelper Calls\nTailCalls\nBPF to BPF Calls\n\n\n\n\neBPF高级实现bpftrace\nbpftrace安装\nbpftrace使用\n\n\n参考文档\n\n\n\n\n\n发展历程#由来#我们先从eBPF的发展历程开始了解eBPF。了解一下eBPF是怎么一步步重新定义了数据面（datapath）的。  \nBPF#说到eBPF，首先必须要提的就是它的前身BPF，即BPF（Berkeley Packet Filter ），中文翻译为伯克利包过滤器，是类 Unix 系统上数据链路层的一种原始接口，提供原始链路层封包的收发。BPF在数据包过滤上开创性的引入了两大革新：  \n\n一个新的虚拟机 (VM) 设计，可以有效地工作在基于寄存器结构的 CPU 之上；\n应用程序使用缓存只复制与过滤数据包相关的数据，不会复制数据包的所有信息。这样可以最大程度地减少BPF 处理的数据这两项革新技术也使得数据包过滤技术比当时最先进的技术快20倍。我们熟知的tcpdump就是BPF的具体实现。  \n\neBPF#在2014年初，在eBPF的第一个patch被拒后的下一年，第一个eBPF patch合并到了内核，从此开启了eBPF的纪元，巧合的是同年Kubernetes也在这一年提交了第一个commit。eBPF（extended Berkeley Packet Filter），即扩展的伯克利包过滤器。eBPF最早出现在3.18内核中，从这以后BPF就被称为经典的BPF，即cBPF（classic BPF）。eBPF的特点有以下几点：\n\n不能crash内核（最重要的一点）\n执行起来与内核模块一样快\n提供稳定的API\n\nBPF VS eBPF#\n\n\n维度\ncBPF\neBPF\n\n\n\n内核版本\nLinux 2.1.75（1997年）\nLinux 3.18（2014年）[4.x for kprobe/uprobe/tracepoint/perf-event]\n\n\n寄存器数目\n2个：A, X\n10个： R0–R9, 另外 R10 是一个只读的帧指针\n\n\n寄存器宽度\n32位\n64位\n\n\n存储\n16 个内存位: M[0–15]\n512 字节堆栈，无限制大小的 “map” 存储\n\n\n限制的内核调用\n非常有限，仅限于 JIT 特定\n有限，通过 bpf_call 指令调用\n\n\n目标事件\n数据包、 seccomp-BPF\n数据包、内核函数、用户函数、跟踪点 PMCs 等\n\n\n发展历程#我们可以通过下面RoadMap看到eBPF的发展历程：  \n\n\n2013年这个时期有OpenvSwitch（OVS）、tc（Traffic control），以及内核子系统Netfilter(包括iptables、ipvs、nftables工具)，可以用来对datapath进行“编程”。同时期的tcpdump应用BPF在内核前端位置进行抓包。在这种非常限制的情况下，eBPF以能够创建新的datapath的特性提了一个“巨型”patch，然而因为侵入性太强遭拒。\n2014年第一个eBPF patch合并到内核。用一个扩展的指令集逐步替换原来老的BPF解释器，实现了自动新老BPF转换(in-kernel translation)。后续patch将eBPF暴露给UAPI，并添加了verifier代码和JIT代码。\n2015年eBPF分成了两个方向： networking和tracing。eBPF backend合并到LLVM 3.7。支持将eBPF attach到kprobes。通过cls_bpf使tc子系统可编程，为tc添加了一个lockless ingress和egress hook点。eBPF tracing项目bcc发布。\n2016年eBPF添加了一个新fast path: XDP，并合并到内核，支持驱动的ingress层attach BPF程序。Cilium项目发布。  \n2017年eBPF开始大规模应用于生产环境，如Netflix,facebook, Cloudflare等。添加BPF函数调用。提供了用户态依赖库libbpf。eBPF成为内核独立子系统。  \n2018年Cilium 1.0发布，标记着eBPF开始影响到K8S领域。  \n2019年bpftrace发布。BPF backend合并到GCC，至此，GCC和LLVM都支持了BPF backend。Cilium 1.6发布，第一次支持完全干掉基于iptables的kube-proxy  \n2020年Cilium 1.8发布，支持基于XDP的Service负载均衡和host network policies。eBPF在Linux security modules上开始发力。  \n\n重要知识点#在我开始了解eBPF知识之前，先来看看eBPF架构图\n\n从架构图中可以看出用户空间程序与内核中的BPF字节码交互的流程主要如下：  \n\n使用 LLVM 或者 GCC 工具将编写的 BPF 代码程序编译成 BPF 字节码；   \n使用加载程序 Loader 将字节码加载至内核；  \n内核使用验证器（Verfier） 组件保证执行字节码的安全性，以避免对内核造成灾难，在确认字节码安全后将其加载对应的内核模块执行；   \n内核中运行的 BPF 字节码程序可以使用两种方式将数据回传至用户空间：  \nmaps 方式可用于将内核中实现的统计摘要信息（比如测量延迟、堆栈信息）等回传至用户空间；  \nperf-event 用于将内核采集的事件实时发送至用户空间，用户空间程序实时读取分析；接下来我们来了解一下这其中的相关技术。  \n\n\n\nHook Overview#Linux Hook技术主要有四种：  \n\n内核模块hook\n应用层inline hook\n应用层Got hook\n应用层preload hook\n\n而BPF hook是Linux内核众多hook点中的一类。eBPF 程序都是事件驱动的，它们会在内核或者应用程序经过某个确定的 Hook 点的时候运行，这些 Hook 点都是提前定义的，包括系统调用、函数进入/退出、内核 tracepoints、网络事件等。如果针对某个特定需求的 Hook 点不存在，可以通过 kprobe 或者 uprobe 来在内核或者用户程序的几乎所有地方挂载 eBPF 程序。更多Hook信息可以看这里。\nVerifier#eBPF程序能挂载在内核的大部分地方，这样带来的风险是及其巨大的，如果程序不正确极有可能导致内核崩溃，这时一种可靠的校验是十分重要的。Verifier就是那个强大的保障，保证每个eBPF程序加载到内核之前都是正确和安全的，主要校验以下几点：  \n\n要保证 加载 eBPF 程序的进程有必要的特权级，除非节点开启了 unpriviledged 特性，只有特权级的程序才能够加载 eBPF 程序  \n内核提供了一个配置项 /proc/sys/kernel/unprivileged_bpf_disabled 来禁止非特权用户使用 bpf(2) 系统调用，可以通过 sysctl 命令修改  \n比较特殊的一点是，这个配置项特意设计为一次性开关（one-time kill switch）， 这意味着一旦将它设为 1，就没有办法再改为 0 了，除非重启内核  \n一旦设置为 1 之后，只有初始命名空间中有 CAP_SYS_ADMIN 特权的进程才可以调用 bpf(2) 系统调用 。Cilium 启动后也会将这个配置项设为 1：\n\n\n要保证 eBPF 程序不会崩溃或者使得系统出故障\n要保证 eBPF 程序不能陷入死循环，能够 runs to completion\n要保证 eBPF 程序必须满足系统要求的大小，过大的 eBPF 程序不允许被加载进内核\n要保证 eBPF 程序的复杂度有限，Verifier 将会评估 eBPF 程序所有可能的执行路径，必须能够在有限时间内完成 eBPF 程序复杂度分析  \n\nJIT#Just-In-Time(JIT) 编译用来将通用的 eBPF 字节码翻译成与机器相关的指令集，从而极大加速 BPF 程序的执行：  \n\n与解释器相比，它们可以降低每个指令的开销。通常，指令可以 1:1 映射到底层架构的原生指令  \n这也会减少生成的可执行镜像的大小，因此对 CPU 的指令缓存更友好  \n特别地，对于 CISC 指令集（例如 x86），JIT 做了很多特殊优化，目的是为给定的指令产生可能的最短操作码，以降低程序翻译过程所需的空间64 位的 x86_64、arm64、ppc64、s390x、mips64、sparc64 和 32 位的 arm 、x86_32 架构都内置了 in-kernel eBPF JIT 编译器，它们的功能都是一样的，可以用如下方式打开：$ echo 1 &gt; /proc/sys/net/core/bpf_jit_enable\n\n32 位的 mips、ppc 和 sparc 架构目前内置的是一个 cBPF JIT 编译器。这些只有 cBPF JIT 编译器的架构，以及那些甚至完全没有 BPF JIT 编译器的架构，需要通过内核中的解释器（in-kernel interpreter）执行 eBPF 程序。  \nMaps#Map是内核中用来保存信息的重要数据结构，BPF自然也有属于自己的Map数据结构。BPF Map 是驻留在内核空间中的高效 Key/Value store，包含多种类型的 Map，由内核实现其功能。BPF Map 的交互场景有以下几种：  \n\nBPF 程序和用户态程序的交互：BPF 程序运行完，得到的结果存储到 map 中，供用户态程序通过文件描述符访问  \nBPF 程序和内核态程序的交互：和 BPF 程序以外的内核程序交互，也可以使用 map 作为中介  \nBPF 程序间交互：如果 BPF 程序内部需要用全局变量来交互，但是由于安全原因 BPF 程序不允许访问全局变量，可以使用 map 来充当全局变量  \nBPF Tail call：Tail call 是一个 BPF 程序跳转到另一 BPF 程序，BPF 程序首先通过 BPF_MAP_TYPE_PROG_ARRAY 类型的 map 来知道另一个 BPF 程序的指针，然后调用 tail_call() 的 helper function 来执行 Tail call  \n\n共享 map 的 BPF 程序不要求是相同的程序类型，例如 tracing 程序可以和网络程序共享 map，单个 BPF 程序目前最多可直接访问 64 个不同 map。\n当前可用的 通用 map 有：  \n\nBPF_MAP_TYPE_HASH  \nBPF_MAP_TYPE_ARRAY  \nBPF_MAP_TYPE_PERCPU_HASH  \nBPF_MAP_TYPE_PERCPU_ARRAY  \nBPF_MAP_TYPE_LRU_HASH  \nBPF_MAP_TYPE_LRU_PERCPU_HASH  \nBPF_MAP_TYPE_LPM_TRIE  \n\n以上 map 都使用相同的一组 BPF 辅助函数来执行查找、更新或删除操作，但各自实现了不同的后端，这些后端各有不同的语义和性能特点。随着多 CPU 架构的成熟发展，BPF Map 也引入了 per-cpu 类型，如BPF_MAP_TYPE_PERCPU_HASH、BPF_MAP_TYPE_PERCPU_ARRAY等，当你使用这种类型的 BPF Map 时，每个 CPU 都会存储并看到它自己的 Map 数据，从属于不同 CPU 之间的数据是互相隔离的，这样做的好处是，在进行查找和聚合操作时更加高效，性能更好，尤其是你的 BPF 程序主要是在做收集时间序列型数据，如流量数据或指标等。\n当前内核中的 非通用 map 有：  \n\nBPF_MAP_TYPE_PROG_ARRAY：一个数组 map，用于 hold 其他的 BPF 程序\nBPF_MAP_TYPE_PERF_EVENT_ARRAY  \nBPF_MAP_TYPE_CGROUP_ARRAY：用于检查 skb 中的 cgroup2 成员信息  \nBPF_MAP_TYPE_STACK_TRACE：用于存储栈跟踪的 MAP  \nBPF_MAP_TYPE_ARRAY_OF_MAPS：持有（hold） 其他 map 的指针，这样整个 map 就可以在运行时实现原子替换  \nBPF_MAP_TYPE_HASH_OF_MAPS：持有（hold） 其他 map 的指针，这样整个 map 就可以在运行时实现原子替换  \n\n调用机制#Helper Calls#eBPF程序不能够随意调用内核函数，如果这么做的话会导致 eBPF 程序与特定的内核版本绑定，相反它内核定义的一系列 Helper functions。Helper functions 使得 BPF 能够通过一组内核定义的稳定的函数调用来从内核中查询数据，或者将数据推送到内核。  \nTailCalls#尾调用的机制是指：一个 BPF 程序可以调用另一个 BPF 程序，并且调用完成后不用返回到原来的程序。  \n\n和普通函数调用相比，这种调用方式开销最小，因为它是用长跳转（long jump）实现的，复用了原来的栈帧 （stack frame）  \nBPF 程序都是独立验证的，因此要传递状态，要么使用 per-CPU map 作为 scratch 缓冲区 ，要么如果是 tc 程序的话，还可以使用 skb 的某些字段（例如 cb[]）  \n相同类型的程序才可以尾调用，而且它们还要与 JIT 编译器相匹配，因此要么是 JIT 编译执行，要么是解释器执行（invoke interpreted programs），但不能同时使用两种方式  \n\nBPF to BPF Calls#除了 BPF 辅助函数和 BPF 尾调用之外，BPF 核心基础设施最近刚加入了一个新特性：BPF to BPF calls。当 LLVM 编译和生成 BPF 对象文件时，所有这些函数将被内联，因此会在生成的对象文件中重 复多次，导致代码尺寸膨胀。BPF 到 BPF 调用是一个重要的性能优化，极大减小了生成的 BPF 代码大小，因此 对 CPU 指令缓存（instruction cache，i-cache）更友好。\neBPF高级实现bpftrace#BPFTrace 是基于 BPF 和 BCC 的开源项目，与 BCC 不同的是其提供了更高层次的抽象，可以使用类似 AWK 脚本语言来编写基于 BPF 的跟踪或者性能排查工具，更加易于入门和编写，该工具的主要灵感来自于 Solaris 的 D 语言。BPFTrace 更方便与编写单行的程序。\nbpftrace安装#bpftrace支持多种安装方式，具体安装步骤可以看这里。  \nbpftrace使用#\n根据不同的tracepoint查看：\n# 统计进程调用sys_enter的次数\nbpftrace -e &#39;tracepoint:raw_syscalls:sys_enter &#123; @[comm] &#x3D; count(); &#125;&#39;\n\n\n# 统计内核中函数堆栈的次数\nbpftrace -e &#39;profile:hz:99 &#123; @[kstack] &#x3D; count(); &#125;&#39;\n\n\n还可以使用内部脚本进行追踪：\n# \b内核函数调用情况查看\n.&#x2F;tools&#x2F;syscount.bt\n\n参考文档#\nhttps://ebpf.io/what-is-ebpf#hook-overview\nhttps://cloudnative.to/blog/bpf-intro/#43-bpftrace\nhttps://mp.weixin.qq.com/s/zCjk5WmnwLD0J3J9gC4e0Q\n\n\n\n","slug":"让内核开发变的简单的eBPF","date":"2022-04-10T12:43:44.000Z","categories_index":"eBPF","tags_index":"eBPF,Linux","author_index":"Asura"},{"id":"78cf92f2c0639bec80519c22649cfa6b","title":"Prometheus数据流","content":"\n\n\n\n\n\n\n\n\nPrometheus是一个开源的监控和报警工具。它优秀的设计理念，灵活的扩展，丰富的生态以及活跃的社区使它正在成为众多开发者喜爱的监控工具。\n\n\n\n\n\n简介\n数据拉取\nMetrics数据\n时间向量\nSample\nMetric\n构造Metrics\n\n\n数据抓取\nJob\nTarget\n采集方式\n服务发现\n\n\n\n\n数据存储\nRemoteWrite\nRemoteRead\n数据存储\n存储结构\n数据分析\n\n\n\n\n报警\n查询\n倒排索引\n\n\n\n\n\n\n\n简介#在开始之前，我们先看看官方给出的Prometheus的生态架构图：从图上我们可以看出Prometheus数据流转有几处比较核心：  \n\n拉取数据(pull metrics)\n采集任务(Job)\n服务发现(ServiceDiscovery)\n数据存储(TSDB)\n报警(Alert)\n查询(PromQL)\n\n数据拉取#Metrics数据#在了解Prometheus数据流转之前，需要对流转的数据有充分的认识，及Metrics。下面是一条Metrics数据的数据结构，由指标名，标签集，数值和时间戳构成。\ngo_gc_duration_seconds&#123;quantile&#x3D;&quot;0&quot;&#125; 3.4524e-05@1639549978.439\n|--------指标名--------|-labelsets--| |--value--||---timestamp--|\n\n时间向量#以时间方向，保存的数值，分为Instant vector（瞬时向量）和Range vector（区间向量）时序数据有“垂直写，水平读”的模式，在同一个时间点上垂直写入多个指标数据，读取一段时间的某个指标的数值  \n^\n│   . . . . . . . . . . . . . . . . .   . .   node_cpu_seconds_total&#123;cpu&#x3D;&quot;cpu0&quot;,mode&#x3D;&quot;idle&quot;&#125;\n│     . . . . . . . . . . . . . . . . . . .   node_cpu_seconds_total&#123;cpu&#x3D;&quot;cpu0&quot;,mode&#x3D;&quot;system&quot;&#125;\n│     . . . . . . . . . .   . . . . . . . .   node_load1&#123;&#125;\n│     . . . . . . . . . . . . . . . .   . .  \nv\n  &lt;------------------ 时间 ----------------&gt;\nSample#上图中的每一个点都是一个Sample（样本数据），每个Sample包含一个时间戳和float64的数值，Sample是Prometheus中最小的数据存储单位。\ntype Sample struct &#123;\n\tValue float64 &#96;protobuf:&quot;fixed64,1,opt,name&#x3D;value,proto3&quot; json:&quot;value,omitempty&quot;&#96;\n\t&#x2F;&#x2F; timestamp is in ms format, see pkg&#x2F;timestamp&#x2F;timestamp.go for\n\t&#x2F;&#x2F; conversion from time.Time to Prometheus timestamp.\n\tTimestamp            int64    &#96;protobuf:&quot;varint,2,opt,name&#x3D;timestamp,proto3&quot; json:&quot;timestamp,omitempty&quot;&#96;\n\tXXX_NoUnkeyedLiteral struct&#123;&#125; &#96;json:&quot;-&quot;&#96;\n\tXXX_unrecognized     []byte   &#96;json:&quot;-&quot;&#96;\n\tXXX_sizecache        int32    &#96;json:&quot;-&quot;&#96;\n&#125;\nMetric#一个Metric包含指标名和LabelSets，能唯一标识出监控数据的含义\ntype metricMap struct &#123;\n\tmtx       sync.RWMutex &#x2F;&#x2F; Protects metrics.\n\tmetrics   map[uint64][]metricWithLabelValues\n\tdesc      *Desc\n\tnewMetric func(labelValues ...string) Metric\n&#125;\n\ntype metricWithLabelValues struct &#123;\n\tvalues []string\n\tmetric Metric\n&#125;\n\n构造Metrics#以Prometheus SDK client_golang为例，使用SDK构造Metrics的流程：\n\n\n\n\n\n\n\n\n\n声明不同类型Metric的Collector —&gt;  注册Collectors到本地Register —&gt;  暴露端口  —&gt;  数据存储到Map输出\nMetric类型包括下面四种：\n\nCounter：计数类型指标，只增不减\n\n\nCounter Struct\ntype counter struct {    // valBits contains the bits of the represented float64 value, while    // valInt stores values that are exact integers. Both have to go first    // in the struct to guarantee alignment for atomic operations.    // http://golang.org/pkg/sync/atomic/#pkg-note-BUG    valBits uint64    valInt  uint64\nselfCollector\ndesc *Desc\n\nlabelPairs []*dto.LabelPair\nexemplar   atomic.Value // Containing nil or a *dto.Exemplar.\n\nnow func() time.Time // To mock out time.Now() for testing.\n}\ntype CounterVec struct {    *MetricVec}\ntype Counter interface {    Metric    Collector\n// Inc increments the counter by 1. Use Add to increment it by arbitrary\n// non-negative values.\nInc()\n// Add adds the given value to the counter. It panics if the value is &lt;\n// 0.\nAdd(float64)\n}\n\n\nGauge：瞬时指标，主要反映瞬时状态\n\n\nGauge Struct\ntype gauge struct {    // valBits contains the bits of the represented float64 value. It has    // to go first in the struct to guarantee alignment for atomic    // operations.  http://golang.org/pkg/sync/atomic/#pkg-note-BUG    valBits uint64\nselfCollector\n\ndesc       *Desc\nlabelPairs []*dto.LabelPair\n}\ntype GaugeVec struct {    *MetricVec}\ntype Gauge interface {    Metric    Collector\n// Set sets the Gauge to an arbitrary value.\nSet(float64)\n// Inc increments the Gauge by 1. Use Add to increment it by arbitrary\n// values.\nInc()\n// Dec decrements the Gauge by 1. Use Sub to decrement it by arbitrary\n// values.\nDec()\n// Add adds the given value to the Gauge. (The value can be negative,\n// resulting in a decrease of the Gauge.)\nAdd(float64)\n// Sub subtracts the given value from the Gauge. (The value can be\n// negative, resulting in an increase of the Gauge.)\nSub(float64)\n\n// SetToCurrentTime sets the Gauge to the current Unix time in seconds.\nSetToCurrentTime()\n}\n\n\nHistogram：直方图统计，对指标比较全面的描述， 包括sum和count及不同区间的统计，le标签标记\n\n\nHistogram Struct\ntype histogram struct {    // countAndHotIdx enables lock-free writes with use of atomic updates.    // The most significant bit is the hot index [0 or 1] of the count field    // below. Observe calls update the hot one. All remaining bits count the    // number of Observe calls. Observe starts by incrementing this counter,    // and finish by incrementing the count field in the respective    // histogramCounts, as a marker for completion.    //    // Calls of the Write method (which are non-mutating reads from the    // perspective of the histogram) swap the hot–cold under the writeMtx    // lock. A cooldown is awaited (while locked) by comparing the number of    // observations with the initiation count. Once they match, then the    // last observation on the now cool one has completed. All cool fields must    // be merged into the new hot before releasing writeMtx.    //    // Fields with atomic access first! See alignment constraint:    // http://golang.org/pkg/sync/atomic/#pkg-note-BUG    countAndHotIdx uint64\nselfCollector\ndesc     *Desc\nwriteMtx sync.Mutex // Only used in the Write method.\n\n// Two counts, one is &quot;hot&quot; for lock-free observations, the other is\n// &quot;cold&quot; for writing out a dto.Metric. It has to be an array of\n// pointers to guarantee 64bit alignment of the histogramCounts, see\n// http://golang.org/pkg/sync/atomic/#pkg-note-BUG.\ncounts [2]*histogramCounts\n\nupperBounds []float64\nlabelPairs  []*dto.LabelPair\nexemplars   []atomic.Value // One more than buckets (to include +Inf), each a *dto.Exemplar.\n\nnow func() time.Time // To mock out time.Now() for testing.\n}\ntype HistogramOpts struct {    // Namespace, Subsystem, and Name are components of the fully-qualified    // name of the Histogram (created by joining these components with    // “_”). Only Name is mandatory, the others merely help structuring the    // name. Note that the fully-qualified name of the Histogram must be a    // valid Prometheus metric name.    Namespace string    Subsystem string    Name      string\n// Help provides information about this Histogram.\n//\n// Metrics with the same fully-qualified name must have the same Help\n// string.\nHelp string\n\n// ConstLabels are used to attach fixed labels to this metric. Metrics\n// with the same fully-qualified name must have the same label names in\n// their ConstLabels.\n//\n// ConstLabels are only used rarely. In particular, do not use them to\n// attach the same labels to all your metrics. Those use cases are\n// better covered by target labels set by the scraping Prometheus\n// server, or by one specific metric (e.g. a build_info or a\n// machine_role metric). See also\n// https://prometheus.io/docs/instrumenting/writing_exporters/#target-labels-not-static-scraped-labels\nConstLabels Labels\n\n// Buckets defines the buckets into which observations are counted. Each\n// element in the slice is the upper inclusive bound of a bucket. The\n// values must be sorted in strictly increasing order. There is no need\n// to add a highest bucket with +Inf bound, it will be added\n// implicitly. The default value is DefBuckets.\nBuckets []float64\n}\ntype histogramCounts struct {    // sumBits contains the bits of the float64 representing the sum of all    // observations. sumBits and count have to go first in the struct to    // guarantee alignment for atomic operations.    // http://golang.org/pkg/sync/atomic/#pkg-note-BUG    sumBits uint64    count   uint64    buckets []uint64}\ntype HistogramVec struct {    *MetricVec}\ntype Histogram interface {    Metric    Collector\n// Observe adds a single observation to the histogram. Observations are\n// usually positive or zero. Negative observations are accepted but\n// prevent current versions of Prometheus from properly detecting\n// counter resets in the sum of observations. See\n// https://prometheus.io/docs/practices/histograms/#count-and-sum-of-observations\n// for details.\nObserve(float64)\n}\n\n\nSummary：和Histogram类似，包括sum和count及不同分位值，quantile标记\n\n\nSummary Struct\ntype summary struct {    selfCollector\nbufMtx sync.Mutex // Protects hotBuf and hotBufExpTime.\nmtx    sync.Mutex // Protects every other moving part.\n// Lock bufMtx before mtx if both are needed.\n\ndesc *Desc\n\nobjectives       map[float64]float64\nsortedObjectives []float64\n\nlabelPairs []*dto.LabelPair\n\nsum float64\ncnt uint64\n\nhotBuf, coldBuf []float64\n\nstreams                          []*quantile.Stream\nstreamDuration                   time.Duration\nheadStream                       *quantile.Stream\nheadStreamIdx                    int\nheadStreamExpTime, hotBufExpTime time.Time\n}\ntype SummaryVec struct {    *MetricVec}\ntype SummaryOpts struct {    // Namespace, Subsystem, and Name are components of the fully-qualified    // name of the Summary (created by joining these components with    // “_”). Only Name is mandatory, the others merely help structuring the    // name. Note that the fully-qualified name of the Summary must be a    // valid Prometheus metric name.    Namespace string    Subsystem string    Name      string\n// Help provides information about this Summary.\n//\n// Metrics with the same fully-qualified name must have the same Help\n// string.\nHelp string\n\n// ConstLabels are used to attach fixed labels to this metric. Metrics\n// with the same fully-qualified name must have the same label names in\n// their ConstLabels.\n//\n// Due to the way a Summary is represented in the Prometheus text format\n// and how it is handled by the Prometheus server internally, “quantile”\n// is an illegal label name. Construction of a Summary or SummaryVec\n// will panic if this label name is used in ConstLabels.\n//\n// ConstLabels are only used rarely. In particular, do not use them to\n// attach the same labels to all your metrics. Those use cases are\n// better covered by target labels set by the scraping Prometheus\n// server, or by one specific metric (e.g. a build_info or a\n// machine_role metric). See also\n// https://prometheus.io/docs/instrumenting/writing_exporters/#target-labels-not-static-scraped-labels\nConstLabels Labels\n\n// Objectives defines the quantile rank estimates with their respective\n// absolute error. If Objectives[q] = e, then the value reported for q\n// will be the φ-quantile value for some φ between q-e and q+e.  The\n// default value is an empty map, resulting in a summary without\n// quantiles.\nObjectives map[float64]float64\n\n// MaxAge defines the duration for which an observation stays relevant\n// for the summary. Only applies to pre-calculated quantiles, does not\n// apply to _sum and _count. Must be positive. The default value is\n// DefMaxAge.\nMaxAge time.Duration\n\n// AgeBuckets is the number of buckets used to exclude observations that\n// are older than MaxAge from the summary. A higher number has a\n// resource penalty, so only increase it if the higher resolution is\n// really required. For very high observation rates, you might want to\n// reduce the number of age buckets. With only one age bucket, you will\n// effectively see a complete reset of the summary each time MaxAge has\n// passed. The default value is DefAgeBuckets.\nAgeBuckets uint32\n\n// BufCap defines the default sample stream buffer size.  The default\n// value of DefBufCap should suffice for most uses. If there is a need\n// to increase the value, a multiple of 500 is recommended (because that\n// is the internal buffer size of the underlying package\n// &quot;github.com/bmizerany/perks/quantile&quot;).\nBufCap uint32\n}\ntype SummaryVec struct {    *MetricVec}\ntype MetricVec struct {    *metricMap\ncurry []curriedLabelValue\n\n// hashAdd and hashAddByte can be replaced for testing collision handling.\nhashAdd     func(h uint64, s string) uint64\nhashAddByte func(h uint64, b byte) uint64\n}\ntype metricMap struct {    mtx       sync.RWMutex // Protects metrics.    metrics   map[uint64][]metricWithLabelValues    desc      *Desc    newMetric func(labelValues …string) Metric}\n\n数据抓取#Prometheus抓取数据是通过配置不同的抓取任务进行的，我们先看看Prometheus抓取代码：\n\nscrape.go\n&#x2F;&#x2F; 新建抓取线程池\nfunc newScrapePool(cfg *config.ScrapeConfig, app storage.Appendable, jitterSeed uint64, logger log.Logger, reportExtraMetrics bool, httpOpts []config_util.HTTPClientOption) (*scrapePool, error) &#123;\n\ttargetScrapePools.Inc()\n\tif logger &#x3D;&#x3D; nil &#123;\n\t\tlogger &#x3D; log.NewNopLogger()\n\t&#125;\n\n    &#x2F;&#x2F; 构建抓取所用HTTP Client\n\tclient, err :&#x3D; config_util.NewClientFromConfig(cfg.HTTPClientConfig, cfg.JobName, httpOpts...)\n\tif err !&#x3D; nil &#123;\n\t\ttargetScrapePoolsFailed.Inc()\n\t\treturn nil, errors.Wrap(err, &quot;error creating HTTP client&quot;)\n\t&#125;\n\n\tbuffers :&#x3D; pool.New(1e3, 100e6, 3, func(sz int) interface&#123;&#125; &#123; return make([]byte, 0, sz) &#125;)\n\n\tctx, cancel :&#x3D; context.WithCancel(context.Background())\n    &#x2F;&#x2F; 构建一个抓取线程池\n\tsp :&#x3D; &amp;scrapePool&#123;\n\t\tcancel:        cancel,\n\t\tappendable:    app,\n\t\tconfig:        cfg,\n\t\tclient:        client,\n\t\tactiveTargets: map[uint64]*Target&#123;&#125;,\n\t\tloops:         map[uint64]loop&#123;&#125;,\n\t\tlogger:        logger,\n\t\thttpOpts:      httpOpts,\n\t&#125;\n\tsp.newLoop &#x3D; func(opts scrapeLoopOptions) loop &#123;\n\t\t&#x2F;&#x2F; Update the targets retrieval function for metadata to a new scrape cache.\n\t\tcache :&#x3D; opts.cache\n\t\tif cache &#x3D;&#x3D; nil &#123;\n\t\t\tcache &#x3D; newScrapeCache()\n\t\t&#125;\n\t\topts.target.SetMetadataStore(cache)\n\n\t\t&#x2F;&#x2F; Store the cache in the context.\n\t\tloopCtx :&#x3D; ContextWithMetricMetadataStore(ctx, cache)\n\t\tloopCtx &#x3D; ContextWithTarget(loopCtx, opts.target)\n\n        &#x2F;&#x2F; Loop执行抓取\n\t\treturn newScrapeLoop(\n\t\t\tloopCtx,\n\t\t\topts.scraper,\n\t\t\tlog.With(logger, &quot;target&quot;, opts.target),\n\t\t\tbuffers,\n\t\t\tfunc(l labels.Labels) labels.Labels &#123;\n\t\t\t\treturn mutateSampleLabels(l, opts.target, opts.honorLabels, opts.mrc)\n\t\t\t&#125;,\n\t\t\tfunc(l labels.Labels) labels.Labels &#123; return mutateReportSampleLabels(l, opts.target) &#125;,\n\t\t\tfunc(ctx context.Context) storage.Appender &#123; return app.Appender(ctx) &#125;,\n\t\t\tcache,\n\t\t\tjitterSeed,\n\t\t\topts.honorTimestamps,\n\t\t\topts.sampleLimit,\n\t\t\topts.labelLimits,\n\t\t\topts.interval,\n\t\t\topts.timeout,\n\t\t\treportExtraMetrics,\n\t\t)\n\t&#125;\n\n\treturn sp, nil\n&#125;\n\n&#x2F;&#x2F; 定时循环抓取数据\nfunc (sl *scrapeLoop) run(errc chan&lt;- error) &#123;\n\tselect &#123;\n\tcase &lt;-time.After(sl.scraper.offset(sl.interval, sl.jitterSeed)):\n\t\t&#x2F;&#x2F; Continue after a scraping offset.\n\tcase &lt;-sl.ctx.Done():\n\t\tclose(sl.stopped)\n\t\treturn\n\t&#125;\n\n\tvar last time.Time\n\n\talignedScrapeTime :&#x3D; time.Now().Round(0)\n    &#x2F;&#x2F; 根据配置抓取间隔\n\tticker :&#x3D; time.NewTicker(sl.interval)\n\tdefer ticker.Stop()\n\nmainLoop:\n\tfor &#123;\n\t\tselect &#123;\n\t\tcase &lt;-sl.parentCtx.Done():\n\t\t\tclose(sl.stopped)\n\t\t\treturn\n\t\tcase &lt;-sl.ctx.Done():\n\t\t\tbreak mainLoop\n\t\tdefault:\n\t\t&#125;\n\n\t\t&#x2F;&#x2F; Temporary workaround for a jitter in go timers that causes disk space\n\t\t&#x2F;&#x2F; increase in TSDB.\n\t\t&#x2F;&#x2F; See https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;issues&#x2F;7846\n\t\t&#x2F;&#x2F; Calling Round ensures the time used is the wall clock, as otherwise .Sub\n\t\t&#x2F;&#x2F; and .Add on time.Time behave differently (see time package docs).\n\t\tscrapeTime :&#x3D; time.Now().Round(0)\n\t\tif AlignScrapeTimestamps &amp;&amp; sl.interval &gt; 100*ScrapeTimestampTolerance &#123;\n\t\t\t&#x2F;&#x2F; For some reason, a tick might have been skipped, in which case we\n\t\t\t&#x2F;&#x2F; would call alignedScrapeTime.Add(interval) multiple times.\n\t\t\tfor scrapeTime.Sub(alignedScrapeTime) &gt;&#x3D; sl.interval &#123;\n\t\t\t\talignedScrapeTime &#x3D; alignedScrapeTime.Add(sl.interval)\n\t\t\t&#125;\n\t\t\t&#x2F;&#x2F; Align the scrape time if we are in the tolerance boundaries.\n\t\t\tif scrapeTime.Sub(alignedScrapeTime) &lt;&#x3D; ScrapeTimestampTolerance &#123;\n\t\t\t\tscrapeTime &#x3D; alignedScrapeTime\n\t\t\t&#125;\n\t\t&#125;\n\n\t\tlast &#x3D; sl.scrapeAndReport(last, scrapeTime, errc)\n\n\t\tselect &#123;\n\t\tcase &lt;-sl.parentCtx.Done():\n\t\t\tclose(sl.stopped)\n\t\t\treturn\n\t\tcase &lt;-sl.ctx.Done():\n\t\t\tbreak mainLoop\n\t\tcase &lt;-ticker.C:\n\t\t&#125;\n\t&#125;\n\n\tclose(sl.stopped)\n\n\tif !sl.disabledEndOfRunStalenessMarkers &#123;\n\t\tsl.endOfRunStaleness(last, ticker, sl.interval)\n\t&#125;\n&#125;\n\n&#x2F;&#x2F; 抓取Target数据\nfunc (s *targetScraper) scrape(ctx context.Context, w io.Writer) (string, error) &#123;\n\tif s.req &#x3D;&#x3D; nil &#123;\n\t\treq, err :&#x3D; http.NewRequest(&quot;GET&quot;, s.URL().String(), nil)\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn &quot;&quot;, err\n\t\t&#125;\n\t\treq.Header.Add(&quot;Accept&quot;, acceptHeader)\n\t\treq.Header.Add(&quot;Accept-Encoding&quot;, &quot;gzip&quot;)\n\t\treq.Header.Set(&quot;User-Agent&quot;, UserAgent)\n\t\treq.Header.Set(&quot;X-Prometheus-Scrape-Timeout-Seconds&quot;, strconv.FormatFloat(s.timeout.Seconds(), &#39;f&#39;, -1, 64))\n\n\t\ts.req &#x3D; req\n\t&#125;\n\n    &#x2F;&#x2F; 对Target发起HTTP请求\n\tresp, err :&#x3D; s.client.Do(s.req.WithContext(ctx))\n\tif err !&#x3D; nil &#123;\n\t\treturn &quot;&quot;, err\n\t&#125;\n\tdefer func() &#123;\n\t\tio.Copy(ioutil.Discard, resp.Body)\n\t\tresp.Body.Close()\n\t&#125;()\n\n\tif resp.StatusCode !&#x3D; http.StatusOK &#123;\n\t\treturn &quot;&quot;, errors.Errorf(&quot;server returned HTTP status %s&quot;, resp.Status)\n\t&#125;\n\n\tif s.bodySizeLimit &lt;&#x3D; 0 &#123;\n\t\ts.bodySizeLimit &#x3D; math.MaxInt64\n\t&#125;\n\tif resp.Header.Get(&quot;Content-Encoding&quot;) !&#x3D; &quot;gzip&quot; &#123;\n\t\tn, err :&#x3D; io.Copy(w, io.LimitReader(resp.Body, s.bodySizeLimit))\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn &quot;&quot;, err\n\t\t&#125;\n\t\tif n &gt;&#x3D; s.bodySizeLimit &#123;\n\t\t\ttargetScrapeExceededBodySizeLimit.Inc()\n\t\t\treturn &quot;&quot;, errBodySizeLimit\n\t\t&#125;\n\t\treturn resp.Header.Get(&quot;Content-Type&quot;), nil\n\t&#125;\n\n\tif s.gzipr &#x3D;&#x3D; nil &#123;\n\t\ts.buf &#x3D; bufio.NewReader(resp.Body)\n\t\ts.gzipr, err &#x3D; gzip.NewReader(s.buf)\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn &quot;&quot;, err\n\t\t&#125;\n\t&#125; else &#123;\n\t\ts.buf.Reset(resp.Body)\n\t\tif err &#x3D; s.gzipr.Reset(s.buf); err !&#x3D; nil &#123;\n\t\t\treturn &quot;&quot;, err\n\t\t&#125;\n\t&#125;\n\n\tn, err :&#x3D; io.Copy(w, io.LimitReader(s.gzipr, s.bodySizeLimit))\n\ts.gzipr.Close()\n\tif err !&#x3D; nil &#123;\n\t\treturn &quot;&quot;, err\n\t&#125;\n\tif n &gt;&#x3D; s.bodySizeLimit &#123;\n\t\ttargetScrapeExceededBodySizeLimit.Inc()\n\t\treturn &quot;&quot;, errBodySizeLimit\n\t&#125;\n\treturn resp.Header.Get(&quot;Content-Type&quot;), nil\n&#125;\n\n\nJob#Prometheus抓取数据是通过定义一组Scrape采集任务来进行的，一个Scrape配置多个Job任务，Job是多个相同Target的实例的组合\nTarget#Target是一个具体采集实例，包含多个Metric\n采集方式#监控数据的采集方式分为Pull和Push两种类型，官方推荐Pull类型。\n\nPull：以Prometheus为代表的采集方式，采用Pull类型拉取监控数据，通过配置不同的定时任务以及需要拉取的实例列表，向指定的地址发起HTTP请求拉取监控数据并存储\nPush：以Telegraf和Pushgateway为代表的采集方式，Telegraf的Input插件负责采集监控指标，Output插件负责将监控数据推送到指定的TSDB，而Pushgateway，是监控程序主动推送到Pushgateway，然后由Prometheus去Pushgateway拉取监控数据\n\n服务发现#这是一种非常高效的采集方式，程序将自己的监控地址注册到服务注册中心，如Consul，Eureka等，然后Prometheus配置对应的采集任务，定时去拉取服务注册中心注册的监控地址，生成对应的Target进行数据来取\n数据存储#Prometheus支持本地存储，同时Prometheus还具备将Sample远程写入TSDB和读取远程TSDB数据的能力。  \nRemoteWrite#Prometheus配置RemoteWrite\nremote_write:\n- url: https:&#x2F;&#x2F;xxxxx&#x2F;api&#x2F;monitor&#x2F;v1&#x2F;prom&#x2F;write\n  remote_timeout: 30s\n  queue_config:\n    capacity: 500\n    max_shards: 1000\n    min_shards: 1\n    max_samples_per_send: 100\n    batch_send_deadline: 5s\n    min_backoff: 30ms\n    max_backoff: 100ms\nPrometheus远程写入以队列为单位，每个队列包含多个shard，每个shard包含多个Sample，下面是计算公式：单次写入Sample数量=单次Shard数量*单个Shard容量CapacityPrometheus远程写入过程：Samples ==&gt; Protobuf序列号 ==&gt; Snappy压缩 ==&gt; TSDB\n\nremotewrite\nfunc (t *QueueManager) sendMetadataWithBackoff(ctx context.Context, metadata []prompb.MetricMetadata) error &#123;\n\t&#x2F;&#x2F; Build the WriteRequest with no samples.\n\treq, _, err :&#x3D; buildWriteRequest(nil, metadata, nil)\n\tif err !&#x3D; nil &#123;\n\t\treturn err\n\t&#125;\n\n\tmetadataCount :&#x3D; len(metadata)\n\n\tattemptStore :&#x3D; func(try int) error &#123;\n\t\tspan, ctx :&#x3D; opentracing.StartSpanFromContext(ctx, &quot;Remote Metadata Send Batch&quot;)\n\t\tdefer span.Finish()\n\n\t\tspan.SetTag(&quot;metadata&quot;, metadataCount)\n\t\tspan.SetTag(&quot;try&quot;, try)\n\t\tspan.SetTag(&quot;remote_name&quot;, t.storeClient.Name())\n\t\tspan.SetTag(&quot;remote_url&quot;, t.storeClient.Endpoint())\n\n\t\tbegin :&#x3D; time.Now()\n\t\terr :&#x3D; t.storeClient.Store(ctx, req)\n\t\tt.metrics.sentBatchDuration.Observe(time.Since(begin).Seconds())\n\n\t\tif err !&#x3D; nil &#123;\n\t\t\tspan.LogKV(&quot;error&quot;, err)\n\t\t\text.Error.Set(span, true)\n\t\t\treturn err\n\t\t&#125;\n\n\t\treturn nil\n\t&#125;\n\n\tretry :&#x3D; func() &#123;\n\t\tt.metrics.retriedMetadataTotal.Add(float64(len(metadata)))\n\t&#125;\n\terr &#x3D; sendWriteRequestWithBackoff(ctx, t.cfg, t.logger, attemptStore, retry)\n\tif err !&#x3D; nil &#123;\n\t\treturn err\n\t&#125;\n\tt.metrics.metadataTotal.Add(float64(len(metadata)))\n\tt.metrics.metadataBytesTotal.Add(float64(len(req)))\n\treturn nil\n&#125;\n\nfunc buildWriteRequest(samples []prompb.TimeSeries, metadata []prompb.MetricMetadata, buf []byte) ([]byte, int64, error) &#123;\n\tvar highest int64\n\tfor _, ts :&#x3D; range samples &#123;\n\t\t&#x2F;&#x2F; At the moment we only ever append a TimeSeries with a single sample or exemplar in it.\n\t\tif len(ts.Samples) &gt; 0 &amp;&amp; ts.Samples[0].Timestamp &gt; highest &#123;\n\t\t\thighest &#x3D; ts.Samples[0].Timestamp\n\t\t&#125;\n\t\tif len(ts.Exemplars) &gt; 0 &amp;&amp; ts.Exemplars[0].Timestamp &gt; highest &#123;\n\t\t\thighest &#x3D; ts.Exemplars[0].Timestamp\n\t\t&#125;\n\t&#125;\n\n\treq :&#x3D; &amp;prompb.WriteRequest&#123;\n\t\tTimeseries: samples,\n\t\tMetadata:   metadata,\n\t&#125;\n\n\tdata, err :&#x3D; proto.Marshal(req)\n\tif err !&#x3D; nil &#123;\n\t\treturn nil, highest, err\n\t&#125;\n\n\t&#x2F;&#x2F; snappy uses len() to see if it needs to allocate a new slice. Make the\n\t&#x2F;&#x2F; buffer as long as possible.\n\tif buf !&#x3D; nil &#123;\n\t\tbuf &#x3D; buf[0:cap(buf)]\n\t&#125;\n\tcompressed :&#x3D; snappy.Encode(buf, data)\n\treturn compressed, highest, nil\n&#125;\n\n\nTSDB实现写入接口，以VictoriaMetrics为例\n\ninsertRows\nfunc insertRows(timeseries []prompb.TimeSeries, extraLabels []prompbmarshal.Label) error &#123;\n\tctx :&#x3D; common.GetInsertCtx()\n\tdefer common.PutInsertCtx(ctx)\n\n\trowsLen :&#x3D; 0\n\tfor i :&#x3D; range timeseries &#123;\n\t\trowsLen +&#x3D; len(timeseries[i].Samples)\n\t&#125;\n\tctx.Reset(rowsLen)\n\trowsTotal :&#x3D; 0\n\thasRelabeling :&#x3D; relabel.HasRelabeling()\n\tfor i :&#x3D; range timeseries &#123;\n\t\tts :&#x3D; ×eries[i]\n\t\trowsTotal +&#x3D; len(ts.Samples)\n\t\tctx.Labels &#x3D; ctx.Labels[:0]\n\t\tsrcLabels :&#x3D; ts.Labels\n\t\tfor _, srcLabel :&#x3D; range srcLabels &#123;\n\t\t\tctx.AddLabelBytes(srcLabel.Name, srcLabel.Value)\n\t\t&#125;\n\t\tfor j :&#x3D; range extraLabels &#123;\n\t\t\tlabel :&#x3D; &amp;extraLabels[j]\n\t\t\tctx.AddLabel(label.Name, label.Value)\n\t\t&#125;\n\t\tif hasRelabeling &#123;\n\t\t\tctx.ApplyRelabeling()\n\t\t&#125;\n\t\tif len(ctx.Labels) &#x3D;&#x3D; 0 &#123;\n\t\t\t&#x2F;&#x2F; Skip metric without labels.\n\t\t\tcontinue\n\t\t&#125;\n\t\tctx.SortLabelsIfNeeded()\n\t\tvar metricNameRaw []byte\n\t\tvar err error\n\t\tsamples :&#x3D; ts.Samples\n\t\tfor i :&#x3D; range samples &#123;\n\t\t\tr :&#x3D; &amp;samples[i]\n\t\t\tmetricNameRaw, err &#x3D; ctx.WriteDataPointExt(metricNameRaw, ctx.Labels, r.Timestamp, r.Value)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn err\n\t\t\t&#125;\n\t\t&#125;\n\t&#125;\n\trowsInserted.Add(rowsTotal)\n\trowsPerInsert.Update(float64(rowsTotal))\n\treturn ctx.FlushBufs()\n&#125;\n\nfunc (ctx *InsertCtx) WriteDataPointExt(metricNameRaw []byte, labels []prompb.Label, timestamp int64, value float64) ([]byte, error) &#123;\n\tif len(metricNameRaw) &#x3D;&#x3D; 0 &#123;\n\t\tmetricNameRaw &#x3D; ctx.marshalMetricNameRaw(nil, labels)\n\t&#125;\n\terr :&#x3D; ctx.addRow(metricNameRaw, timestamp, value)\n\treturn metricNameRaw, err\n&#125;\n\nfunc (ctx *InsertCtx) addRow(metricNameRaw []byte, timestamp int64, value float64) error &#123;\n\tmrs :&#x3D; ctx.mrs\n\tif cap(mrs) &gt; len(mrs) &#123;\n\t\tmrs &#x3D; mrs[:len(mrs)+1]\n\t&#125; else &#123;\n\t\tmrs &#x3D; append(mrs, storage.MetricRow&#123;&#125;)\n\t&#125;\n\tmr :&#x3D; &amp;mrs[len(mrs)-1]\n\tctx.mrs &#x3D; mrs\n\tmr.MetricNameRaw &#x3D; metricNameRaw\n\tmr.Timestamp &#x3D; timestamp\n\tmr.Value &#x3D; value\n\tif len(ctx.metricNamesBuf) &gt; 16*1024*1024 &#123;\n\t\tif err :&#x3D; ctx.FlushBufs(); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t&#125;\n\t\n    return nil\n&#125;\n\n\nRemoteRead#远程读取流程和写入相反\n\nremoterRead\nfunc (h *readHandler) remoteReadSamples(\n\tctx context.Context,\n\tw http.ResponseWriter,\n\treq *prompb.ReadRequest,\n\texternalLabels map[string]string,\n\tsortedExternalLabels []prompb.Label,\n) &#123;\n\tw.Header().Set(&quot;Content-Type&quot;, &quot;application&#x2F;x-protobuf&quot;)\n\tw.Header().Set(&quot;Content-Encoding&quot;, &quot;snappy&quot;)\n\n\tresp :&#x3D; prompb.ReadResponse&#123;\n\t\tResults: make([]*prompb.QueryResult, len(req.Queries)),\n\t&#125;\n\tfor i, query :&#x3D; range req.Queries &#123;\n\t\tif err :&#x3D; func() error &#123;\n\t\t\tfilteredMatchers, err :&#x3D; filterExtLabelsFromMatchers(query.Matchers, externalLabels)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn err\n\t\t\t&#125;\n\n\t\t\tquerier, err :&#x3D; h.queryable.Querier(ctx, query.StartTimestampMs, query.EndTimestampMs)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn err\n\t\t\t&#125;\n\t\t\tdefer func() &#123;\n\t\t\t\tif err :&#x3D; querier.Close(); err !&#x3D; nil &#123;\n\t\t\t\t\tlevel.Warn(h.logger).Log(&quot;msg&quot;, &quot;Error on querier close&quot;, &quot;err&quot;, err.Error())\n\t\t\t\t&#125;\n\t\t\t&#125;()\n\n\t\t\tvar hints *storage.SelectHints\n\t\t\tif query.Hints !&#x3D; nil &#123;\n\t\t\t\thints &#x3D; &amp;storage.SelectHints&#123;\n\t\t\t\t\tStart:    query.Hints.StartMs,\n\t\t\t\t\tEnd:      query.Hints.EndMs,\n\t\t\t\t\tStep:     query.Hints.StepMs,\n\t\t\t\t\tFunc:     query.Hints.Func,\n\t\t\t\t\tGrouping: query.Hints.Grouping,\n\t\t\t\t\tRange:    query.Hints.RangeMs,\n\t\t\t\t\tBy:       query.Hints.By,\n\t\t\t\t&#125;\n\t\t\t&#125;\n\n\t\t\tvar ws storage.Warnings\n\t\t\tresp.Results[i], ws, err &#x3D; ToQueryResult(querier.Select(false, hints, filteredMatchers...), h.remoteReadSampleLimit)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn err\n\t\t\t&#125;\n\t\t\tfor _, w :&#x3D; range ws &#123;\n\t\t\t\tlevel.Warn(h.logger).Log(&quot;msg&quot;, &quot;Warnings on remote read query&quot;, &quot;err&quot;, w.Error())\n\t\t\t&#125;\n\t\t\tfor _, ts :&#x3D; range resp.Results[i].Timeseries &#123;\n\t\t\t\tts.Labels &#x3D; MergeLabels(ts.Labels, sortedExternalLabels)\n\t\t\t&#125;\n\t\t\treturn nil\n\t\t&#125;(); err !&#x3D; nil &#123;\n\t\t\tif httpErr, ok :&#x3D; err.(HTTPError); ok &#123;\n\t\t\t\thttp.Error(w, httpErr.Error(), httpErr.Status())\n\t\t\t\treturn\n\t\t\t&#125;\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t&#125;\n\t&#125;\n\n\tif err :&#x3D; EncodeReadResponse(&amp;resp, w); err !&#x3D; nil &#123;\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t&#125;\n&#125;\n\n\n数据存储#Prometheus在进行数据存储的时候并不是实时写入，而是在达到设置的最大缓存后才会落盘，使得单个时间序列的一段Chunk在存储上相邻，从而允许较快的横向读取时序数据，并对单个Chunk进行压缩，节省存储和计算成本。在V2架构中，还更改为俩小时落一个Block，存储着两小时内所有Chunk和索引\n\nFlushWAL\nconst (\n\t&#x2F;&#x2F; Default duration of a block in milliseconds.\n\tDefaultBlockDuration &#x3D; int64(2 * time.Hour &#x2F; time.Millisecond)\n\n\t......\n)\n\n&#x2F;&#x2F; FlushWAL creates a new block containing all data that&#39;s currently in the memory buffer&#x2F;WAL.\n&#x2F;&#x2F; Samples that are in existing blocks will not be written to the new block.\n&#x2F;&#x2F; Note that if the read only database is running concurrently with a\n&#x2F;&#x2F; writable database then writing the WAL to the database directory can race.\nfunc (db *DBReadOnly) FlushWAL(dir string) (returnErr error) &#123;\n\tblockReaders, err :&#x3D; db.Blocks()\n\tif err !&#x3D; nil &#123;\n\t\treturn errors.Wrap(err, &quot;read blocks&quot;)\n\t&#125;\n\tmaxBlockTime :&#x3D; int64(math.MinInt64)\n\tif len(blockReaders) &gt; 0 &#123;\n\t\tmaxBlockTime &#x3D; blockReaders[len(blockReaders)-1].Meta().MaxTime\n\t&#125;\n\tw, err :&#x3D; wal.Open(db.logger, filepath.Join(db.dir, &quot;wal&quot;))\n\tif err !&#x3D; nil &#123;\n\t\treturn err\n\t&#125;\n\topts :&#x3D; DefaultHeadOptions()\n\topts.ChunkDirRoot &#x3D; db.dir\n\thead, err :&#x3D; NewHead(nil, db.logger, w, opts, NewHeadStats())\n\tif err !&#x3D; nil &#123;\n\t\treturn err\n\t&#125;\n\tdefer func() &#123;\n\t\treturnErr &#x3D; tsdb_errors.NewMulti(\n\t\t\treturnErr,\n\t\t\terrors.Wrap(head.Close(), &quot;closing Head&quot;),\n\t\t).Err()\n\t&#125;()\n\t&#x2F;&#x2F; Set the min valid time for the ingested wal samples\n\t&#x2F;&#x2F; to be no lower than the maxt of the last block.\n\tif err :&#x3D; head.Init(maxBlockTime); err !&#x3D; nil &#123;\n\t\treturn errors.Wrap(err, &quot;read WAL&quot;)\n\t&#125;\n\tmint :&#x3D; head.MinTime()\n\tmaxt :&#x3D; head.MaxTime()\n\trh :&#x3D; NewRangeHead(head, mint, maxt)\n\tcompactor, err :&#x3D; NewLeveledCompactor(\n\t\tcontext.Background(),\n\t\tnil,\n\t\tdb.logger,\n\t\tExponentialBlockRanges(DefaultOptions().MinBlockDuration, 3, 5),\n\t\tchunkenc.NewPool(),\n\t\tnil,\n\t)\n\tif err !&#x3D; nil &#123;\n\t\treturn errors.Wrap(err, &quot;create leveled compactor&quot;)\n\t&#125;\n\t&#x2F;&#x2F; Add +1 millisecond to block maxt because block intervals are half-open: [b.MinTime, b.MaxTime).\n\t&#x2F;&#x2F; Because of this block intervals are always +1 than the total samples it includes.\n\t_, err &#x3D; compactor.Write(dir, rh, mint, maxt+1, nil)\n\treturn errors.Wrap(err, &quot;writing WAL&quot;)\n&#125;\n\n\nChunk写入\n\nWriteChunks\n&#x2F;&#x2F; WriteChunks writes as many chunks as possible to the current segment,\n&#x2F;&#x2F; cuts a new segment when the current segment is full and\n&#x2F;&#x2F; writes the rest of the chunks in the new segment.\nfunc (w *Writer) WriteChunks(chks ...Meta) error &#123;\n\tvar (\n\t\tbatchSize  &#x3D; int64(0)\n\t\tbatchStart &#x3D; 0\n\t\tbatches    &#x3D; make([][]Meta, 1)\n\t\tbatchID    &#x3D; 0\n\t\tfirstBatch &#x3D; true\n\t)\n\n\tfor i, chk :&#x3D; range chks &#123;\n\t\t&#x2F;&#x2F; Each chunk contains: data length + encoding + the data itself + crc32\n\t\tchkSize :&#x3D; int64(MaxChunkLengthFieldSize) &#x2F;&#x2F; The data length is a variable length field so use the maximum possible value.\n\t\tchkSize +&#x3D; ChunkEncodingSize              &#x2F;&#x2F; The chunk encoding.\n\t\tchkSize +&#x3D; int64(len(chk.Chunk.Bytes()))  &#x2F;&#x2F; The data itself.\n\t\tchkSize +&#x3D; crc32.Size                     &#x2F;&#x2F; The 4 bytes of crc32.\n\t\tbatchSize +&#x3D; chkSize\n\n\t\t&#x2F;&#x2F; Cut a new batch when it is not the first chunk(to avoid empty segments) and\n\t\t&#x2F;&#x2F; the batch is too large to fit in the current segment.\n\t\tcutNewBatch :&#x3D; (i !&#x3D; 0) &amp;&amp; (batchSize+SegmentHeaderSize &gt; w.segmentSize)\n\n\t\t&#x2F;&#x2F; When the segment already has some data than\n\t\t&#x2F;&#x2F; the first batch size calculation should account for that.\n\t\tif firstBatch &amp;&amp; w.n &gt; SegmentHeaderSize &#123;\n\t\t\tcutNewBatch &#x3D; batchSize+w.n &gt; w.segmentSize\n\t\t\tif cutNewBatch &#123;\n\t\t\t\tfirstBatch &#x3D; false\n\t\t\t&#125;\n\t\t&#125;\n\n\t\tif cutNewBatch &#123;\n\t\t\tbatchStart &#x3D; i\n\t\t\tbatches &#x3D; append(batches, []Meta&#123;&#125;)\n\t\t\tbatchID++\n\t\t\tbatchSize &#x3D; chkSize\n\t\t&#125;\n\t\tbatches[batchID] &#x3D; chks[batchStart : i+1]\n\t&#125;\n\n\t&#x2F;&#x2F; Create a new segment when one doesn&#39;t already exist.\n\tif w.n &#x3D;&#x3D; 0 &#123;\n\t\tif err :&#x3D; w.cut(); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t&#125;\n\n\tfor i, chks :&#x3D; range batches &#123;\n\t\tif err :&#x3D; w.writeChunks(chks); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t\t&#x2F;&#x2F; Cut a new segment only when there are more chunks to write.\n\t\t&#x2F;&#x2F; Avoid creating a new empty segment at the end of the write.\n\t\tif i &lt; len(batches)-1 &#123;\n\t\t\tif err :&#x3D; w.cut(); err !&#x3D; nil &#123;\n\t\t\t\treturn err\n\t\t\t&#125;\n\t\t&#125;\n\t&#125;\n\treturn nil\n&#125;\n\n&#x2F;&#x2F; writeChunks writes the chunks into the current segment irrespective\n&#x2F;&#x2F; of the configured segment size limit. A segment should have been already\n&#x2F;&#x2F; started before calling this.\nfunc (w *Writer) writeChunks(chks []Meta) error &#123;\n\tif len(chks) &#x3D;&#x3D; 0 &#123;\n\t\treturn nil\n\t&#125;\n\n\tvar seq &#x3D; uint64(w.seq()) &lt;&lt; 32\n\tfor i :&#x3D; range chks &#123;\n\t\tchk :&#x3D; &amp;chks[i]\n\n\t\t&#x2F;&#x2F; The reference is set to the segment index and the offset where\n\t\t&#x2F;&#x2F; the data starts for this chunk.\n\t\t&#x2F;&#x2F;\n\t\t&#x2F;&#x2F; The upper 4 bytes are for the segment index and\n\t\t&#x2F;&#x2F; the lower 4 bytes are for the segment offset where to start reading this chunk.\n\t\tchk.Ref &#x3D; seq | uint64(w.n)\n\n\t\tn :&#x3D; binary.PutUvarint(w.buf[:], uint64(len(chk.Chunk.Bytes())))\n\n\t\tif err :&#x3D; w.write(w.buf[:n]); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t\tw.buf[0] &#x3D; byte(chk.Chunk.Encoding())\n\t\tif err :&#x3D; w.write(w.buf[:1]); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t\tif err :&#x3D; w.write(chk.Chunk.Bytes()); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\n\t\tw.crc32.Reset()\n\t\tif err :&#x3D; chk.writeHash(w.crc32, w.buf[:]); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t\tif err :&#x3D; w.write(w.crc32.Sum(w.buf[:0])); err !&#x3D; nil &#123;\n\t\t\treturn err\n\t\t&#125;\n\t&#125;\n\treturn nil\n&#125;\n\n\n数据压缩\n\nCompact\n&#x2F;&#x2F; Compact creates a new block in the compactor&#39;s directory from the blocks in the\n&#x2F;&#x2F; provided directories.\nfunc (c *LeveledCompactor) Compact(dest string, dirs []string, open []*Block) (uid ulid.ULID, err error) &#123;\n\tvar (\n\t\tblocks []BlockReader\n\t\tbs     []*Block\n\t\tmetas  []*BlockMeta\n\t\tuids   []string\n\t)\n\tstart :&#x3D; time.Now()\n\n\tfor _, d :&#x3D; range dirs &#123;\n\t\tmeta, _, err :&#x3D; readMetaFile(d)\n\t\tif err !&#x3D; nil &#123;\n\t\t\treturn uid, err\n\t\t&#125;\n\n\t\tvar b *Block\n\n\t\t&#x2F;&#x2F; Use already open blocks if we can, to avoid\n\t\t&#x2F;&#x2F; having the index data in memory twice.\n\t\tfor _, o :&#x3D; range open &#123;\n\t\t\tif meta.ULID &#x3D;&#x3D; o.Meta().ULID &#123;\n\t\t\t\tb &#x3D; o\n\t\t\t\tbreak\n\t\t\t&#125;\n\t\t&#125;\n\n\t\tif b &#x3D;&#x3D; nil &#123;\n\t\t\tvar err error\n\t\t\tb, err &#x3D; OpenBlock(c.logger, d, c.chunkPool)\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\treturn uid, err\n\t\t\t&#125;\n\t\t\tdefer b.Close()\n\t\t&#125;\n\n\t\tmetas &#x3D; append(metas, meta)\n\t\tblocks &#x3D; append(blocks, b)\n\t\tbs &#x3D; append(bs, b)\n\t\tuids &#x3D; append(uids, meta.ULID.String())\n\t&#125;\n\n\tuid &#x3D; ulid.MustNew(ulid.Now(), rand.Reader)\n\n\tmeta :&#x3D; CompactBlockMetas(uid, metas...)\n\terr &#x3D; c.write(dest, meta, blocks...)\n\tif err &#x3D;&#x3D; nil &#123;\n\t\tif meta.Stats.NumSamples &#x3D;&#x3D; 0 &#123;\n\t\t\tfor _, b :&#x3D; range bs &#123;\n\t\t\t\tb.meta.Compaction.Deletable &#x3D; true\n\t\t\t\tn, err :&#x3D; writeMetaFile(c.logger, b.dir, &amp;b.meta)\n\t\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\t\tlevel.Error(c.logger).Log(\n\t\t\t\t\t\t&quot;msg&quot;, &quot;Failed to write &#39;Deletable&#39; to meta file after compaction&quot;,\n\t\t\t\t\t\t&quot;ulid&quot;, b.meta.ULID,\n\t\t\t\t\t)\n\t\t\t\t&#125;\n\t\t\t\tb.numBytesMeta &#x3D; n\n\t\t\t&#125;\n\t\t\tuid &#x3D; ulid.ULID&#123;&#125;\n\t\t\tlevel.Info(c.logger).Log(\n\t\t\t\t&quot;msg&quot;, &quot;compact blocks resulted in empty block&quot;,\n\t\t\t\t&quot;count&quot;, len(blocks),\n\t\t\t\t&quot;sources&quot;, fmt.Sprintf(&quot;%v&quot;, uids),\n\t\t\t\t&quot;duration&quot;, time.Since(start),\n\t\t\t)\n\t\t&#125; else &#123;\n\t\t\tlevel.Info(c.logger).Log(\n\t\t\t\t&quot;msg&quot;, &quot;compact blocks&quot;,\n\t\t\t\t&quot;count&quot;, len(blocks),\n\t\t\t\t&quot;mint&quot;, meta.MinTime,\n\t\t\t\t&quot;maxt&quot;, meta.MaxTime,\n\t\t\t\t&quot;ulid&quot;, meta.ULID,\n\t\t\t\t&quot;sources&quot;, fmt.Sprintf(&quot;%v&quot;, uids),\n\t\t\t\t&quot;duration&quot;, time.Since(start),\n\t\t\t)\n\t\t&#125;\n\t\treturn uid, nil\n\t&#125;\n\n\terrs :&#x3D; tsdb_errors.NewMulti(err)\n\tif err !&#x3D; context.Canceled &#123;\n\t\tfor _, b :&#x3D; range bs &#123;\n\t\t\tif err :&#x3D; b.setCompactionFailed(); err !&#x3D; nil &#123;\n\t\t\t\terrs.Add(errors.Wrapf(err, &quot;setting compaction failed for block: %s&quot;, b.Dir()))\n\t\t\t&#125;\n\t\t&#125;\n\t&#125;\n\n\treturn uid, errs.Err()\n&#125;\n\n\n监控数据目录结构如下： \n.\n├── 01FQ6T1CN7SJK7AAXEVAD7W51J\n│   ├── chunks\n│   │   └── 000001\n│   ├── index\n│   ├── meta.json\n│   └── tombstones\n├── chunks_head\n│   ├── 002333\n│   └── 002334\n├── lock\n├── queries.active\n└── wal\n    ├── 00009654\n    ├── 00009655\n    ├── 00009656\n    ├── 00009657\n    └── checkpoint.00009653\n        └── 00000000\n\nmeta.json \n&#123;\n\t&quot;ulid&quot;: &quot;01FQ6T1CN7SJK7AAXEVAD7W51J&quot;,\n\t&quot;minTime&quot;: 1639821600349,\n\t&quot;maxTime&quot;: 1639828800000,\n\t&quot;stats&quot;: &#123;\n\t\t&quot;numSamples&quot;: 17738865,\n\t\t&quot;numSeries&quot;: 147825,\n\t\t&quot;numChunks&quot;: 147825\n\t&#125;,\n\t&quot;compaction&quot;: &#123;\n\t\t&quot;level&quot;: 1,\n\t\t&quot;sources&quot;: [\n\t\t\t&quot;01FQ6T1CN7SJK7AAXEVAD7W51J&quot;\n\t\t]\n\t&#125;,\n\t&quot;version&quot;: 1\n&#125;\n\n存储结构#\n数据分析#我们可以使用promtool来分析监控数据，执行以下命令：promtool tsdb analyze /data/ 01FXSQS1WSJJ8R909EPN9DZ0FE\n报警#Prometheus根据配置的查询规则，每间隔一定时间进行评估，由参数evaluation_interval决定，当第一次触发报警，会进入“PENDING”状态，并记录当前active的时间，下一个周期如果报警条件依然成立，这时会判断rule中报警持续时间”for“，如果active时间超过了持续时间，则alert的状态变为“FIRING”；同时调用Alertmanager接口，发送相关报警数据。Evaluation Rule -&gt; Active -&gt; PENDING -&gt; Large than for -&gt; FIRING -&gt; Send to Alertmanager\n\nAlert\n&#x2F;&#x2F; 检查规则\nfunc (g *Group) Eval(ctx context.Context, ts time.Time) &#123;\n\tvar samplesTotal float64\n    &#x2F;&#x2F; 遍历规则列表\n\tfor i, rule :&#x3D; range g.rules &#123;\n\t\tselect &#123;\n\t\tcase &lt;-g.done:\n\t\t\treturn\n\t\tdefault:\n\t\t&#125;\n        &#x2F;&#x2F; 检查规则\n\t\tfunc(i int, rule Rule) &#123;\n\t\t\tctx, sp :&#x3D; otel.Tracer(&quot;&quot;).Start(ctx, &quot;rule&quot;)\n\t\t\tsp.SetAttributes(attribute.String(&quot;name&quot;, rule.Name()))\n\t\t\tdefer func(t time.Time) &#123;\n\t\t\t\tsp.End()\n\n\t\t\t\tsince :&#x3D; time.Since(t)\n\t\t\t\tg.metrics.EvalDuration.Observe(since.Seconds())\n\t\t\t\trule.SetEvaluationDuration(since)\n\t\t\t\trule.SetEvaluationTimestamp(t)\n\t\t\t&#125;(time.Now())\n\n\t\t\tg.metrics.EvalTotal.WithLabelValues(GroupKey(g.File(), g.Name())).Inc()\n\n\t\t\tvector, err :&#x3D; rule.Eval(ctx, ts, g.opts.QueryFunc, g.opts.ExternalURL, g.Limit())\n\t\t\tif err !&#x3D; nil &#123;\n\t\t\t\trule.SetHealth(HealthBad)\n\t\t\t\trule.SetLastError(err)\n\t\t\t\tg.metrics.EvalFailures.WithLabelValues(GroupKey(g.File(), g.Name())).Inc()\n\n\t\t\t\t&#x2F;&#x2F; Canceled queries are intentional termination of queries. This normally\n\t\t\t\t&#x2F;&#x2F; happens on shutdown and thus we skip logging of any errors here.\n\t\t\t\tif _, ok :&#x3D; err.(promql.ErrQueryCanceled); !ok &#123;\n\t\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Evaluating rule failed&quot;, &quot;rule&quot;, rule, &quot;err&quot;, err)\n\t\t\t\t&#125;\n\t\t\t\treturn\n\t\t\t&#125;\n            &#x2F;&#x2F; 记录规则状态\n\t\t\trule.SetHealth(HealthGood)\n\t\t\trule.SetLastError(nil)\n\t\t\tsamplesTotal +&#x3D; float64(len(vector))\n\n\t\t\tif ar, ok :&#x3D; rule.(*AlertingRule); ok &#123;\n                &#x2F;&#x2F; 达成报警条件，发送报警\n\t\t\t\tar.sendAlerts(ctx, ts, g.opts.ResendDelay, g.interval, g.opts.NotifyFunc)\n\t\t\t&#125;\n\t\t\tvar (\n\t\t\t\tnumOutOfOrder &#x3D; 0\n\t\t\t\tnumDuplicates &#x3D; 0\n\t\t\t)\n\n\t\t\tapp :&#x3D; g.opts.Appendable.Appender(ctx)\n\t\t\tseriesReturned :&#x3D; make(map[string]labels.Labels, len(g.seriesInPreviousEval[i]))\n\t\t\tdefer func() &#123;\n\t\t\t\tif err :&#x3D; app.Commit(); err !&#x3D; nil &#123;\n\t\t\t\t\trule.SetHealth(HealthBad)\n\t\t\t\t\trule.SetLastError(err)\n\t\t\t\t\tg.metrics.EvalFailures.WithLabelValues(GroupKey(g.File(), g.Name())).Inc()\n\n\t\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Rule sample appending failed&quot;, &quot;err&quot;, err)\n\t\t\t\t\treturn\n\t\t\t\t&#125;\n\t\t\t\tg.seriesInPreviousEval[i] &#x3D; seriesReturned\n\t\t\t&#125;()\n\n\t\t\tfor _, s :&#x3D; range vector &#123;\n\t\t\t\tif _, err :&#x3D; app.Append(0, s.Metric, s.T, s.V); err !&#x3D; nil &#123;\n\t\t\t\t\trule.SetHealth(HealthBad)\n\t\t\t\t\trule.SetLastError(err)\n\n\t\t\t\t\tswitch errors.Cause(err) &#123;\n\t\t\t\t\tcase storage.ErrOutOfOrderSample:\n\t\t\t\t\t\tnumOutOfOrder++\n\t\t\t\t\t\tlevel.Debug(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Rule evaluation result discarded&quot;, &quot;err&quot;, err, &quot;sample&quot;, s)\n\t\t\t\t\tcase storage.ErrDuplicateSampleForTimestamp:\n\t\t\t\t\t\tnumDuplicates++\n\t\t\t\t\t\tlevel.Debug(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Rule evaluation result discarded&quot;, &quot;err&quot;, err, &quot;sample&quot;, s)\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Rule evaluation result discarded&quot;, &quot;err&quot;, err, &quot;sample&quot;, s)\n\t\t\t\t\t&#125;\n\t\t\t\t&#125; else &#123;\n\t\t\t\t\tbuf :&#x3D; [1024]byte&#123;&#125;\n\t\t\t\t\tseriesReturned[string(s.Metric.Bytes(buf[:]))] &#x3D; s.Metric\n\t\t\t\t&#125;\n\t\t\t&#125;\n\t\t\tif numOutOfOrder &gt; 0 &#123;\n\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Error on ingesting out-of-order result from rule evaluation&quot;, &quot;numDropped&quot;, numOutOfOrder)\n\t\t\t&#125;\n\t\t\tif numDuplicates &gt; 0 &#123;\n\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Error on ingesting results from rule evaluation with different value but same timestamp&quot;, &quot;numDropped&quot;, numDuplicates)\n\t\t\t&#125;\n\n\t\t\tfor metric, lset :&#x3D; range g.seriesInPreviousEval[i] &#123;\n\t\t\t\tif _, ok :&#x3D; seriesReturned[metric]; !ok &#123;\n\t\t\t\t\t&#x2F;&#x2F; Series no longer exposed, mark it stale.\n\t\t\t\t\t_, err &#x3D; app.Append(0, lset, timestamp.FromTime(ts), math.Float64frombits(value.StaleNaN))\n\t\t\t\t\tswitch errors.Cause(err) &#123;\n\t\t\t\t\tcase nil:\n\t\t\t\t\tcase storage.ErrOutOfOrderSample, storage.ErrDuplicateSampleForTimestamp:\n\t\t\t\t\t\t&#x2F;&#x2F; Do not count these in logging, as this is expected if series\n\t\t\t\t\t\t&#x2F;&#x2F; is exposed from a different rule.\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tlevel.Warn(g.logger).Log(&quot;name&quot;, rule.Name(), &quot;index&quot;, i, &quot;msg&quot;, &quot;Adding stale sample failed&quot;, &quot;sample&quot;, lset.String(), &quot;err&quot;, err)\n\t\t\t\t\t&#125;\n\t\t\t\t&#125;\n\t\t\t&#125;\n\t\t&#125;(i, rule)\n\t&#125;\n\tif g.metrics !&#x3D; nil &#123;\n\t\tg.metrics.GroupSamples.WithLabelValues(GroupKey(g.File(), g.Name())).Set(samplesTotal)\n\t&#125;\n\tg.cleanupStaleSeries(ctx, ts)\n&#125;\n\n\n查询#倒排索引#Prometheus采用的是倒排索引，根据Label查找对应的时间序列，从而找到对应的Chunk。Prometheus索引代码：\n\nIndex\n&#x2F;&#x2F; Writer implements the IndexWriter interface for the standard\n&#x2F;&#x2F; serialization format.\ntype Writer struct &#123;\n\tctx context.Context\n\n\t&#x2F;&#x2F; For the main index file.\n\tf *FileWriter\n\n\t&#x2F;&#x2F; Temporary file for postings.\n\tfP *FileWriter\n\t&#x2F;&#x2F; Temporary file for posting offsets table.\n\tfPO   *FileWriter\n\tcntPO uint64\n\n\ttoc           TOC\n\tstage         indexWriterStage\n\tpostingsStart uint64 &#x2F;&#x2F; Due to padding, can differ from TOC entry.\n\n\t&#x2F;&#x2F; Reusable memory.\n\tbuf1 encoding.Encbuf\n\tbuf2 encoding.Encbuf\n\n\tnumSymbols  int\n\tsymbols     *Symbols\n\tsymbolFile  *fileutil.MmapFile\n\tlastSymbol  string\n\tsymbolCache map[string]symbolCacheEntry\n\n\tlabelIndexes []labelIndexHashEntry &#x2F;&#x2F; Label index offsets.\n\tlabelNames   map[string]uint64     &#x2F;&#x2F; Label names, and their usage.\n\n\t&#x2F;&#x2F; Hold last series to validate that clients insert new series in order.\n\tlastSeries labels.Labels\n\tlastRef    uint64\n\n\tcrc32 hash.Hash\n\n\tVersion int\n&#125;\n\ntype Reader struct &#123;\n\tb   ByteSlice\n\ttoc *TOC\n\n\t&#x2F;&#x2F; Close that releases the underlying resources of the byte slice.\n\tc io.Closer\n\n\t&#x2F;&#x2F; Map of LabelName to a list of some LabelValues&#39;s position in the offset table.\n\t&#x2F;&#x2F; The first and last values for each name are always present.\n\tpostings map[string][]postingOffset\n\t&#x2F;&#x2F; For the v1 format, labelname -&gt; labelvalue -&gt; offset.\n\tpostingsV1 map[string]map[string]uint64\n\n\tsymbols     *Symbols\n\tnameSymbols map[uint32]string &#x2F;&#x2F; Cache of the label name symbol lookups,\n\t&#x2F;&#x2F; as there are not many and they are half of all lookups.\n\n\tdec *Decoder\n\n\tversion int\n&#125;\n\n&#x2F;&#x2F; TOC represents index Table Of Content that states where each section of index starts.\ntype TOC struct &#123;\n\tSymbols           uint64\n\tSeries            uint64\n\tLabelIndices      uint64\n\tLabelIndicesTable uint64\n\tPostings          uint64\n\tPostingsTable     uint64\n&#125;\n\ntype postingOffset struct &#123;\n\tvalue string\n\toff   int\n&#125;\n\ntype Symbols struct &#123;\n\tbs      ByteSlice\n\tversion int\n\toff     int\n\n\toffsets []int\n\tseen    int\n&#125;\n\n\n一个索引包含了以下内容：\n\nSymbol：数据字典，每个Label的Name和Value都对应一个ID\nTOC：记录了时间序列的ID和Label的Name和Value的对应关系\nPosting：每个label值对应series列表的偏移量倒排索引的工作流程：\n\n\n","slug":"Prometheus数据流","date":"2022-04-08T08:46:07.000Z","categories_index":"Monitor","tags_index":"Monitor,Prometheus,Metrics","author_index":"Asura"},{"id":"96badec9d2a8ff4ed297d972e3ac1aab","title":"容器技术实现原理","content":"\n\n\n\n\n\n\n\n\n开始之前为了方便的理解容器，先举个简单的例子：假如把一套房子比作操作系统，这套房子的中控就是内核，每间卧室可以理解为是虚拟化出来的虚拟机，他们可以拥有各自独立的中控，假设这套房子中控控制着台冰箱，冰箱里有多个隔层，那么每个隔层就是一个个独立运行的容器。\n\n\n\n\n\nLinux容器\n进程\nNamespace\nCgroup\n网络\n文件系统\n\n\n镜像\n总结\n\n\n\n\n\nLinux容器#进程#在了解容器之前，我们有必要对Linux进程有一定的了解，因为容器本质上就是一个特殊的进程。关于Linux进程我们先看看下面这张图\n\nLinux操作系统启动后切进用户态随之开启进程调度，首先会往GDT（全局描述符表）里写入两个结构TSS（任务状态段）和LDT（局部描述符表）。  \n\n\nTSS用来保存和恢复进程的上下文，即各个寄存器的信息；\nLDT与GDT相对应，内核态的代码用GDT里的数据段和代码段，而用户态的进程代码用每个用户自己的LDT里的数据段和代码段。\n\n\n接下来会初始化一个长度为64的task_struct的数组来存放所有进程信息，并附上初始值init_task.init，即0号进程的信息。\n最后设置了时钟中断0x20和系统调用0x80，一个作为进程调度的起点，一个作为用户程序调用操作系统功能的桥梁。\n\n\ntask_struct\nstruct task_struct {/* these are hardcoded - don’t touch /    long state; / -1 unrunnable, 0 runnable, &gt;0 stopped /    long counter;    long priority;    long signal;    struct sigaction sigaction[32];    long blocked; / bitmap of masked signals /  / various fields /    int exit_code;    unsigned long start_code,end_code,end_data,brk,start_stack;    long pid,father,pgrp,session,leader;    unsigned short uid,euid,suid;    unsigned short gid,egid,sgid;    long alarm;    long utime,stime,cutime,cstime,start_time;    unsigned short used_math;  / file system info /    int tty;  / -1 if no tty, so it must be signed /    unsigned short umask;    struct m_inode * pwd;    struct m_inode * root;    struct m_inode * executable;    unsigned long close_on_exec;    struct file * filp[NR_OPEN];  / ldt for this task 0 - zero 1 - cs 2 - ds&amp;ss /    struct desc_struct ldt[3];  / tss for this task */    struct tss_struct tss;};\n\n关于Linux进程，我们还有必要了解一下0号进程和1号进程，尤其是1号进程和容器关系密切。  \n\n0号进程：0号进程是系统初始化时创建的，即所有进程的父进程，其数据大部分是由预先定义好的INIT_TASK，INIT_MM等宏初始化；  \n1号进程：0号进程通过调用kernel_thread创建一个内核线程去执行init函数，init函数在完成内核初始化后调用execve系统调用，以装入用户空间的可执行程序/sbin/init，这就诞生了真正意义上的init进程，即1号进程。1号进程主要负责执行内核初始化工作及系统配置，并创建若干用于高速缓存和虚拟主存管理的内核线程。  \n\n1号进程创建后就可以调用execve（）运行init程序，演变成用户态1号进程，根据/etc/initab的配置，接着创建编号1号，2号。。。注册进程getty，从而形成Linux的树形结构进程列表。  \n至此，我们了解了Linux进程的创建过程。那么容器和进程又有什么关系呢？前面我们已经说过了，容器本质上是一个特殊的进程，他的特殊之处又在哪呢？容器进程的特殊性表现在容器进程永远认为自己是1号进程，然而它在系统中的进程号可能是99号，而让容器产生这种特性的技术是Linux的PID Namespace机制。  \nNamespace#在我们开始了解Linux Namespace机制之前，我们先来看看上面说的容器进程伪装隔离的实现原理：Linux在创建用户态进程是会调用clone()方法并返回一个int类型的PIDint pid = clone(main_function, stack_size, SIGCHLD, NULL);如果我们传入一个可选参数CLONE_NEWPID，那么新创建的进程将会是一个全新的进程空间，在这个进程空间里，它的进程号就是1，这就是一个容器。int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL);上面就是PID Namespace的实现。  \nLinux Namespace简单来说就是一种资源隔离的机制，主要是三个系统调用clone()，unshare()，setns()。上面提到的进程隔离PID namespace只是众多Namespace中的一种。Linux中还提供了如Mount、Network、User、UTS、IPC等多种资源隔离机制：\n\n\n\nNamespace\n用途\n\n\n\nPID Namespace\n隔离进程的ID\n\n\nMount Namespace\n隔离文件系统挂载点\n\n\nNetwork Namespace\n隔离网络资源\n\n\nUser Namespace\n隔离用户和用户组ID\n\n\nUTS Namespace\n隔离主机名和域名信息\n\n\nIPC Namespace\n隔离进程间通信\n\n\n可以在/proc/$pid/ns目录查看Namespace文件信息。Namespace机制虽然能帮助我们隔离了资源互不干扰，但是其本质还是在同一个内核进程上创建出来的，共享着宿主机内核，这就限制了不能在不同架构下运行相同的镜像，比如，arm架构的服务器不能运行x86架构下制作的镜像。由于容器进程是共享着宿主机资源，如果只是通过Namespace对容器进行隔离，并不能有效的限制容器之间的资源争抢，以及资源问题导致的容器间运行互相干扰，这时就需要另一项技术Cgroup，对容器进行资源限制。  \nCgroup#Linux Cgroup 即 Linux Control Group。它最主要的作用是限制一个进程组资源，包括CPU，Memory，磁盘，带宽等。通常可以在/sys/fs/cgroup路径下看到相应的文件信息，可以通过下面命令查看：mount -t cgroup  \nCgroup通过下面几个子系统协调完成工作：  \n\n\n\n子系统\n用途\n\n\n\ndevices\n设备权限控制\n\n\ncpuset\n分配指定的CPU和内存节点\n\n\nCPU\n控制CPU使用率\n\n\ncpuacct\n统计CPU使用情况\n\n\nmemory\n限制内存的使用上限\n\n\nfreezer\n暂停Cgroup中的进程\n\n\nblkio\n限制进程的块设备io\n\n\nnet_cls\n配合流控限制网络带宽\n\n\nnet_prio\n设置进程的网络流量优先级\n\n\nperf_event\n允许Perf工具基于Cgroup分组做性能检测\n\n\nhuge_tlb\n限制HugeTLB使用\n\n\nns\n可以使不同cgroups下面的进程使用不同的namespace\n\n\n内核使用cgroup结构体来表示一个cgroup对一个或者某几个cgroup子系统的资源限制，cgroup结构体就组成了一个树结构。在这个cgroup树上的节点都有一个控制任务列表，一个cgroup可以控制多个进程，同时一个进程也可以加入到多个cgroup中，进程与cgroup是多对多的关系。\n网络#我们已经了解到容器是通过Namespace进行资源隔离和Cgroup进行资源限制创建出来的特殊进程，然而新创建的容器怎么与宿主机网络通信以及宿主机上其他Namespace进行通信？这是就要用到上面提到的Network Namespace。Network Namespace的作用是创建以及管理Namespace的网络。可以通过Network Namespace创建多个隔离的网络空间，每个网络空间都拥有独自的网络栈信息。俩个Namespace之间通信可以通过Linux提供的网卡对veth pair实现，但是多个Namespace之间通信如果还借助veth pair来实现就会十分复杂，这个时候就需要借助虚拟网络来实现，虚拟网络有以下三种模式：  \n\n桥接模式\nNAT模式\n主机模式文件系统#通过Namespace和Cgroup可以有效的隔离和限制容器的运行环境，但是程序的运行还需要文件系统的支撑，这时就要用到我们上面说的Mount Namespace对容器运行的文件系统进行隔离。如果不修改根目录的挂载点，那么容器启动之后会默认继承宿主机的挂载点，如果修改了根目录的挂载点，那么我们创建容器后会得到一个空的根目录。Linux的文件目录挂载涉及下面几个工具：  \n\n\nchroot：全称change root file system，这个命令能帮我们将任意目录转化为指定进程的根目录；\nrootfs：负责把除操作系统内核外的文件、配置和目录挂载到容器进程的根目录；\nUnionFS：不同容器的rootfs大多是重合的，为了有效的利用宿主机存储空间，这是就需要对重复的文件就行合并，也就产生了联合挂载Union File System的挂载方式，将多个不同位置的目录联合挂载到同一个目录下面，这也是Docker采用的技术。\n\n镜像#Docker结合UnionFS技术，使用多个增量的rootfs联合挂载一个完整的rootfs，就此诞生了分层镜像。这样的分层结构极大的利用了宿主机的存储空间，并且还能基础镜像上进行堆叠不同的层，使得使用更加灵活，扩展性更强，同时保证了基础镜像的强一致性。Docker image分层结构：\n总结#通过对进程、Namespace、Cgroup和Rootfs的了解，容器归根结底是一个特殊的进程，它的特殊性可以概括为下面的等式：容器 = Namespace + Cgroups + Rootfs + 容器引擎\n","slug":"容器技术实现原理","date":"2022-04-05T14:56:56.000Z","categories_index":"Container","tags_index":"Linux,Container","author_index":"Asura"},{"id":"f21ed2995d94acc11636251a32951b12","title":"开源协议对比","content":"\n\n\n\n\n\n\n\n\n当今开源软件基本都会选择自己的开源协议，这些开源协议中大部分都是由Open Source Initiative（OSI）批准通过的。  \n\n\n\n\n\n简介\nApache License 2.0 (Apache-2.0)\nMIT\nBSD\nGPL\nLGPL\nMPL\n其他\n总结\n\n\n\n\n\n简介#我们都知道现在大部分开源协议都是由Open Source Initiative（简称 OSI）批准通过的，OSI即开放源代码促进会，是一个旨在推动开源软件发展的非盈利组织，其中常见的有Apache-2.0，MIT，BSD，GPL等等，其他开源协议可以查看 这里。\n作为一个开发者，我们身处的这个时代充斥着各式各样的开源项目，当我们去使用或者自己开源代码的时候，对开源协议的理解和选择尤其重要，所以在这里总结和对比当下主流的开源协议，供大家参考。\n话不多说，先看一下各开源协议对比图：\nApache License 2.0 (Apache-2.0)#Apache-2.0相信大家都不会陌生，是Apache开源组织采用的协议。该开源协议授予项目版权及专利许可，鼓励共享代码，同时允许需改代码，作为开源或商业软件再发布。  \n\n\n\n\n\n\n\n\n\n“Derivative Works” shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.“Contribution” shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, “submitted” means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as “Not a Contribution.”\n 再发布，需要遵守下面几点要求：\n\n\n\n\n\n\n\n\n\n1.You must give any other recipients of the Work or Derivative Works a copy of this License; and2.You must cause any modified files to carry prominent notices stating that You changed the files; and3.You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and4.If the Work includes a “NOTICE” text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\n\n 必须向作品或衍生作品的任何其他接收者提供本许可的副本；\n必须使任何修改后的文件带有显着的通知，说明已更改文件；\n必须以您分发的任何衍生作品的源形式保留作品源形式的所有版权、专利、商标和归属通知，不包括那些与衍生作品的任何部分无关的通知；\n如果作品在其分发中包含“通知”文本文件，则您分发的任何衍生作品必须包含该通知文件中包含的归属通知的可读副本，不包括那些不属于任何部分的通知衍生作品，至少位于以下位置之一： 在作为衍生作品的一部分分发的 NOTICE 文本文件中；如果与衍生作品一起提供，则在源表格或文档中；或者，在衍生作品生成的显示中，如果此类第三方通知通常出现以及出现在何处。NOTICE 文件的内容仅供参考，不得修改许可。您可以在您分发的衍生作品中添加您自己的归属通知，与作品的通知文本一起或作为附录添加\n\n可以说Apache-2.0给予了开发足够的自由的同时极大程度的保护了原作者的权益。我们熟知的遵循Apache-2.0协议的开源项目有Kubernetes，Android等\nMIT#MIT也是我们熟知的开源协议，出自麻省理工学院的许可证，是一种极度开放自由的开源协议。作者只需保留版权，开发者可以免费且不受限的处理该软件，包括但不限于使用、复制、修改、合并的权利、发布、分发、再许可和/或出售本软件的副本，并允许向其提供本软件的人这样做，但是必须保留该软件的版权声明和许可声明。\n\n\n\n\n\n\n\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.我们熟知的遵循MIT协议的开源项目有React，Node等\nBSD#BSD有多个版本，当今使用较多的是2-Clause BSD也被称为Simplified BSD License或者FreeBSD License，还有修订版本3-Clause BSD。BSD也是一个给予开发者极大自由度的开源协议，允许开发者以源代码和二进制的形式进行使用和重新发布，但必须遵循以下要求：  \n\n\n\n\n\n\n\n\n\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.  \nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.  \nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.  \n\n\n源代码的再分发必须保留原先版权声明、此条件列表和以下免责声明。\n二进制形式的再分发必须在随分发提供的文档和/或其他材料中复制上述版权声明、此条件列表和以下免责声明。\n未经事先书面许可，不得使用版权所有者的姓名或其贡献者的姓名来认可或推广从本软件衍生的产品。\n\n\n\n\n\n\n\n\n\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.  \n我们熟知的遵循BSD协议的开源项目有蓝鲸，Matlab等\nGPL#GPL协议给予开发者的自由度并不像Apache，BSD，MIT这么高，更确切的说是对商业化应用并不是很友好，允许开源代码的免费使用和再发布，但是不能作为闭源的商业软件。GPL本身还具备很强的传递性，任何修改和衍生品都必须遵循GPL协议。  \n\n\n\n\n\n\n\n\n\nWe protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software.Also, for each author’s protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors’ reputations.Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone’s free use or not licensed at all.\n如果开发者需要使用和再发布，需要遵循的条款很多，下面列举几点比较重要的：\n\n\n\n\n\n\n\n\n\n\nThis License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The “Program”, below, refers to any such program or work, and a “work based on the Program” means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term “modification”.) Each licensee is addressed as “you”.Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does.  \nYou may copy and distribute verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program.You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee.……\n\n\n任何引用、修改和翻译都需要告知版权方\n任何衍生品都需要保留源先的版权许可声明\n必须有条件的更改，使用等等\n\n有关于GPL协议比较有争议的还有GPL V2和V3修正版，有兴趣的同学可以自行查阅资料。我们熟知的遵循GPL协议的开源项目有鼎鼎大名的Linux，Proxysql等。\nLGPL#LGPL是GPL的一个为主要为类库使用设计的开源协议。和GPL要求任何使用/修改/衍生之GPL类库的的软件必须采用GPL协议不同。 LGPL 允许商业软件通过类库引用(link)方式使用LGPL类库而不需要开源商业软件的代码。这使得采用LGPL协议的开源代码可以被商业软件作为类库引用并 发布和销售。\n但是如果修改LGPL协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用LGPL协议。因此LGPL协议的开源 代码很适合作为第三方类库被商业软件引用，但不适合希望以LGPL协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。\nGPL/LGPL都保障原作者的知识产权，避免有人利用开源代码复制并开发类似的产品。我们熟知的遵循LGPL协议的开源项目有OpenPDF，echo3等\nMPL#Mozilla公共许可证（MPL），由Mozilla基金会开发维护。MPL出现的最重要原因就是，Netscape公司认为GPL没有很好地平衡开发者对源代码的需求和他们利用源代码获得的利益。MPL再发布需要遵循以下几点：  \n\nMPL虽然要求对于经MPL许可证发布的源代码的修改也要以MPL许可证的方式再许可出来，以保证其他人可以在MPL的条款下共享源代码。但是，在MPL许可证中对“发布”的定义是“以源代码方式发布的文件”，这就意味着MPL允许一个企业在自己已有的源代码库上加一个接口，除了接口程序的源代码以MPL许可证的形式对外许可外，源代码库中的源代码就可以不用MPL许可证的方式强制对外许可。这些，就为借鉴别人的源代码用做自己商业软件开发的行为留了一个豁口。\nMPL许可证第三条第7款中允许被许可人将经过MPL许可证获得的源代码同自己其他类型的代码混合得到自己的软件程序。\n对软件专利的态度，MPL许可证不像GPL许可证那样明确表示反对软件专利，但是却明确要求源代码的提供者不能提供已经受专利保护的源代码（除非他本人是专利权人，并书面向公众免费许可这些源代码），也不能在将这些源代码以开放源代码许可证形式许可后再去申请与这些源代码有关的专利。\n对源代码的定义。在MPL（1.1版本）许可证中，对源代码的定义是:“源代码指的是对作品进行修改最优先择取的形式，它包括:所有模块的所有源程序，加上有关的接口的定义，加上控制可执行作品的安装和编译的‘原本’（原文为‘Script’），或者不是与初始源代码显著不同的源代码就是被源代码贡献者选择的从公共领域可以得到的程序代码。”\nMPL许可证第3条有专门的一款是关于对源代码修改进行描述的规定，就是要求所有再发布者都得有一个专门的文件就对源代码程序修改的时间和修改的方式有描述\n\n我们熟知的遵循MPL协议的开源项目有Firefox，osbpm等\n其他#除了上面几种比较著名的开源协议，还有许多比较特定的开源协议:  \n\nW3C：万网开源协议  \nPHP-3.0：PHP软件开源协议  \nNTP：网络时间协议……\n\n总结#上面介绍的几种开源协议各有各的特点，相对于自由度来讲MIT具备更高的自由度，GPL对开源项目版权更加重视和保护。如果在选择使用开源项目时可能会根据实际情况进行更改，甚至可能后期售卖可以考虑Apache-2.0，BSD和MIT。如果仅仅是使用，并且可以参与到其建设，可以考虑选择GPL，LGP，MPL。如果是将自己的项目开源，并且给予广大开发者极大的权限，可以选择MIT，BSD。如果在开源项目的同时，想要保有自己的版权许可，可以选择Apache，GPL。当然，在现实环境中存在各种各样不确定因素，需要开发者们根据实际情况以及对应的法律条文进行选择，以上描述仅供参考。\n","slug":"开源协议对比","date":"2022-04-05T14:30:00.000Z","categories_index":"OSI","tags_index":"OSI","author_index":"Asura"}]